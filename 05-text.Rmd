# ç¬¬å…­ç«  æ–‡æœ¬æ•°æ®åˆ†æ

&emsp;&emsp;æ–‡æœ¬æ•°æ®å±äºéç»“æ„åŒ–æ•°æ®ï¼Œé€šå¸¸éœ€è¦é€šè¿‡åˆ†è¯ã€è¯æ€§èµ‹ç ç­‰è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œå°†æ–‡æœ¬æ•°æ®è¿›è¡Œç»“æ„åŒ–ã€‚
åœ¨è¯­è¨€å­¦ç ”ç©¶ä¸­ï¼Œè¯­æ–™åº“ç ”ç©¶ä¸æ–‡æœ¬æœ€ä¸ºç›¸å…³ã€‚Tognini-Bonelliï¼ˆ2001ï¼‰æå‡ºåŸºäºè¯­æ–™åº“ï¼ˆcorpus-basedï¼‰å’Œè¯­æ–™åº“é©±åŠ¨ï¼ˆcorpus-drivenï¼‰çš„åŒºåˆ†ã€‚åŸºäºè¯­æ–™åº“çš„ç ”ç©¶å°†è¯­æ–™åº“ç”¨ä½œéªŒè¯ç ”ç©¶è€…ç›´è§‰æˆ–æ£€æŸ¥å°å‹æ•°æ®é›†ä¸­è¯­è¨€çš„é¢‘ç‡å’Œ/æˆ–åˆç†æ€§çš„ä¾‹å­æ¥æºã€‚ç ”ç©¶è€…ä¸ä¼šè´¨ç–‘é¢„å…ˆå­˜åœ¨çš„ä¼ ç»Ÿæè¿°å•å…ƒå’Œç±»åˆ«ã€‚è¯­æ–™åº“é©±åŠ¨çš„åˆ†æåˆ™æ˜¯ä¸€ç§æ›´å…·å½’çº³æ€§çš„è¿‡ç¨‹ï¼šè¯­æ–™åº“æœ¬èº«å°±æ˜¯æ•°æ®ï¼Œåˆ†æè¿‡ç¨‹ä¸­é€šè¿‡è®°å½•è¯­æ–™åº“ä¸­çš„æ¨¡å¼æ¥è¡¨è¾¾è¯­è¨€ä¸­çš„è§„å¾‹æ€§ï¼ˆå’Œä¾‹å¤–æƒ…å†µï¼‰ã€‚è¯­æ–™åº“é©±åŠ¨çš„åˆ†æå€¾å‘äºåªä½¿ç”¨å…³äºè¯­æ³•ç»“æ„çš„æœ€ä½é™åº¦çš„ç†è®ºé¢„è®¾ã€‚

&emsp;&emsp;åœ¨è¯æ±‡å±‚é¢çš„è¯­æ–™åº“ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šè¿›è¡Œè¯é¢‘ç»Ÿè®¡ï¼ˆWord Frequency Countï¼‰ï¼Œå³ç»Ÿè®¡è¯æ±‡åœ¨è¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œä»¥äº†è§£è¯æ±‡çš„ä½¿ç”¨é¢‘ç‡å’Œåˆ†å¸ƒæƒ…å†µï¼Œæ„å»ºè¯è¡¨ã€‚å…¶æ¬¡ï¼Œä¸å‚è€ƒè¯­æ–™åº“æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œå…³é”®è¯æå–ï¼ˆKeyword Extractionï¼‰å³è¯†åˆ«å‡ºåœ¨ç‰¹å®šè¯­å¢ƒæˆ–æ–‡æœ¬ç±»å‹ä¸­ç‰¹åˆ«é¢‘ç¹å‡ºç°çš„è¯æ±‡ã€‚

&emsp;&emsp;å†è€…ï¼Œå¦‚æœå¯¹äºæŸäº›ç‰¹å®šè¯æ±‡æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å…±ç°åˆ†æï¼ˆconcordance Analysisï¼‰ç ”ç©¶å“ªäº›è¯æ±‡ç»å¸¸ä¸€èµ·å‡ºç°ï¼Œä»¥äº†è§£è¯æ±‡ä¹‹é—´çš„å…³ç³»å’Œè¯­å¢ƒã€‚

&emsp;&emsp;æœ€åï¼Œé€šè¿‡collocation anlysisæˆ‘ä»¬å¯ä»¥åˆ†æè¯ç»„æ­é…ã€‚

&emsp;&emsp;é™¤äº†å¯¹æ–‡æœ¬ä¸­çš„è¯æ±‡è¿›è¡Œåˆ†æï¼Œä¹Ÿå¯ä»¥é€šè¿‡è¯æ±‡æ¥åˆ†ææ–‡æœ¬çš„é£æ ¼ã€‚è¯æ±‡ä¸°å¯Œåº¦å’Œå¯è¯»æ€§åˆ†æå¯ä»¥å¸®æˆ‘ä»¬æ¯”è¾ƒä¸åŒæ–‡æœ¬ä¹‹é—´è¯æ±‡å¤æ‚æ€§çš„å·®å¼‚ã€‚åœ¨åˆ†æä¸åŒç¿»è¯‘æ–‡æœ¬çš„ç‰¹å¾æ˜¯æˆ‘ä»¬å¸¸å¸¸ç”¨åˆ°ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ–‡æœ¬ä¸­æœ€é«˜é¢‘çš„å‡ ç™¾ä¸ªè¯çš„é¢‘ç‡ç‰¹å¾æˆ‘ä»¬å¯ä»¥è§£é”ä¸åŒä½œå®¶çš„åˆ›ä½œæŒ‡çº¹ã€‚è¿™æ ·çš„åˆ†æå¯ä»¥å¸®æˆ‘ä»¬é‰´å®šä¸€äº›å½’å±å­˜ç–‘çš„æ–‡æœ¬ä½œå“ã€‚

&emsp;&emsp;åœ¨Rè¯­è¨€ä¸­å¸¸ç”¨çš„æ–‡æœ¬å¤„ç†çš„ç¨‹åºåŒ…æœ‰tidytextå’Œquentedaã€‚ä»–ä»¬å„æœ‰ä¸åŒçš„åŠŸèƒ½ï¼ŒåŒæ—¶å½¼æ­¤ä¹‹é—´å¯ä»¥ç›¸äº’è½¬åŒ–ã€‚


```{r message=FALSE}
library(tidyverse)
setwd("~/Nutstore Files/310_Tutorial/LanguageDS-e")
library(quanteda)
#install.packages("readtext")
require(readtext)
library(DT)
library(DiagrammeR)

library(FactoMineR)
library(factoextra)
library(flextable)
library(GGally)
library(ggdendro)
library(igraph)
library(network)
library(Matrix)
library(quanteda.textstats)
library(quanteda.textplots)
library(tm)
library(sna)
library(udpipe)
```


```{r}
grViz("
digraph text_workflow {

  # a 'graph' statement
  graph [overlap = true, 
        fontsize = 10, 
        rankdir = LR]

  # èŠ‚ç‚¹
  node [shape = box,
        style = unfilled,
        fontname = Helvetica]
  æ–‡æœ¬åˆ†æ[shape = folder]; 
  è¯æ±‡åˆ†æ; 
  çŸ­è¯­åˆ†æ; 
  å¥å­åˆ†æ; 

  
  node [shape = oval,
        style = filled,
        fontname = Helvetica]
   è¯é¢‘åˆ†æ;
   å…³é”®è¯åˆ†æ; 
   æ­é…åˆ†æ; 
   è¯æ±‡ä¸°å¯Œåº¦;  
   ä¿¡æ¯ç†µ;  
   æƒ…æ„Ÿåˆ†æ; 
   å¿ƒç†è¯­è¨€å­¦ç‰¹å¾  ;  
   è¯æ±‡è¯­ä¹‰ç›¸ä¼¼åº¦ ;  
   ä¸Šä¸‹æ–‡åˆ†æ;  
   æ­é…åˆ†æ;  
   è¯æ€§åˆ†æ;  
   å¥å­ä¾å­˜åˆ†æ;  
   å¯è¯»æ€§åˆ†æ; 
  
# è¾¹çº¿
  æ–‡æœ¬åˆ†æ ->è¯æ±‡åˆ†æ
  æ–‡æœ¬åˆ†æ ->çŸ­è¯­åˆ†æ
  æ–‡æœ¬åˆ†æ ->å¥å­åˆ†æ


  è¯æ±‡åˆ†æ-> è¯é¢‘åˆ†æ;
   è¯æ±‡åˆ†æ-> å…³é”®è¯åˆ†æ; 
   è¯æ±‡åˆ†æ-> è¯æ±‡ä¸°å¯Œåº¦;  
   è¯æ±‡åˆ†æ-> ä¿¡æ¯ç†µ;  
   è¯æ±‡åˆ†æ-> æƒ…æ„Ÿåˆ†æ; 
   è¯æ±‡åˆ†æ-> å¿ƒç†è¯­è¨€å­¦ç‰¹å¾  ;  
   è¯æ±‡åˆ†æ-> è¯æ±‡è¯­ä¹‰ç›¸ä¼¼åº¦ ;  
  
  çŸ­è¯­åˆ†æ->ä¸Šä¸‹æ–‡åˆ†æ;  
   çŸ­è¯­åˆ†æ->æ­é…åˆ†æ;  
   å¥å­åˆ†æ->è¯æ€§åˆ†æ;  
   å¥å­åˆ†æ->å¥å­ä¾å­˜åˆ†æ;  
   å¥å­åˆ†æ->å¯è¯»æ€§åˆ†æ; 
  



}
")

```






## è¯æ±‡ç‰¹å¾



```{r}


grViz("
digraph data_wrangling_workflow {

  # a 'graph' statement
  graph [overlap = true, 
        fontsize = 10, 
        rankdir = LR]

  # èŠ‚ç‚¹
  node [shape = box,
        style = unfilled,
        fontname = Helvetica]
  å¯¼å…¥readtext[label = 'å¯¼å…¥ \n readtext', shape = folder, fillcolor = Beige]; 
  æ„å»ºè¯­æ–™åº“corpus[label = 'æ„å»ºè¯­æ–™åº“ \n corpus']; 
  åˆ†è¯tokens[label = 'åˆ†è¯ \n tokens']; 
  åˆ†è¯unnest_tokens[label = 'åˆ†è¯ \n unnest_tokens', color = red]; 
  dfm ;   

  
  node [shape = oval,
        style = filled,
        fontname = Helvetica]
   è¯é¢‘textstat_frequency[label = 'è¯é¢‘ \n textstat_frequency'];
   KWIC[label = 'KWIC \n kwic'] ; 
   æ­é…textstat_collocations[label = 'æ­é… \n textstat_collocations']; 
   å…³é”®è¯åˆ†ætextstat_keyness[label = 'å…³é”®è¯åˆ†æ \n textstat_keyness']   ;  
  
# è¾¹çº¿
å¯¼å…¥readtext ->æ„å»ºè¯­æ–™åº“corpus->åˆ†è¯tokens-> dfm -> è¯é¢‘textstat_frequency
dfm -> å…³é”®è¯åˆ†ætextstat_keyness
åˆ†è¯tokens -> KWIC
åˆ†è¯tokens -> æ­é…textstat_collocations

æ„å»ºè¯­æ–™åº“corpus->åˆ†è¯unnest_tokens
åˆ†è¯unnest_tokens -> è¯é¢‘textstat_frequency
åˆ†è¯unnest_tokens -> å…³é”®è¯åˆ†ætextstat_keyness
}
")

```

### è¯é¢‘æå–

&emsp;&emsp;è¯é¢‘æ˜¯è¯­æ–™åº“è¯­è¨€å­¦æœ€åŸºæœ¬çš„æ¦‚å¿µã€‚é¢‘ç‡å¯ä»¥ä»¥åŸå§‹æ•°æ®çš„å½¢å¼ç»™å‡ºï¼Œä¾‹å¦‚åœ¨æŸä¸ªæ–‡æœ¬ä¸­ï¼ŒæŸä¸ªè¯å‡ºç°äº†58æ¬¡ï¼›æˆ–è€…å¯ä»¥ä»¥ç™¾åˆ†æ¯”æˆ–æ¯”ä¾‹çš„å½¢å¼ç»™å‡ºï¼ŒæŸä¸ªè¯åœ¨æ¯ç™¾ä¸‡è¯ä¸­å‡ºç°602.91æ¬¡ã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨ä¸åŒå¤§å°çš„è¯­æ–™åº“ä¹‹é—´è¿›è¡Œæ¯”è¾ƒã€‚

&emsp;&emsp;é€šè¿‡å¯¹è¯­æ–™åº“ä¸­æ¯ä¸ªè¯è¿›è¡Œè¯é¢‘ç»Ÿè®¡ç¼–åˆ¶çš„è¯è¡¨å¯ä»¥ç”¨æ¥ç”Ÿæˆå…³é”®è¯åˆ—è¡¨ã€‚


<font color=red size=5>*quanteda package*</font>

```{r message=FALSE}

# quanteda
detectives.raw <- readtext("data/ch6/detectives/*txt")

corps.detectives = corpus(detectives.raw)

summary(corpus(corps.detectives), 5)

docvars(corps.detectives, "book") <- names(corps.detectives)

tidy.corps.detectives = tidytext::tidy(corps.detectives)

```


```{r }

# make a dfm

dfm_detectives <- corps.detectives %>%
   tokens(remove_punct = TRUE) %>%
   tokens_remove(stopwords("en")) %>%
   dfm()%>%
   dfm_group(groups = book)

print(dfm_detectives)

topfeatures(dfm_detectives, 20)


```



```{r}


library("quanteda.textstats")
library("quanteda.textplots")

dfm_detectives.freq <- textstat_frequency(dfm_detectives, 
                                          n = 20, 
                                          groups = book)

ggplot(dfm_detectives.freq, 
       aes(x = frequency, y = reorder(feature, frequency))) +
    geom_point() + 
    labs(x = "Frequency", y = "Feature")+
    facet_wrap(~ group, scales = "free") 




```

### å…³é”®è¯æå–
&emsp;&emsp;åœ¨è¯­æ–™åº“è¯­è¨€å­¦ä¸­ï¼Œå…³é”®è¯åˆ†æï¼ˆkeyword analysisï¼‰æ˜¯ä¸€ç§é€šè¿‡ä¸å‚è€ƒè¯­æ–™åº“è¿›è¡Œæ¯”è¾ƒï¼Œè¯†åˆ«å‡ºåœ¨ç‰¹å®šè¯­æ–™åº“ä¸­å‡ºç°é¢‘ç‡å¼‚å¸¸é«˜ï¼ˆæ­£å…³é”®è¯ï¼‰æˆ–å¼‚å¸¸ä½ï¼ˆè´Ÿå…³é”®è¯ï¼‰çš„è¯è¯­çš„æ–¹æ³•ã€‚

&emsp;&emsp;**å…³é”®è¯ï¼ˆKeywordï¼‰**æ˜¯æŒ‡åœ¨ä¸€ä¸ªæ–‡æœ¬æˆ–è¯­æ–™åº“ä¸­ï¼Œä¸å‚è€ƒè¯­æ–™åº“ç›¸æ¯”ï¼Œå‡ºç°é¢‘ç‡æ˜¾è‘—é«˜äºæˆ–ä½äºé¢„æœŸçš„è¯è¯­ã€‚é€šå¸¸ï¼Œä½¿ç”¨ç»Ÿè®¡æµ‹è¯•ï¼ˆå¦‚å¯¹æ•°ä¼¼ç„¶æ£€éªŒæˆ–å¡æ–¹æ£€éªŒï¼‰æ¥æ¯”è¾ƒä¸¤ä¸ªè¯è¡¨ï¼Œä»¥å¾—å‡ºå…³é”®è¯ã€‚
&emsp;&emsp;**å‚è€ƒè¯­æ–™åº“ï¼ˆReference corpusï¼‰**æ˜¯ä¸€ä¸ªå¹³è¡¡ä¸”ä»£è¡¨æ€§çš„è¯­æ–™åº“ï¼Œé€šå¸¸ç”¨äºå…³é”®è¯åˆ†æä¸­æä¾›å‚è€ƒè¯è¡¨ã€‚åœ¨å…³é”®è¯åˆ†æä¸­ï¼Œé€šè¿‡ä¸å‚è€ƒè¯­æ–™åº“çš„æ¯”è¾ƒï¼Œå¯ä»¥å‘ç°å“ªäº›è¯åœ¨ç‰¹å®šè¯­æ–™åº“ä¸­æ˜¯å…³é”®çš„ã€‚

&emsp;&emsp;åœ¨å…³é”®è¯åˆ†æä¸­ï¼Œä¸€ä¸ªè¯å¯èƒ½ä»…ä»…å› ä¸ºåœ¨æŸäº›æ–‡æœ¬ä¸­é¢‘ç‡æé«˜ï¼Œè€Œè¢«è¯¯è®¤ä¸ºæ˜¯å…³é”®è¯ã€‚ä¸ºäº†ç¡®è®¤ä¸€ä¸ªè¯æ˜¯å¦çœŸçš„å…·æœ‰ä»£è¡¨æ€§ï¼Œå¯ä»¥ä½¿ç”¨åˆ†å¸ƒå›¾æŸ¥çœ‹è¿™ä¸ªè¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„åˆ†å¸ƒï¼Œæˆ–è€…è®¡ç®—åœ¨å¤šä¸ªæ–‡æœ¬ä¸­å‡ä¸ºå…³é”®è¯çš„è¯æ±‡åˆ—è¡¨ï¼Œä»¥é¿å…å› ä¸å‡åŒ€åˆ†å¸ƒå¯¼è‡´çš„åå·®ã€‚

#### quanteda package

```{r cache=TRUE}
# Calculate keyness and determine Trump as target group
tstat_keyness <- textstat_keyness(dfm_detectives , 
                                  target = "Dee.txt")

tstat_keyness%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
)

# Plot estimated word keyness
textplot_keyness(tstat_keyness) 
```

#### tidytext
```{r cache=TRUE}



texts_df = detectives.raw%>%
  unnest_tokens(word, text)%>%
  mutate(doc_id = dplyr::recode(doc_id , "Dee.txt" = "text1",
                                  "Sherlock.txt" = "text2"))%>%
  group_by(doc_id,word)%>%
  summarise(freq = n())%>%
  spread(doc_id, freq, fill = 0)%>%
  dplyr::rename(token = "word")
  
  
stats_tb2 = texts_df %>%
  dplyr::mutate(text1 = as.numeric(text1),
                text2 = as.numeric(text2)) %>%
  dplyr::mutate(C1 = sum(text1),
                C2 = sum(text2),
                N = C1 + C2) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(R1 = text1+text2,
                R2 = N - R1,
                O11 = text1,
                O12 = R1-O11,
                O21 = C1-O11,
                O22 = C2-O12) %>%
  dplyr::mutate(E11 = (R1 * C1) / N,
                E12 = (R1 * C2) / N,
                E21 = (R2 * C1) / N,
                E22 = (R2 * C2) / N) %>%
  dplyr::select(-text1, -text2)

head(stats_tb2)%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 


assoc_tb3 = stats_tb2 %>%
  # determine number of rows
  dplyr::mutate(Rws = nrow(.)) %>%   
  # work row-wise
  dplyr::rowwise() %>%
    # calculate fishers' exact test
  dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), 
                                                        ncol = 2, byrow = T))[1]))) %>%

# extract descriptives
    dplyr::mutate(ptw_target = O11/C1*1000,
                  ptw_ref = O12/C2*1000) %>%
    
    # extract x2 statistics
    dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %>%
    
    # extract keyness measures
    dplyr::mutate(phi = sqrt((X2 / N)),
                  MI = log2(O11 / E11),
                  t.score = (O11 - E11) / sqrt(O11),
                  PMI = log2( (O11 / N) / ((O11+O12) / N) * 
                                ((O11+O21) / N) ),
                  DeltaP = (O11 / R1) - (O21 / R2),
                  LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5))  / ( (O12 + 0.5) * (O21 + 0.5) )),
                  G2 = 2 * ((O11+ 0.001) * log((O11+ 0.001) / E11) + (O12+ 0.001) * log((O12+ 0.001) / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22)),
                  
                  # traditional keyness measures
                  RateRatio = ((O11+ 0.001)/(C1*1000)) / ((O12+ 0.001)/(C2*1000)),
                  RateDifference = (O11/(C1*1000)) - (O12/(C2*1000)),
                  DifferenceCoefficient = RateDifference / sum((O11/(C1*1000)), (O12/(C2*1000))),
                  OddsRatio = ((O11 + 0.5) * (O22 + 0.5))  / ( (O12 + 0.5) * (O21 + 0.5) ),
                  LLR = 2 * (O11 * (log((O11 / E11)))),
                  RDF = abs((O11 / C1) - (O12 / C2)),
                  PDiff = abs(ptw_target - ptw_ref) / ((ptw_target + ptw_ref) / 2) * 100,
                  SignedDKL = sum(ifelse(O11 > 0, O11 * log(O11 / ((O11 + O12) / 2)), 0) - ifelse(O12 > 0, O12 * log(O12 / ((O11 + O12) / 2)), 0))) %>%
    
    # determine Bonferroni corrected significance
    dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws > .05 ~ "n.s.",
                                                   p / Rws > .01 ~ "p < .05*",
                                                   p / Rws > .001 ~ "p < .01**",
                                                   p / Rws <= .001 ~ "p < .001***",
                                                   T ~ "N.A.")) %>% 
    # round p-value
    dplyr::mutate(p = round(p, 5),
                  type = ifelse(E11 > O11, "antitype", "type"),
                  phi = ifelse(E11 > O11, -phi, phi),
                  G2 = ifelse(E11 > O11, -G2, G2)) %>%
    # filter out non significant results
    dplyr::filter(Sig_corrected != "n.s.") %>%
    # arrange by G2
    dplyr::arrange(-G2) %>%
    # remove superfluous columns
    dplyr::select(-any_of(c("TermCoocFreq", "AllFreq", "NRows", 
                            "R1", "R2", "C1", "C2", "E12", "E21",
                            "E22", "upp", "low", "op", "t.score", "z.score", "Rws"))) %>%
    dplyr::relocate(any_of(c("token", "type", "Sig_corrected", "O11", "O12",
                             "ptw_target", "ptw_ref", "G2",  "RDF", "RateRatio", 
                             "RateDifference", "DifferenceCoefficient", "LLR", "SignedDKL",
                             "PDiff", "LogOddsRatio", "MI", "PMI", "phi", "X2",  
                             "OddsRatio", "DeltaP", "p", "E11", "O21", "O22"))) 
head(assoc_tb3)%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 


```

### ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯ç´¢å¼•ï¼ˆKWICï¼‰


&emsp;&emsp;ç´¢å¼•ï¼ˆConcordanceï¼‰æ˜¯æŒ‡åœ¨è¯­æ–™åº“ä¸­æŒ‰ç…§å­—æ¯é¡ºåºæ’åˆ—çš„æœç´¢æ¨¡å¼ç´¢å¼•ï¼Œæ˜¾ç¤ºè¯¥æœç´¢æ¨¡å¼åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡ä¸­çš„å‡ºç°ã€‚ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯ç´¢å¼•ï¼ˆKey-Word-In-Context Concordanceï¼Œç®€ç§°KWICï¼‰æ˜¯ä¸€ç§æ˜¾ç¤ºå…³é”®è¯åœ¨è¯­æ–™åº“ä¸­å®é™…ä½¿ç”¨æƒ…å†µçš„å·¥å…·æˆ–æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒKWIC ç´¢å¼•ä¼šå°†ç›®æ ‡å…³é”®è¯æ”¾åœ¨ä¸­å¿ƒä½ç½®ï¼Œå¹¶æ˜¾ç¤ºè¯¥è¯åœ¨æ–‡æœ¬ä¸­çš„æ‰€æœ‰å‡ºç°ä½ç½®ï¼ŒåŒæ—¶æ˜¾ç¤ºå®ƒå‰åçš„ä¸€å®šæ•°é‡çš„å•è¯æˆ–è¯ç»„ã€‚è¿™ç§æ’åˆ—æ–¹å¼å¯ä»¥å¸®åŠ©ç ”ç©¶è€…è§‚å¯Ÿå…³é”®è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„ç”¨æ³•å’Œæ„ä¹‰ã€‚

&emsp;&emsp;ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³ç ”ç©¶è‹±è¯­ä¸­â€œeducationâ€ä¸€è¯çš„ç”¨æ³•ï¼ŒKWIC ç´¢å¼•ä¼šå°†æ‰€æœ‰åŒ…å«â€œeducationâ€çš„å¥å­æˆ–å¥æ®µåˆ—å‡ºï¼Œå±•ç¤ºè¯¥è¯å·¦å³çš„ä¸Šä¸‹æ–‡ã€‚è¿™æ ·ï¼Œç ”ç©¶è€…å°±å¯ä»¥ä¸€ç›®äº†ç„¶åœ°çœ‹åˆ°â€œeducationâ€åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„ä½¿ç”¨æ–¹å¼ï¼Œå¹¶åˆ†æå…¶é¢‘ç‡ã€æ­é…ã€è¯­æ³•ç‰¹å¾ä»¥åŠè¯­ä¹‰å˜åŒ–ã€‚

&emsp;&emsp;KWIC ç´¢å¼•åœ¨è¯­æ–™åº“è¯­è¨€å­¦ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…æ·±å…¥ç†è§£è¯è¯­åœ¨å®é™…è¯­è¨€ä½¿ç”¨ä¸­çš„è¡Œä¸ºï¼Œè€Œä¸ä»…ä»…æ˜¯å…³æ³¨è¯é¢‘ç­‰è¡¨é¢æ•°æ®ã€‚

```{r }
toks_corpus_detectives <- corps.detectives%>%
   tokens(remove_punct = TRUE)%>%
   tokens_remove(stopwords("en")) 

kwic(toks_corpus_detectives , pattern = "murder") %>%
  head()%>%
  as.data.frame()
```

å¹³è¡Œè¯­æ–™åº“å…±ç°åˆ†æ

```{r}
# å¯¼å…¥
law_ce <- read_csv("data/ch6/law.csv",  col_names = TRUE)

# æ£€ç´¢ä¸­æ–‡
law_ce%>%
  filter(str_detect(cn, "å½“äº‹äºº"))%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) # put it at the top of the table


law_ce%>%
  mutate(count = str_count(cn, "å½“äº‹äºº"))%>%
  filter(count > 0)

##ä¸Šä¸‹æ’ç‰ˆçš„æ–‡æœ¬å¦‚ä½•æ„å»ºå¹³è¡Œè¯­æ–™åº“

trans_corpus <- read_csv("data/ch6/è‹±æ±‰å¯¹ç…§æ–‡æœ¬.csv",  col_names = FALSE)
row.no = nrow(trans_corpus)

trans.new = trans_corpus%>%
  mutate(key = rep(c("E","C"), (row.no/2)),
         id = rep(1:(row.no/2),each=2))%>%
  spread(key, X1)
trans.new %>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) # put it at the top of the table


#search
trans.new%>%
  filter(str_detect(E, "good"))%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) # put it at the top of the table



```



#### è¯æ±‡æ–‡æœ¬åˆ†å¸ƒ
```{r }
kwic(toks_corpus_detectives , pattern = "murder") %>%
    textplot_xray()

textplot_xray(
    kwic(toks_corpus_detectives, pattern = "murder"),
    kwic(toks_corpus_detectives, pattern = "judge"),
    kwic(toks_corpus_detectives, pattern = "police"),
    scale = "absolute")


```





## çŸ­è¯­åˆ†æ


### ä¸Šä¸‹æ–‡å…±ç°

```{r}
# å¯¹äºè¯ç»„
kwic(toks_corpus_detectives, pattern = phrase("commit crime")) %>%
    head()%>%
   as.data.frame()
```


### æ­é…åˆ†æ
å‚è€ƒhttps://ladal.edu.au/coll.html#Identifying_collocations_using_kwics

&emsp;&emsp;æ­é…åˆ†æï¼ˆcollocation analysisï¼‰æ˜¯è¯­æ–™åº“è¯­è¨€å­¦ä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œç”¨äºç ”ç©¶è¯æ±‡ä¹‹é—´çš„å…±ç°å…³ç³»ï¼Œå³åœ¨è‡ªç„¶è¯­è¨€ä¸­æŸäº›è¯è¯­ç»å¸¸ä¸€èµ·å‡ºç°çš„ç°è±¡ã€‚æ­é…åˆ†æé€šè¿‡è¯†åˆ«å’Œåˆ†æè¿™äº›è¯è¯­çš„å…±ç°é¢‘ç‡å’Œæ¨¡å¼ï¼Œå¯ä»¥æ­ç¤ºå‡ºè¯è¯­ä¹‹é—´çš„è¯­ä¹‰æˆ–è¯­æ³•å…³ç³»ã€‚

**æ­é…ï¼ˆCollocationï¼‰**ï¼šæ­é…æ˜¯æŒ‡ä¸¤ä¸ªæˆ–å¤šä¸ªè¯åœ¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­ç»å¸¸ä¸€èµ·å‡ºç°ã€‚ä¾‹å¦‚ï¼Œåœ¨è‹±è¯­ä¸­ï¼Œâ€œstrongâ€å¸¸å¸¸ä¸â€œteaâ€æ­é…ï¼Œè€Œâ€œpowerfulâ€åˆ™å¾ˆå°‘ä¸â€œteaâ€æ­é…ã€‚ç±»ä¼¼åœ°ï¼Œâ€œmake a decisionâ€å’Œâ€œtake a photoâ€ä¹Ÿæ˜¯å¸¸è§çš„æ­é…ã€‚

**æ­é…å¼ºåº¦ï¼ˆCollocational Strengthï¼‰**ï¼šæ­é…å¼ºåº¦æ˜¯è¡¡é‡è¯è¯­ä¹‹é—´å…±ç°å…³ç³»çš„ç´§å¯†ç¨‹åº¦ã€‚å®ƒé€šå¸¸é€šè¿‡ç»Ÿè®¡æ–¹æ³•è®¡ç®—ï¼Œæ¯”å¦‚äº’ä¿¡æ¯ï¼ˆMutual Informationï¼ŒMIï¼‰æˆ–tæ£€éªŒï¼ˆt-scoreï¼‰ç­‰ã€‚é«˜æ­é…å¼ºåº¦è¡¨æ˜è¿™äº›è¯è¯­ç»å¸¸ä¸€èµ·å‡ºç°ï¼Œå¹¶ä¸”æ¯”éšæœºå‡ºç°çš„å¯èƒ½æ€§å¤§å¾—å¤šã€‚

**æ­é…èŒƒå›´ï¼ˆCollocational Rangeï¼‰**ï¼šæ­é…èŒƒå›´æŒ‡çš„æ˜¯è¯è¯­åœ¨å¤šå¤§èŒƒå›´å†…ï¼ˆæ¯”å¦‚åœ¨å‰åå‡ ä¸ªè¯å†…ï¼‰å‡ºç°çš„é¢‘ç‡ã€‚ä¾‹å¦‚ï¼Œâ€œmakeâ€åœ¨å‰é¢ç´§è·Ÿç€â€œdecisionâ€æˆ–â€œmistakeâ€æ—¶ï¼Œæ˜¯ä¸€ä¸ªå¸¸è§çš„æ­é…ï¼Œè¿™ä¸ªèŒƒå›´é€šå¸¸ç§°ä¸ºâ€œwindow sizeâ€ã€‚

**æ­é…åˆ†æçš„åº”ç”¨**ï¼š
1. **è¯­è¨€å­¦ä¹ **ï¼š
   æ­é…åˆ†ææœ‰åŠ©äºè¯­è¨€å­¦ä¹ è€…ç†è§£å“ªäº›è¯è¯­ç»å¸¸ä¸€èµ·ä½¿ç”¨ï¼Œä»è€Œæé«˜è¯­è¨€çš„è‡ªç„¶æ€§å’Œæµåˆ©åº¦ã€‚ä¾‹å¦‚ï¼Œå­¦ä¹ è€…å¯ä»¥é€šè¿‡æ­é…åˆ†æäº†è§£â€œdo homeworkâ€æ¯”â€œmake homeworkâ€æ›´è‡ªç„¶ã€‚

2. **è¯å…¸ç¼–çº‚**ï¼š
   è¯å…¸ç¼–çº‚è€…ä½¿ç”¨æ­é…åˆ†ææ¥ç¡®å®šå“ªäº›è¯è¯­ç»„åˆåº”è¯¥è¢«åˆ—ä¸ºå›ºå®šæ­é…ï¼Œä»è€Œä¸ºä½¿ç”¨è€…æä¾›æ›´å‡†ç¡®çš„è¯è¯­ç”¨æ³•ä¿¡æ¯ã€‚

3. **è¯­ä¹‰åˆ†æ**ï¼š
   é€šè¿‡åˆ†æä¸€ä¸ªè¯çš„æ­é…ï¼Œå¯ä»¥æ¨æµ‹å…¶è¯­ä¹‰ã€‚ä¾‹å¦‚ï¼Œâ€œdarkâ€é€šå¸¸ä¸â€œnightâ€æ­é…ï¼Œè€Œâ€œdarkâ€ä¸â€œmoodâ€æ­é…æ—¶ï¼Œå¯ä»¥æš—ç¤ºâ€œmoodâ€æ˜¯æ¶ˆæçš„ã€‚

æˆ‘ä»¬éœ€è¦åŒºåˆ†ä»¥ä¸‹æ¦‚å¿µï¼š

**æ­é… (collocations)**ï¼šæŒ‡çš„æ˜¯æ˜¾è‘—å½¼æ­¤å¸å¼•ä¸”å¸¸å¸¸ä¸€èµ·å‡ºç°çš„å•è¯ï¼ˆä½†ä¸ä¸€å®šæ˜¯ç›¸é‚»çš„ï¼‰ï¼Œä¾‹å¦‚ *black* å’Œ *coffee*ã€‚

**nå…ƒè¯­æ³• (n-grams)**ï¼šæŒ‡çš„æ˜¯ç›¸é‚»çš„å•è¯ç»„åˆï¼Œæ¯”å¦‚ *This is*ã€*is a* å’Œ *a sentence*ï¼Œè¿™äº›åŒè¯ç»„ (bi-grams) ç»„æˆäº†å¥å­ *This is a sentence*ã€‚

&emsp;&emsp;è¿™æ ·çš„å•è¯é…å¯¹æˆ–ç»„åˆè¡¨ç°å‡ºä¸€å®šçš„è‡ªç„¶æ€§ï¼Œå¹¶ä¸”å€¾å‘äºå½¢æˆé‡å¤çš„æ¨¡å¼ã€‚å®ƒä»¬åœ¨è¯­è¨€ä¹ å¾—ã€å­¦ä¹ ã€æµåˆ©åº¦å’Œä½¿ç”¨ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¹¶ä¸”æœ‰åŠ©äºè‡ªç„¶å’Œæƒ¯ç”¨çš„æ€æƒ³è¡¨è¾¾ã€‚ä¸€ä¸ªå…¸å‹çš„æ­é… (collocation) ä¾‹å­æ˜¯ *Merry Christmas*ï¼Œå› ä¸º *merry* å’Œ *Christmas* æ¯”å•è¯éšæœºç»„åˆæ—¶æ›´é¢‘ç¹åœ°ä¸€èµ·å‡ºç°ã€‚å…¶ä»–æ­é…çš„ä¾‹å­åŒ…æ‹¬ *strong coffee*ã€*make a decision* æˆ– *take a risk*ã€‚å¯¹äºè¯­è¨€å­¦ä¹ è€…æ¥è¯´ï¼Œè¯†åˆ«å’Œç†è§£æ­é… (collocations) æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºè¿™å¯ä»¥å¢å¼ºä»–ä»¬ç”Ÿæˆåœ°é“ä¸”ç¬¦åˆè¯­å¢ƒçš„è¯­è¨€çš„èƒ½åŠ›ã€‚

&emsp;&emsp;è¯†åˆ«æ­é…çš„è¯å¯¹ï¼ˆ*w1* å’Œ *w2*ï¼Œå³æ­é… (collocations)ï¼‰å¹¶ç¡®å®šå…¶å…³è”å¼ºåº¦ï¼ˆè¡¡é‡å•è¯å½¼æ­¤å¸å¼•çš„å¼ºåº¦ï¼‰æ˜¯åŸºäºè¯å¯¹åœ¨åˆ—è”è¡¨ä¸­çš„å…±ç°é¢‘ç‡ï¼ˆè§ä¸‹è¡¨ï¼Œ*O* ä»£è¡¨è§‚å¯Ÿé¢‘ç‡ï¼‰ã€‚

&emsp;&emsp;æ­é…åˆ†ææ˜¯é€šè¿‡ç»Ÿè®¡æ–¹æ³•ç ”ç©¶è¯è¯­ä¹‹é—´çš„å…±ç°å…³ç³»ï¼Œæ­ç¤ºè¯­è¨€ä½¿ç”¨ä¸­çš„è§„å¾‹æ€§å’Œéšå«è¯­ä¹‰ã€‚å®ƒåœ¨è¯­è¨€ç ”ç©¶ã€æ•™è‚²å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚
```{r }
toks_corpus_detectives%>%
  #æå–åŒ…å«å¤§å†™å­—æ¯çš„ä¸“æœ‰åè¯
    tokens_select(pattern = "^[A-Z]", 
                  valuetype = "regex", 
                  case_insensitive = FALSE, 
                  padding = TRUE) %>%
    textstat_collocations(min_count = 5, 
                          size = 3, 
                          tolower = FALSE)



collocation.3wd <- textstat_collocations(corps.detectives, 
                                         size = 3, 
                                         tolower = FALSE)
```



&emsp;&emsp;ä»è¿™ä¸ªåˆ—è”è¡¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºå¦‚æœå•è¯ä¹‹é—´æ²¡æœ‰ä»»ä½•å¸å¼•æˆ–æ’æ–¥å…³ç³»æ—¶çš„æœŸæœ›é¢‘ç‡ï¼ˆè§ä¸‹è¡¨ï¼ŒE è¡¨ç¤ºæœŸæœ›é¢‘ç‡ï¼‰ã€‚

è§‚å¯Ÿé¢‘ç‡ (Observed Frequency)

|            | w2 å‡ºç° | w2 æœªå‡ºç° | åˆè®¡    |
|------------|---------|-----------|---------|
| w1 å‡ºç°    | O11     | O12       | R1      |
| w1 æœªå‡ºç°  | O21     | O22       | R2      |
| åˆè®¡       | C1      | C2        | N       |

æœŸæœ›é¢‘ç‡ (Expected Frequency)

|            | w2 å‡ºç°                                 | w2 æœªå‡ºç°                                | åˆè®¡    |
|------------|-----------------------------------------|------------------------------------------|---------|
| w1 å‡ºç°    | E11 = (R1 * C1) / N                     | E12 = (R1 * C2) / N                      | R1      |
| w1 æœªå‡ºç°  | E21 = (R2 * C1) / N                     | E22 = (R2 * C2) / N                      | R2      |
| åˆè®¡       | C1                                     | C2                                      | N       |

---

&emsp;&emsp;é€šè¿‡è¿™ä¸ªåˆ—è”è¡¨ï¼ŒæœŸæœ›é¢‘ç‡è¡¨ç¤ºåœ¨å•è¯ä¹‹é—´æ²¡æœ‰å¸å¼•æˆ–æ’æ–¥çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¸€èµ·å‡ºç°çš„é¢‘ç‡ã€‚

&emsp;&emsp;å…³è”åº¦é‡ä½¿ç”¨ä¸Šé¢åˆ—è”è¡¨ä¸­çš„é¢‘ç‡ä¿¡æ¯æ¥è¯„ä¼°å•è¯ä¹‹é—´çš„å¸å¼•æˆ–æ’æ–¥å¼ºåº¦ã€‚å› æ­¤ï¼Œå…³è”åº¦é‡æ˜¯ç”¨æ¥é‡åŒ–è¯è¯­åœ¨æ­é… (collocation) ä¸­å…³ç³»å¼ºåº¦å’Œæ˜¾è‘—æ€§çš„ç»Ÿè®¡æŒ‡æ ‡ã€‚è¿™äº›åº¦é‡å¸®åŠ©è¯„ä¼°ä¸¤ä¸ªå•è¯æ˜¯å¦æ¯”éšæœºå‡ºç°æ›´é¢‘ç¹åœ°ä¸€èµ·å‡ºç°ã€‚åœ¨æ­é…åˆ†æä¸­ï¼Œå¸¸ç”¨çš„å‡ ç§å…³è”åº¦é‡åŒ…æ‹¬ï¼š

&emsp;&emsp;**Griesçš„å…³è”åº¦é‡ (Griesâ€™ AM)**ï¼šGriesçš„AMï¼ˆGries 2022ï¼‰å¯èƒ½æ˜¯åŸºäºæ¡ä»¶æ¦‚ç‡çš„æœ€ä½³å…³è”åº¦é‡ã€‚å…³äºå…¶è®¡ç®—æ–¹å¼ï¼Œè¯·å‚è€ƒGries (2022)ã€‚ä¸å…¶ä»–å…³è”åº¦é‡ç›¸æ¯”ï¼Œå®ƒæœ‰ä¸‰ä¸ªä¸»è¦ä¼˜ç‚¹ï¼š
1. å®ƒè€ƒè™‘åˆ°å•è¯1å’Œå•è¯2ä¹‹é—´çš„å…³è”å¹¶éå¯¹ç§°çš„ï¼ˆå•è¯1å¯èƒ½æ¯”å•è¯2æ›´å¼ºçƒˆåœ°å¸å¼•å•è¯2ï¼Œåä¹‹äº¦ç„¶ï¼‰ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šï¼Œå®ƒéå¸¸ç±»ä¼¼äºÎ”Pã€‚
2. å®ƒä¸å—é¢‘ç‡å½±å“ï¼Œè€Œå…¶ä»–å…³è”åº¦é‡åˆ™ä¼šå—åˆ°å½±å“ï¼ˆè¿™æ˜¯ä¸€ä¸ªä¸¥é‡çš„é—®é¢˜ï¼Œå› ä¸ºå…³è”åº¦é‡åº”è¯¥åæ˜ å…³è”å¼ºåº¦è€Œä¸æ˜¯é¢‘ç‡ï¼‰ã€‚
3. å®ƒæ˜¯å½’ä¸€åŒ–çš„ï¼Œå› ä¸ºå®ƒè€ƒè™‘åˆ°ä¸åŒå…ƒç´ çš„å€¼èŒƒå›´ä¸åŒï¼ˆä¸€äº›å•è¯å¯ä»¥æœ‰éå¸¸é«˜çš„å€¼ï¼Œè€Œå…¶ä»–å•è¯åˆ™ä¸èƒ½ï¼‰ã€‚

&emsp;&emsp;**Î”P** (delta P)ï¼šÎ”Pï¼ˆEllis 2007ï¼›Gries 2013ï¼‰æ˜¯ä¸€ç§åŸºäºæ¡ä»¶æ¦‚ç‡çš„å…³è”åº¦é‡ï¼Œå®ƒåœ¨MSä¸­éšå« (Gries 2013, 141)ã€‚Î”Pæœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼šå®ƒè€ƒè™‘åˆ°å•è¯1å’Œå•è¯2ä¹‹é—´çš„å…³è”å¹¶éå¯¹ç§°çš„ï¼ˆå•è¯1å¯èƒ½æ¯”å•è¯2æ›´å¼ºçƒˆåœ°å¸å¼•å•è¯2ï¼Œåä¹‹äº¦ç„¶ï¼‰ï¼Œå¹¶ä¸”å®ƒä¸å—é¢‘ç‡å½±å“ï¼ˆè¿™æ˜¯ä¸€ä¸ªä¸¥é‡çš„é—®é¢˜ï¼Œå› ä¸ºå…³è”åº¦é‡åº”è¯¥åæ˜ å…³è”å¼ºåº¦è€Œä¸æ˜¯é¢‘ç‡ï¼‰ï¼ˆè§Gries 2022ï¼‰ã€‚

Î”ğ‘ƒ1 = ğ‘ƒ(ğ‘¤1|ğ‘¤2) = (ğ‘‚11 / ğ‘…1) âˆ’ (ğ‘‚21 / ğ‘…2)

Î”ğ‘ƒ2 = ğ‘ƒ(ğ‘¤2|ğ‘¤1) = (ğ‘‚11 / ğ¶1) âˆ’ (ğ‘‚21 / ğ¶2)

&emsp;&emsp;**é€ç‚¹äº’ä¿¡æ¯ (PMI)**ï¼šPMI è¡¡é‡ä¸¤ä¸ªå•è¯å…±åŒå‡ºç°çš„å¯èƒ½æ€§ï¼Œä¸å®ƒä»¬åˆ†åˆ«ç‹¬ç«‹å‡ºç°çš„å¯èƒ½æ€§ç›¸æ¯”ã€‚è¾ƒé«˜çš„PMIåˆ†æ•°è¡¨æ˜è¾ƒå¼ºçš„å…³è”ã€‚

PMI(ğ‘¤1,ğ‘¤2) = log2(ğ‘ƒ(ğ‘¤1âˆ©ğ‘¤2) / (ğ‘ƒ(ğ‘¤1) â‹… ğ‘ƒ(ğ‘¤2)))

&emsp;&emsp;**å¯¹æ•°ä¼¼ç„¶æ¯” (LLR)**ï¼šLLR å°†è§‚å¯Ÿåˆ°çš„å•è¯ç»„åˆå‡ºç°çš„å¯èƒ½æ€§ä¸åŸºäºå•è¯ä¸ªä½“é¢‘ç‡çš„æœŸæœ›å¯èƒ½æ€§è¿›è¡Œæ¯”è¾ƒã€‚è¾ƒé«˜çš„LLRå€¼è¡¨æ˜æ›´æ˜¾è‘—çš„å…³è”ï¼ˆå…¶ä¸­ğ‘‚ğ‘–æ˜¯è§‚å¯Ÿé¢‘ç‡ï¼Œğ¸ğ‘–æ˜¯æ¯ä¸ªç»„åˆçš„æœŸæœ›é¢‘ç‡ï¼‰ã€‚

LLR(ğ‘¤1,ğ‘¤2) = 2 âˆ‘(ğ‘‚ğ‘– âˆ’ ğ¸ğ‘–)^2 / ğ¸ğ‘–

&emsp;&emsp;**Diceç³»æ•°**ï¼šè¯¥åº¦é‡è€ƒè™‘äº†å•è¯çš„å…±ç°æƒ…å†µï¼Œè®¡ç®—ä¸¤ä¸ªå•è¯çš„é‡å ä¸å®ƒä»¬å„è‡ªé¢‘ç‡ä¹‹å’Œçš„æ¯”ç‡ã€‚Diceç³»æ•°çš„èŒƒå›´æ˜¯0åˆ°1ï¼Œå€¼è¶Šé«˜è¡¨æ˜å…³è”è¶Šå¼ºã€‚

Dice(ğ‘¤1,ğ‘¤2) = 2 Ã— freq(ğ‘¤1âˆ©ğ‘¤2) / (freq(ğ‘¤1) + freq(ğ‘¤2))

&emsp;&emsp;**å¡æ–¹æ£€éªŒ (Chi-Square)**ï¼šå¡æ–¹æ£€éªŒè¡¡é‡å•è¯å…±ç°çš„è§‚å¯Ÿé¢‘ç‡ä¸æœŸæœ›é¢‘ç‡ä¹‹é—´çš„å·®å¼‚ã€‚è¾ƒé«˜çš„å¡æ–¹å€¼è¡¨æ˜æ›´æ˜¾è‘—çš„å…³è”ï¼ˆå…¶ä¸­ğ‘‚ğ‘–æ˜¯è§‚å¯Ÿé¢‘ç‡ï¼Œğ¸ğ‘–æ˜¯æ¯ä¸ªç»„åˆçš„æœŸæœ›é¢‘ç‡ï¼‰ã€‚

ğœ’Â²(ğ‘¤1,ğ‘¤2) = âˆ‘(ğ‘‚ğ‘– âˆ’ ğ¸ğ‘–)^2 / ğ¸ğ‘–

&emsp;&emsp;**t-æ£€éªŒ (t-Score)**ï¼št-æ£€éªŒåŸºäºè§‚å¯Ÿé¢‘ç‡å’ŒæœŸæœ›é¢‘ç‡ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶è¿›è¡Œæ ‡å‡†å·®å½’ä¸€åŒ–ã€‚è¾ƒé«˜çš„t-åˆ†æ•°è¡¨æ˜è¾ƒå¼ºçš„å…³è”ã€‚

t-Score(ğ‘¤1,ğ‘¤2) = (freq(ğ‘¤1âˆ©ğ‘¤2) âˆ’ expected_freq(ğ‘¤1âˆ©ğ‘¤2)) / âˆšfreq(ğ‘¤1âˆ©ğ‘¤2)

&emsp;&emsp;**äº’ä¿¡æ¯ (MI)**ï¼šMI è¡¡é‡æ ¹æ®ä¸€ä¸ªå•è¯çš„å‡ºç°ï¼Œå¦ä¸€ä¸ªå•è¯å‡ºç°çš„ä¸ç¡®å®šæ€§å‡å°‘äº†å¤šå°‘ã€‚è¾ƒé«˜çš„MIå€¼è¡¨æ˜è¾ƒå¼ºçš„å…³è”ï¼ˆå…¶ä¸­ğ‘ƒ(ğ‘¤1âˆ©ğ‘¤2)æ˜¯è”åˆæ¦‚ç‡ï¼Œğ‘ƒ(ğ‘¤1)å’Œğ‘ƒ(ğ‘¤2)æ˜¯ä¸ªä½“æ¦‚ç‡ï¼‰ã€‚

MI(ğ‘¤1,ğ‘¤2) = log2(ğ‘ƒ(ğ‘¤1âˆ©ğ‘¤2) / (ğ‘ƒ(ğ‘¤1) â‹… ğ‘ƒ(ğ‘¤2)))

&emsp;&emsp;**æœ€å°æ•æ„Ÿåº¦ (Minimum Sensitivity, MS)**ï¼šå½“w1å’Œw2æ€»æ˜¯ä¸€èµ·å‡ºç°ä¸”ä»ä¸åˆ†å¼€æ—¶ï¼Œæœ€å°æ•æ„Ÿåº¦ä¸º1ã€‚å½“w1å’Œw2ä»æœªä¸€èµ·å‡ºç°æ—¶ï¼Œæœ€å°æ•æ„Ÿåº¦ä¸º0ã€‚è¾ƒé«˜çš„æœ€å°æ•æ„Ÿåº¦è¡¨ç¤ºä¸¤ä¸ªå•è¯åœ¨åŒè¯ç»„ä¸­çš„ä¾èµ–æ€§æ›´å¼ºï¼ˆPedersen 1998ï¼‰ã€‚

MS = ğ‘šğ‘–ğ‘›(ğ‘ƒ(ğ‘¤1|ğ‘¤2), ğ‘ƒ(ğ‘¤2|ğ‘¤1))

&emsp;&emsp;è¿™äº›å…³è”åº¦é‡å¸®åŠ©ç ”ç©¶äººå‘˜å’Œè¯­è¨€åˆ†æå¸ˆè¯†åˆ«æœ‰æ„ä¹‰ä¸”å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„æ­é…ï¼Œä»è€Œè¾…åŠ©ä»è¯­æ–™åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æé«˜è¯­è¨€å­¦ç ”ç©¶ä¸­æ­é…åˆ†æçš„å‡†ç¡®æ€§ã€‚

#### åŸºäºå¥å­æå–æ­é…

```{r }
# set options
options(stringsAsFactors = F)
options(scipen = 999)
options(max.print=1000)


# read in text
# text <- base::readRDS(url("https://slcladal.github.io/data/cdo.rda", "rb")) %>%
#   #åˆå¹¶ä¸åŒè¡Œ
#   paste0(collapse = " ") %>%
#   #é™¤å»å¤šä½™ç©ºæ ¼
#   stringr::str_squish() %>%
#   stringr::str_remove_all("- ")

# text %>% 
#   # concatenate the elements in the 'text' object
#   paste0(collapse = " ") %>%
#   # separate possessives and contractions
#   stringr::str_replace_all(fixed("'"), fixed(" '")) %>%
#   stringr::str_replace_all(fixed("â€™"), fixed(" '")) %>%
sentences =corps.detectives%>%
  # split text into sentences
  tokenizers::tokenize_sentences() %>%
  # unlist sentences
  unlist() %>%
  # remove non-word characters
  stringr::str_replace_all("\\W", " ") %>%
  stringr::str_replace_all("[^[:alnum:] ]", " ") %>%
  # remove superfluous white spaces
  stringr::str_squish() %>%
  # convert to lower case and save in 'sentences' object
  tolower() 

head(sentences)

# tokenize the 'sentences' data using quanteda package
coll_basic = sentences %>%
  quanteda::tokens() %>%
  # create a document-feature matrix (dfm) using quanteda
  quanteda::dfm() %>%
  # create a feature co-occurrence matrix (fcm) without considering trigrams
  quanteda::fcm(tri = FALSE)%>%
  # tidy the data using tidytext package
  tidytext::tidy() %>%
  # rearrange columns for better readability
  dplyr::relocate(term, document, count) %>%
  # rename columns for better interpretation
  dplyr::rename(w1 = 1,
                w2 = 2,
                O11 = 3) 

head(coll_basic)%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 

  # calculate the total number of observations (N)
colldf = coll_basic %>%  
  dplyr::mutate(N = sum(O11)) %>%
  # calculate R1, O12, and R2
  dplyr::group_by(w1) %>%
  dplyr::mutate(R1 = sum(O11),
                O12 = R1 - O11,
                R2 = N - R1) %>%
  dplyr::ungroup(w1) %>%
  # calculate C1, O21, C2, and O22
  dplyr::group_by(w2) %>%
  dplyr::mutate(C1 = sum(O11),
                O21 = C1 - O11,
                C2 = N - C1,
                O22 = R2 - O21) 
head(colldf )%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 

# reduce and complement data
colldf_redux = colldf %>%
   # determine Term
  dplyr::filter(w1 == "crime",
                # set minimum number of occurrences of w2
                (O11+O21) > 10,
                # set minimum number of co-occurrences of w1 and w2
                O11 > 5)  %>%
  dplyr::rowwise() %>%
  dplyr::mutate(E11 = R1 * C1 / N, 
                E12 = R1 * C2 / N,
                E21 = R2 * C1 / N, 
                E22 = R2 * C2 / N)  
head(colldf_redux )%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 

assoc_tb = colldf_redux %>%
  # determine number of rows
  dplyr::mutate(Rws = nrow(.)) %>%
    # work row-wise
    dplyr::rowwise() %>%
    # calculate fishers' exact test
    dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), 
                                                        ncol = 2, byrow = T))[1]))) %>%
  # extract AM
    # 1. bias towards top left
    dplyr::mutate(btl_O12 = ifelse(C1 > R1, 0, R1-C1),
                  btl_O11 = ifelse(C1 > R1, R1, R1-btl_O12),
                  btl_O21 = ifelse(C1 > R1, C1-R1, C1-btl_O11),
                  btl_O22 = ifelse(C1 > R1, C2, C2-btl_O12),
                  
    # 2. bias towards top right
                  btr_O11 = 0, 
                  btr_O21 = R1,
                  btr_O12 = C1,
                  btr_O22 = C2-R1) %>%
    
    # 3. calculate AM
    dplyr::mutate(upp = btl_O11/R1,
                  low = btr_O11/R1,
                  op = O11/R1) %>%
    dplyr::mutate(AM = op / upp) %>%
    
    # remove superfluous columns
    dplyr::select(-any_of(c("btr_O21", "btr_O12", "btr_O22", "btl_O12", 
                            "btl_O11", "btl_O21", "btl_O22", "btr_O11"))) %>%

    # extract x2 statistics
    dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %>%

    # extract association measures
    dplyr::mutate(phi = sqrt((X2 / N)),
                Dice = (2 * O11) / (R1 + C1),
                LogDice = log((2 * O11) / (R1 + C1)),
                MI = log2(O11 / E11),
                MS = min((O11/C1), (O11/R1)),
                t.score = (O11 - E11) / sqrt(O11),
                z.score = (O11 - E11) / sqrt(E11),
                PMI = log2( (O11 / N) / ((O11+O12) / N) * 
                              ((O11+O21) / N) ),
                DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)),
                DeltaP21 =  (O11 / (O11 + O21)) - (O21 / (O12 + O22)),
                DP = (O11 / R1) - (O21 / R2),
                LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5))  / ( (O12 + 0.5) * (O21 + 0.5) )),
                # calculate LL aka G2
                G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))) %>%

  # determine Bonferroni corrected significance
  dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws > .05 ~ "n.s.",
                                                 p / Rws > .01 ~ "p < .05*",
                                                 p / Rws > .001 ~ "p < .01**",
                                                 p / Rws <= .001 ~ "p < .001***",
                                                 T ~ "N.A.")) %>%
  
  # round p-value
    dplyr::mutate(p = round(p, 5)) %>%
    # filter out non significant results
    dplyr::filter(Sig_corrected != "n.s.",
                # filter out instances where the w1 and w2 repel each other
                E11 < O11) %>%
    # arrange by DeltaP12 (association measure)
    dplyr::arrange(-DeltaP12) %>%
    # remove superfluous columns
    dplyr::select(-any_of(c("TermCoocFreq", "AllFreq", "NRows", "O12", "O21", 
                            "O22", "R1", "R2", "C1", "C2", "E11", "E12", "E21",
                            "E22", "upp", "low", "op", "Rws"))) 

head(assoc_tb)%>%
  datatable(.,
  filter = "top",
  class = 'cell-border stripe hover compact'
) 

```

#### åŸºäºç›®æ ‡è¯æå–æ­é…

```{r}

kwic_words <- quanteda::tokens_select(tokens(corps.detectives,
                                             remove_numbers = TRUE, 
                                             remove_punct = TRUE), 
                                      pattern = "kill", 
                                      window = 5, 
                                      selection = "keep") %>%
  unlist() %>%
  # tabulate results
  table() %>%
  # convert into data frame
  as.data.frame() %>%
  # rename columns
  dplyr::rename(token = 1,
                n = 2) %>%
  # add a column with type
  dplyr::mutate(type = "kwic")

corpus_words <- corps.detectives %>%
  # tokenize the corpus files
  quanteda::tokens( remove_numbers = TRUE, 
                    remove_punct = TRUE) %>%
  # unlist the tokens to create a data frame
  unlist() %>%
  as.data.frame() %>%
  # rename the column to 'token'
  dplyr::rename(token = 1) %>%
  # group by 'token' and count the occurrences
  dplyr::group_by(token) %>%
  dplyr::summarise(n = n()) %>%
  # add column stating where the frequency list is 'from'
  dplyr::mutate(type = "corpus")

freq_df <- dplyr::left_join(corpus_words, kwic_words, by = c("token")) %>%
  # rename columns and select relevant columns
  dplyr::rename(corpus = n.x,
                kwic = n.y) %>%
  dplyr::select(-type.x, -type.y) %>%
  # replace NA values with 0 in 'corpus' and 'kwic' columns
  tidyr::replace_na(list(corpus = 0, kwic = 0))



stats_tb = freq_df %>%
  dplyr::filter(corpus > 0) %>%
  dplyr::mutate(corpus = as.numeric(corpus),
                kwic = as.numeric(kwic)) %>%
  dplyr::mutate(corpus= corpus-kwic,
                C1 = sum(kwic),
                C2 = sum(corpus),
                N = C1 + C2) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(R1 = corpus+kwic,
                R2 = N - R1,
                O11 = kwic,
                O12 = R1-O11,
                O21 = C1-O11,
                O22 = C2-O12) %>%
  dplyr::mutate(E11 = (R1 * C1) / N,
                E12 = (R1 * C2) / N,
                E21 = (R2 * C1) / N,
                E22 = (R2 * C2) / N) %>%
  dplyr::select(-corpus, -kwic) 

assoc_tb2 = stats_tb %>%
  # determine number of rows
  dplyr::mutate(Rws = nrow(.)) %>%
    # work row-wise
    dplyr::rowwise() %>%
    # calculate fishers' exact test
    dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), 
                                                        ncol = 2, byrow = T))[1]))) %>%

      # extract AM
    # 1. bias towards top left
    dplyr::mutate(btl_O12 = ifelse(C1 > R1, 0, R1-C1),
                  btl_O11 = ifelse(C1 > R1, R1, R1-btl_O12),
                  btl_O21 = ifelse(C1 > R1, C1-R1, C1-btl_O11),
                  btl_O22 = ifelse(C1 > R1, C2, C2-btl_O12),
                  
    # 2. bias towards top right
                  btr_O11 = 0, 
                  btr_O21 = R1,
                  btr_O12 = C1,
                  btr_O22 = C2-R1) %>%
    
    # 3. calculate AM
    dplyr::mutate(upp = btl_O11/R1,
                  low = btr_O11/R1,
                  op = O11/R1) %>%
    dplyr::mutate(AM = op / upp) %>%
    
    # remove superfluous columns
    dplyr::select(-any_of(c("btr_O21", "btr_O12", "btr_O22", "btl_O12", 
                            "btl_O11", "btl_O21", "btl_O22", "btr_O11"))) %>% 
  
    # extract x2 statistics
    dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %>%
    # extract expected frequency
    dplyr::mutate(Exp = E11) %>%

    # extract association measures
    dplyr::mutate(phi = sqrt((X2 / N)),
                MS = min((O11/C1), (O11/R1)),
                Dice = (2 * O11) / (R1 + C1),
                LogDice = log((2 * O11) / (R1 + C1)),
                MI = log2(O11 / E11),
                t.score = (O11 - E11) / sqrt(O11),
                z.score = (O11 - E11) / sqrt(E11),
                PMI = log2( (O11 / N) / ((O11+O12) / N) * 
                              ((O11+O21) / N) ),
                DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)),
                DeltaP21 =  (O11 / (O11 + O21)) - (O21 / (O12 + O22)),
                DP = (O11 / R1) - (O21 / R2),
                LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5))  / ( (O12 + 0.5) * (O21 + 0.5) )),
                # calculate LL aka G2
                G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))) %>%
  
  # determine Bonferroni corrected significance
  dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws > .05 ~ "n.s.",
                                                 p / Rws > .01 ~ "p < .05*",
                                                 p / Rws > .001 ~ "p < .01**",
                                                 p / Rws <= .001 ~ "p < .001***",
                                                 T ~ "N.A.")) %>%  
  
  # round p-value
    dplyr::mutate(p = round(p, 5)) %>%
    # filter out non significant results
    dplyr::filter(Sig_corrected != "n.s.",
                # filter out instances where the w1 and w2 repel each other
                E11 < O11) %>%
    # arrange by phi (association measure)
    dplyr::arrange(-DeltaP12) %>%
    # remove superfluous columns
    dplyr::select(-any_of(c("TermCoocFreq", "AllFreq", "NRows", "O12", "O21", 
                            "O22", "R1", "R2", "C1", "C2", "E11", "E12", "E21",
                            "E22", "upp", "low", "op", "Rws"))) 

```

```{r}
# sort the assoc_tb2 data frame in descending order based on the 'phi' column
assoc_tb2 %>%
  dplyr::arrange(-phi) %>%
  # select the top 20 rows after sorting
  head(20) %>%
  # create a ggplot with 'token' on the x-axis (reordered by 'phi') and 'phi' on the y-axis
  ggplot(aes(x = reorder(token, phi, mean), y = phi)) +
  # add a scatter plot with points representing the 'phi' values
  geom_point() +
  # flip the coordinates to have horizontal points
  coord_flip() +
  # set the theme to a basic white and black theme
  theme_bw() +
  # set the x-axis label to "Token" and y-axis label to "Association strength (phi)"
  labs(x = "Token", y = "Association strength (phi)")
```


```{r}
# sort the assoc_tb2 data frame in descending order based on the 'phi' column
assoc_tb2 %>%
  dplyr::arrange(-phi) %>%
  # select the top 20 rows after sorting
  head(20) %>%
  # create a ggplot with 'token' on the x-axis (reordered by 'phi') and 'phi' on the y-axis
  ggplot(aes(x = reorder(token, phi, mean), y = phi, label = phi)) +
  # add a bar plot using the 'phi' values
  geom_bar(stat = "identity") +
  # add text labels above the bars with rounded 'phi' values
  geom_text(aes(y = phi - 0.005, label = round(phi, 3)), color = "white", size = 3) + 
  # flip the coordinates to have horizontal bars
  coord_flip() +
  # set the theme to a basic white and black theme
  theme_bw() +
  # set the x-axis label to "Token" and y-axis label to "Association strength (phi)"
  labs(x = "Token", y = "Association strength (phi)")
```

##### Dendrograms

```{r}
# sort the assoc_tb2 data frame in descending order based on the 'phi' column
top20colls <- assoc_tb2 %>%
  dplyr::arrange(-phi) %>%
  # select the top 20 rows after sorting
  head(20) %>%
  # extract the 'token' column 
  dplyr::pull(token)
# inspect the top 20 tokens with the highest 'phi' values
top20colls

# tokenize the 'sentences' data using quanteda package
keyword_fcm <- sentences %>%
  quanteda::tokens() %>%
  # create a document-feature matrix (dfm) from the tokens
  quanteda::dfm() %>%
  # select features based on 'top20colls' and the term "selection" pattern
  quanteda::dfm_select(pattern = c(top20colls, "kill")) %>%
  # Create a symmetric feature co-occurrence matrix (fcm) 
  quanteda::fcm(tri = FALSE)

# inspect the first 6 rows and 6 columns of the resulting fcm
keyword_fcm[1:6, 1:6]

# create a hierarchical clustering object using the distance matrix of the fcm as data
hclust(dist(keyword_fcm),     
       # use ward.D as linkage method
       method="ward.D2") %>% 
  # generate visualization (dendrogram)
  ggdendrogram() +              
  # add title
  ggtitle("20 most strongly collocating terms of 'kill'")  

```

#### networkplot

```{r}
# create a network plot using the fcm
quanteda.textplots::textplot_network(keyword_fcm,
                                     # set the transparency of edges to 0.8 for visibility
                                     edge_alpha = 0.8,
                                     # set the color of edges to gray
                                     edge_color = "gray",
                                     # set the size of edges to 2 for better visibility
                                     edge_size = 2,
                                     # adjust the size of vertex labels 
                                     # based on the logarithm of row sums of the fcm
                                     vertex_labelsize = log(rowSums(keyword_fcm)))
```






### è¯æ±‡å¤šæ ·æ€§

&emsp;&emsp;è¯æ±‡ä¸°å¯Œåº¦ï¼ˆLexical Richnessï¼‰æ˜¯æŒ‡åœ¨è¯­è¨€å­¦å’Œè¯­è¨€ä¹ å¾—ç ”ç©¶ä¸­ï¼Œç”¨æ¥è¡¡é‡ä¸€ä¸ªäººä½¿ç”¨è¯­è¨€æ—¶ï¼Œæ‰€è¡¨ç°å‡ºçš„è¯æ±‡å¤šæ ·æ€§å’Œå¤æ‚ç¨‹åº¦çš„æŒ‡æ ‡ã€‚å®ƒåæ˜ äº†ä¸€ä¸ªäººè¯­è¨€è¡¨è¾¾çš„å¹¿åº¦å’Œæ·±åº¦ï¼Œå…·ä½“è€Œè¨€ï¼Œå°±æ˜¯åœ¨ä¸€ä¸ªè¯­è¨€ç‰‡æ®µä¸­ï¼Œä½¿ç”¨å¤šå°‘ä¸åŒçš„è¯æ±‡ä»¥åŠè¿™äº›è¯æ±‡çš„å¤æ‚æ€§ã€‚

è¯æ±‡ä¸°å¯Œåº¦é€šå¸¸é€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥è¡¡é‡ï¼š

1. **ç±»å‹-æ ‡è®°æ¯”ç‡ï¼ˆType-Token Ratio, TTRï¼‰**ï¼šè¿™æ˜¯æœ€å¸¸è§çš„è¯æ±‡ä¸°å¯Œåº¦è¡¡é‡æŒ‡æ ‡ã€‚ç±»å‹æŒ‡çš„æ˜¯ä¸åŒçš„è¯æ±‡ç§ç±»ï¼Œè€Œæ ‡è®°æŒ‡çš„æ˜¯è¯æ±‡çš„æ€»æ•°ã€‚ç±»å‹-æ ‡è®°æ¯”ç‡å°±æ˜¯ä¸åŒè¯æ±‡ç§ç±»æ•°ä¸æ€»è¯æ±‡æ•°çš„æ¯”å€¼ã€‚TTR æ¯”å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºè¯æ±‡ä¸°å¯Œåº¦è¶Šé«˜ï¼Œä½†å®ƒä¼šå—åˆ°æ–‡æœ¬é•¿åº¦çš„å½±å“ã€‚

2. **è¯æ±‡å¯†åº¦ï¼ˆLexical Densityï¼‰**ï¼šè¡¡é‡ä¸€ä¸ªæ–‡æœ¬ä¸­å®è¯ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯ï¼‰æ‰€å çš„æ¯”ä¾‹ã€‚é«˜è¯æ±‡å¯†åº¦æ„å‘³ç€æ–‡æœ¬ä¸­ä½¿ç”¨äº†è¾ƒå¤šçš„å®è¯ï¼Œç›¸å¯¹å†…å®¹è¾ƒä¸ºä¸°å¯Œã€‚

3. **å¹³å‡è¯é•¿ï¼ˆMean Word Lengthï¼‰**ï¼šå¹³å‡è¯é•¿å¯ä»¥ä½œä¸ºè¯æ±‡å¤æ‚æ€§çš„ä¸€ç§è¡¡é‡æ–¹å¼ï¼Œé€šå¸¸è¾ƒé•¿çš„è¯æ±‡æ„å‘³ç€è¾ƒé«˜çš„è¯æ±‡ä¸°å¯Œåº¦ã€‚

4. **ç¨€æœ‰è¯æ±‡æ¯”ä¾‹ï¼ˆProportion of Rare Wordsï¼‰**ï¼šè¿™ä¸ªæŒ‡æ ‡è¡¡é‡çš„æ˜¯åœ¨æ–‡æœ¬ä¸­ä½¿ç”¨çš„ç¨€æœ‰æˆ–ä¸å¸¸è§è¯æ±‡çš„æ¯”ä¾‹ã€‚ç¨€æœ‰è¯æ±‡ä½¿ç”¨è¶Šå¤šï¼Œè¯æ±‡ä¸°å¯Œåº¦é€šå¸¸è¶Šé«˜ã€‚

5. **Då€¼ï¼ˆD-measureï¼‰**ï¼šè¿™æ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„æŒ‡æ ‡ï¼Œè¯•å›¾é€šè¿‡å»ºæ¨¡è§£å†³ TTR ä¸­çš„æ–‡æœ¬é•¿åº¦å½±å“é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªæ›´ç¨³å®šçš„è¯æ±‡ä¸°å¯Œåº¦è¡¡é‡æ–¹å¼ã€‚

**è¯æ±‡ä¸°å¯Œåº¦**çš„åˆ†æåœ¨äºŒè¯­ä¹ å¾—ã€è¯­è¨€èƒ½åŠ›è¯„ä¼°ã€å¿ƒç†è¯­è¨€å­¦ç­‰é¢†åŸŸä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡è¡¡é‡ä¸€ä¸ªäººè¯æ±‡ä¸°å¯Œåº¦ï¼Œå¯ä»¥åˆ¤æ–­å…¶è¯­è¨€èƒ½åŠ›ã€è¡¨è¾¾èƒ½åŠ›ã€è®¤çŸ¥æ°´å¹³ï¼Œç”šè‡³å¯ä»¥ç”¨æ¥è¯Šæ–­è¯­è¨€éšœç¢æˆ–è®¤çŸ¥åŠŸèƒ½è¡°é€€ã€‚

```{r }

lexical.diversity <- textstat_lexdiv(dfm_detectives, measure = "all")

lexical.diversity.df = lexical.diversity%>%
  as.data.frame()%>%
  # mutate(id = 1:n())%>%
  # filter(id >1 )%>%
  select("document","TTR","C","R","K","D","Vm")%>%
  gather("measures", "values", -c("document"))%>%
  mutate(measures = as.factor(measures),
         measures = fct_relevel(measures, "TTR","C","R","K","D","Vm"))

lexical.diversity.df

## ç§»åŠ¨çª—å£è¯æ±‡ä¸°å¯Œåº¦
lexical.mattr <- textstat_lexdiv(tokens(corps.detectives), 
                                 measure = "MATTR", MATTR_window = 500)

lexical.mattr
```

### æ–‡æœ¬å¯è¯»æ€§

```{r }
readability <- textstat_readability(corps.detectives, measure = "all")
readability.df = readability %>%
  select("document","ARI","Flesch","FOG",
         "Coleman.Liau.short","Dale.Chall","Spache")%>%
  gather("measures", "values", -c("document"))%>%
  mutate(measures = as.factor(measures),
         measures = fct_relevel(measures, "ARI","Flesch","FOG",
                                "Coleman.Liau.short","Dale.Chall","Spache"))

```

### æ–‡æœ¬ä¿¡æ¯ç†µ
&emsp;&emsp;æ–‡æœ¬ä¿¡æ¯ç†µï¼ˆTextual Information Entropyï¼‰æ˜¯ä¿¡æ¯ç†è®ºä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œç”¨æ¥è¡¡é‡æ–‡æœ¬ä¸­ä¿¡æ¯çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚å®ƒæ˜¯ç”±å…‹åŠ³å¾·Â·é¦™å†œï¼ˆClaude Shannonï¼‰åœ¨1948å¹´æå‡ºçš„ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºè¯­è¨€å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œä¿¡æ¯è®ºç­‰é¢†åŸŸã€‚

**ä¿¡æ¯ç†µçš„åŸºæœ¬æ¦‚å¿µï¼š**

1. **ä¸ç¡®å®šæ€§ï¼ˆUncertaintyï¼‰**ï¼šä¿¡æ¯ç†µåæ˜ äº†ä¸€ä¸ªç³»ç»Ÿçš„ä¸ç¡®å®šæ€§ç¨‹åº¦ã€‚å¦‚æœæŸä¸ªäº‹ä»¶çš„å‘ç”Ÿæ˜¯å®Œå…¨ç¡®å®šçš„ï¼Œé‚£ä¹ˆå…¶ä¿¡æ¯ç†µä¸ºé›¶ï¼›ç›¸åï¼Œå¦‚æœäº‹ä»¶çš„å‘ç”Ÿéå¸¸ä¸ç¡®å®šï¼Œå…¶ä¿¡æ¯ç†µåˆ™è¾ƒé«˜ã€‚

2. **æ¦‚ç‡åˆ†å¸ƒï¼ˆProbability Distributionï¼‰**ï¼šä¿¡æ¯ç†µçš„è®¡ç®—åŸºäºäº‹ä»¶å‡ºç°çš„æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ–‡æœ¬å¤„ç†ä¸­ï¼Œè¿™é€šå¸¸æ„å‘³ç€è®¡ç®—æŸä¸ªå­—æ¯ã€å•è¯æˆ–ç¬¦å·åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­å‡ºç°çš„æ¦‚ç‡ã€‚

3. **é¦™å†œç†µï¼ˆShannon Entropyï¼‰**ï¼šè¿™æ˜¯ä¿¡æ¯ç†µæœ€å¸¸ç”¨çš„å½¢å¼ï¼Œè¡¨ç¤ºä¸ºï¼š
   \[
   H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
   \]
   å…¶ä¸­ï¼Œ\( H(X) \) æ˜¯ä¿¡æ¯ç†µï¼Œ\( P(x_i) \) æ˜¯äº‹ä»¶ \( x_i \) å‘ç”Ÿçš„æ¦‚ç‡ï¼Œ\( n \) æ˜¯å¯èƒ½çš„äº‹ä»¶æ€»æ•°ã€‚å¯¹äºæ–‡æœ¬ä¿¡æ¯ç†µï¼Œäº‹ä»¶é€šå¸¸æ˜¯å­—ç¬¦ã€å•è¯æˆ–å…¶ä»–æ–‡æœ¬å…ƒç´ çš„å‡ºç°ã€‚

**æ–‡æœ¬ä¿¡æ¯ç†µåœ¨è¯­è¨€å­¦ä¸­çš„åº”ç”¨ï¼š**

&emsp;&emsp;é€šè¿‡è®¡ç®—æ–‡æœ¬çš„ç†µï¼Œå¯ä»¥åˆ†ææ–‡æœ¬çš„è¯­è¨€å¤æ‚æ€§ã€‚ç†µå€¼è¶Šé«˜ï¼Œæ„å‘³ç€æ–‡æœ¬çš„ä¿¡æ¯é‡å¤§ã€å¤æ‚æ€§é«˜ï¼›ç†µå€¼è¶Šä½ï¼Œæ„å‘³ç€æ–‡æœ¬æ›´ä¸ºç®€å•ã€ä¿¡æ¯é‡è¾ƒå°‘ã€‚


```{r }

textstat_entropy(dfm_detectives)

```


### æ–‡æœ¬æƒ…æ„Ÿåˆ†æ


```{r }

library(dplyr)
library(stringr)
library(tidytext)

tidy_books <- detectives.raw %>%
  unnest_sentences(sentence, text)%>%
  group_by(doc_id) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, sentence)



tidy.sent = tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(doc_id, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

library(ggplot2)

ggplot(tidy.sent , aes(index, sentiment, fill = doc_id)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~doc_id, ncol = 2, scales = "free_x")

# è¿›ä¸€æ­¥çœ‹çœ‹å“ªäº›ç§¯ææˆ–è€…æ¶ˆæçš„è¯
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
   group_by(doc_id) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(doc_id~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)


```

## å¥å­åˆ†æ

### ä¾å­˜å¥æ³•æ ‡æ³¨


```{r cache=TRUE}



#Load the model
# First load the model which you have downloaded or which you have stored somewhere on disk.
# 
# udmodel <- udpipe_download_model(language = "chinese")

## Either give a file in the current working directory
udmodel_eng <- udpipe_load_model(file = "data/ch6/english-ewt-ud-2.5-191206.udpipe")
# udmodel_chn <- udpipe_load_model(file = "chinese-gsd-ud-2.5-191206.udpipe")



# dee.raw <- scan(file="judge_dee/01.txt", 
#                 what="character", 
#                 sep="\n")

dee.raw = detectives.raw %>%
  filter(doc_id == "Dee.txt")
  
dee.depd <- udpipe_annotate(udmodel_eng, x = dee.raw$text)


dee.df <- as.data.frame(dee.depd)

table(dee.df$upos)
```

### è¯æ€§ç»Ÿè®¡
```{r cache=TRUE}
library(lattice)
stats <- txt_freq(dee.df$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "cadetblue", 
         main = "UPOS (Universal Parts of Speech)\n frequency of occurrence", 
         xlab = "Freq")
```

### ä¾å­˜å¥æ³•ç»“æ„
```{r}
library(textplot)
# generate dependency plot
dplot <- textplot_dependencyparser(filter(dee.df, doc_id == "doc1",
                                                    paragraph_id == 2, 
                                                    sentence_id == 2), size = 3) 
# show plot
dplot
```


```{r}
stats <- subset(dee.df, upos %in% c("NOUN")) 
stats <- udpipe::txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")
```

### åŸºäºè¯æ€§çš„æ­é…æå–

```{r}
## Using RAKE
stats <- keywords_rake(x = dee.df, term = "lemma", group = "doc_id", 
                       relevant = dee.df$upos %in% c("NOUN", "ADJ"))
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "cadetblue", 
         main = "Keywords identified by RAKE", 
         xlab = "Rake")
```


```{r}
## Using Pointwise Mutual Information Collocations
dee.df$word <- tolower(dee.df$token)
stats <- keywords_collocation(x = dee.df, term = "word", group = "doc_id")
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ pmi, data = head(subset(stats, freq > 3), 20), col = "cadetblue", 
         main = "Keywords identified by PMI Collocation", 
         xlab = "PMI (Pointwise Mutual Information)")
```


```{r}
## Using a sequence of POS tags (noun phrases / verb phrases)
dee.df$phrase_tag <- as_phrasemachine(dee.df$upos, type = "upos")
stats <- keywords_phrases(x = dee.df$phrase_tag, term = tolower(dee.df$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)
stats <- subset(stats, ngram > 1 & freq > 3)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")




```


```{r}
library(igraph)
library(ggraph)
library(ggplot2)

## Co-occurrences allow to see how words are used either in the same sentence or next to each other. 
# This R package make creating co-occurrence graphs using the relevant Parts of Speech tags as easy as possible.

# Nouns / adjectives used in same sentence

cooc <- cooccurrence(x = subset(dee.df, upos %in% c("NOUN", "ADJ")), 
                     term = "lemma", 
                     group = c("doc_id", "paragraph_id", "sentence_id"))


wordnetwork <- head(cooc, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "blue") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  theme(legend.position = "none") +
  labs(title = "Cooccurrences within sentence", subtitle = "Nouns & Adjective")
```


```{r}
cooc <- cooccurrence(dee.df$lemma, 
                     relevant = dee.df$upos %in% c("NOUN", "ADJ"), 
                     skipgram = 1)
head(cooc)

wordnetwork <- head(cooc, 15)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc)) +
  geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
  theme_graph(base_family = "Arial Narrow") +
  labs(title = "Words following one another", subtitle = "Nouns & Adjective")
```


```{r}
dee.df$id <- unique_identifier(dee.df, fields = c("sentence_id", "doc_id"))
dtm <- subset(dee.df, upos %in% c("NOUN", "ADJ"))
dtm <- document_term_frequencies(dtm, document = "id", term = "lemma")
dtm <- document_term_matrix(dtm)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 5)
termcorrelations <- dtm_cor(dtm)
y <- as_cooccurrence(termcorrelations)
y <- subset(y, term1 < term2 & abs(cooc) > 0.2)
y <- y[order(abs(y$cooc), decreasing = TRUE), ]
head(y)

```


```{r}
## Define the identifier at which we will build a topic model
dee.df$topic_level_id <- unique_identifier(dee.df, fields = c("doc_id", "paragraph_id", "sentence_id"))
## Get a data.frame with 1 row per id/lemma
dtf <- subset(dee.df, upos %in% c("NOUN"))
dtf <- document_term_frequencies(dtf, document = "topic_level_id", term = "lemma")
head(dtf)

## Create a document/term/matrix for building a topic model
dtm <- document_term_matrix(x = dtf)
## Remove words which do not occur that much
dtm_clean <- dtm_remove_lowfreq(dtm, minfreq = 5)
head(dtm_colsums(dtm_clean))
```


```{r message=FALSE }
############################################################

# Use dependency parsing output to get the nominal subject and the adjective of it
stats <- merge(dee.df, dee.df, 
               by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
               by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
               all.x = TRUE, all.y = FALSE, 
               suffixes = c("", "_parent"), sort = FALSE)

stats <- subset(stats, dep_rel %in% "nsubj" & upos %in% c("NOUN") & upos_parent %in% c("ADJ"))

stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")
stats <- txt_freq(stats$term)

library(wordcloud)
wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, max.words = 100,
          random.order = FALSE, colors = c("#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02"))



```

```{r}
# install.packages("devtools")
# install.packages("devtools")
# devtools::install_github("timmarchand/dtagger")

# library(dtagger)
# ## basic example code
# dtag.df = dtag_directory(path = "data/ch6/detectives", 
#                          n = NULL, ST = FALSE, deflated = TRUE)
# 
# 
#  # Example text:
#  text <- "This is an example sentence to be tagged"
#  # Example speech, tokenized:
#  speech <- c("I","don't", "know" ,  "erm" ,",", "whether" , "to" ,
#  "include" ,"hesitation" , "markers", ".")
#  # Initiate udpipe model
#  init_udpipe_model()
#  
#  udmodel_eng <- udpipe_load_model(file = "data/ch6/english-ewt-ud-2.5-191206.udpipe")
#  # Tag text
#  add_st_tags( udmodel_eng, text)
#  # Tag speech
#  add_st_tags(speech, st_hesitation = TRUE, tokenized = TRUE)
# 

```






