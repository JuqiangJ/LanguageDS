[["index.html", "语言数据科学教程 Chapter 1 第一章 数据科学简介 1.1 数据科学发展简史 1.2 数据科学家 1.3 数据科学工作流程", " 语言数据科学教程 陈居强 2024-09-08 Chapter 1 第一章 数据科学简介 欢迎关注我的个人主页： 1.1 数据科学发展简史 1.2 数据科学家 1.3 数据科学工作流程 "],["第二章-r语言简介.html", "Chapter 2 第二章 R语言简介 2.1 R语言发展简史 2.2 R、R-Studio、R软件包的安装简介 2.3 R语言的数据结构", " Chapter 2 第二章 R语言简介 2.1 R语言发展简史 2.2 R、R-Studio、R软件包的安装简介 R语言的安装 使用这个链接https://cloud.r-project.org/来下载R语言软件。根据你的电脑操作系统，选择相应的版本。 R语言可以在计算机上通过终端命令直接运行，也有自己的图形界面。但是RStudio 是最适用于初学者的R 编程的集成开发环境(integrated development environment，IDE)。使用者可以从 http://www.rstudio.com/download 下载并安装。其中免费版已经可以满足数据科学的需求了。 它提供了许多功能，如代码编辑器、调试器、数据可视化和包管理器等，使得R语言的学习和使用变得更加方便和高效。R-Studio被广泛用于数据科学、统计分析、机器学习和数据可视化等领域。 不同软件包使用：https://github.com/JuqiangJ/cheatsheets 图2.1 R-studio初始化 首先，初始界面有3个区域。在图中左侧区域是输入和运行代码区域，右侧上部的区域目前选中的是environment也就是环境变量的显示窗口。当前并没有输入任何代码，所以环境变量的窗口是空白。在右侧下部，对窗口有5个不同的按键：文件（File），制图（Plots），程序包（Packages），帮助（Help），视图（Viewer）。目前选中的是作图窗口。 在代码运行窗口，我们可以输入命令，然后按回车键即可运行命令。如果我们想要将命令写成脚本，方便重复使用，修改，那需要新建一个脚本文件。 我们推荐两种方式书写脚本，一种是把脚本文件格式，另外一种是Rmarkdown文件格式。 脚本文件格式 图2.2 新建脚本 当我们新建了一个脚本文件，就会出现一个窗口。原先的代码运行窗口，变换到了左边的下部，而在左边上部出现了一个新的窗口。这就是脚本书写窗口。在这里书写的脚本，我们可以通过选中来运行。 安装一个软件包package install.packages(&quot;tidyverse&quot;) ## Error in install.packages : Updating loaded packages 2.3 R语言的数据结构 在R中，基本的数据结构有向量（vector），数组（array），列表（list），数据框（data frame）和因子（factor）。 2.3.1 向量（vector） 向量是一种最基本的一维数据结构，可以包含相同类型的元素（data element）。元素的类型有数值型、字符串型、布尔型。可以通过使用c()函数创建向量，并且可以使用一系列基本函数索引、访问和修改向量中的元素。 #创建一个包含整数的向量 vector &lt;- c(1, 2, 3, 4, 5) # 创建不同类型的向量 # 数值型 a = 8:17 b &lt;- c(9, 10, 100, 38) # 布尔型 c = c (TRUE, FALSE, TRUE, FALSE) c = c (T, F, T, F) # 字符型 d = c (&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;) # 改变向量的类型 as.vector(b, mode = &quot;character&quot;) 值得注意的是，如果不同类型的数据被放入同一个向量，数据类型会发生改变。数值型数据会强制改变为字符型数据或布尔型数据。 e = c(9,10, &quot;ab&quot;, &quot;cd&quot;) f = c(10, 11, T, F) 除了单个输入之外，我们可以借助R语言的一些函数，批量生成向量中的元素。 A = 9:20 + 1 # A 是一个从9到20的向量，并且每个元素都加上了1。 B = seq(1, 10)# B 是一个从1到10的向量，其中的数字是连续的。 C = seq(1, 20, by = 2)#C 是一个从1到20的向量，步长为2，也就是只包含奇数。 D = rep(5, 4)# D 是一个包含了4个5的向量。 E = rep(c(1, 2, 3), 4)# E 是一个重复了4次的向量，其中包含了1、2、3这三个元素。 G = rep(c(1, 2, 3), each = 4)# G 是一个重复了4次的向量，其中每个元素都重复了4次，即先重复1四次，然后重复2四次，最后重复3四次。 R语言中的一些基本函数，可以对包含不同类型数据的向量进行操作。 #length(): 返回向量的长度。 vector &lt;- c(1, 2, 3, 4, 5) length(vector) # is.na(): 检查向量中是否存在缺失值（NA）。 vector&lt;- c(1, NA, 3, NA, 5) is.na(vector) # which(): 返回满足条件的元素在向量中的索引。 vector&lt;- c(1, 2, 6, 7, 8) which(vector &gt; 3) # sample(): 随机从向量中抽取元素。可以用于实验随机抽样。 vector &lt;- c(1:100) sample(vector, size = 3) 针对数值型向量的简单计算函数 # sum(): 计算向量中所有元素的和。 vector &lt;- c(1:50) sum(vector) # mean(): 计算向量的平均值。 mean(vector) # median(): 计算向量的中位数。 median(vector) # min(): 返回向量中的最小值。 min(vector) # max(): 返回向量中的最大值。 max(vector) # sort(): 将向量中的元素按升序排序。 sort(vector) # rev(): 反转向量中元素的顺序。 rev(vector) # unique(): 返回向量中唯一的元素。 vector = c(1,2,33,4,4,4,5,6,345) unique(vector) # range(): 返回向量的取值范围（最小值和最大值）。 range(vector ) quantile(vector) round(sd(vector), 2) 针对字符串型向量的函数 # paste(): 将向量中的元素连接为一个字符串。 vector &lt;- c(&quot;我&quot;, &quot;爱&quot;, &quot;XXX!&quot;) paste(vector, collapse = &quot; &quot;) # tolower(): 转换向量中的字符为小写。 vector &lt;- c(&quot;HELLO&quot;, &quot;WORLD&quot;, &quot;!&quot;) tolower(vector) # toupper(): 转换向量中的字符为大写。 vector &lt;- c(&quot;hello&quot;, &quot;world&quot;, &quot;!&quot;) toupper(vector) # grep(): 在向量中搜索满足条件的模式，并返回其索引。 vector &lt;- c(&quot;苹果&quot;, &quot;香蕉&quot;, &quot;胡萝卜&quot;, &quot;橙子&quot;) grep(&quot;果&quot;, vector) 2.3.2 列表（list） 列表是一种可以包含不同类型的元素的数据结构。列表可以使用list()函数创建，可以使用索引或元素名称来访问和修改列表中的元素。 # 创建一个包含整数、字符和向量的列表： my_list &lt;- list(1, &quot;a&quot;, c(2, 3, 4)) my_list 列表的简单操作 # unlist() 函数：将list转换为向量。 my_list &lt;- list(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;) unlist(my_list) # lapply()：对list中的每个元素应用一个函数。 my_list &lt;- list(1:3, 4:6, 7:9) lapply(my_list, mean) # sapply()：对list中的每个元素应用一个函数，并将结果简化为向量。 my_list &lt;- list(1:3, 4:6, 7:9) sapply(my_list, mean) 在建立回归模型后，可以将结果存储在一个列表中。 # 这行代码使用R中的lm()函数（线性模型）来拟合一个线性回归模型。模型的目标（因变量）是mpg（每加仑英里数），预测变量（自变量）是hp（马力）。数据源是R自带的mtcars数据集。 model &lt;- lm(mpg ~ hp, data = mtcars) # 这行代码创建一个列表，其中包含了线性模型的系数（coefficients）、残差（residuals）和拟合值（fitted.values）。 model_summary &lt;- list(coefficients = model$coefficients, residuals = model$residuals, fitted.values = model$fitted.values) #这行代码返回并打印model_summary列表的内容。 model_summary 2.3.3 矩阵（matrix） 矩阵是一个二维数组，其中的所有元素都具有相同的模式（数字、字符或逻辑）。你可以通过matrix()函数来创建矩阵。 # 创建一个3行2列的矩阵： data &lt;- 1:6 matrix1 &lt;- matrix(data, nrow = 3, ncol = 2) print(matrix1) 在这个示例中，我们首先创建了一个从1到6的向量 data，然后我们创建了一个3行2列的矩阵 matrix1，并将这个向量的数据按列存入矩阵。输出的结果会是一个3行2列的矩阵，元素值按列从1到6。 2.3.4 数据框（data frame） 数据框是一种表格形式的数据结构。数据框可以包含不同类型的列，但是每列长度必须相同。数据框可以使用data.frame()函数创建，并可以使用列名称或索引来访问和修改数据框中的数据。 例如，创建一个包含姓名和年龄的数据框： # 创建一个数据框 df &lt;- data.frame( name = c(&quot;John&quot;, &quot;Mary&quot;, &quot;Peter&quot;), age = c(25, 32, 45), gender = c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;) ) # summary()：用于查看数据框的统计信息。 summary(df) #（输出df的基本统计信息，包括均值、中位数、最大值、最小值等。） 对于字符类型（如 name 和 gender），summary() 函数会显示变量长度。 对于数值类型（如 age），summary() 函数会显示最小值（Min.）、第一四分位数（1st Qu.）、中位数（Median）、平均值（Mean）、第三四分位数（3rd Qu.）和最大值（Max.）。 # 查看数据框的结构 str(df) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ name : chr &quot;John&quot; &quot;Mary&quot; &quot;Peter&quot; ## $ age : num 25 32 45 ## $ gender: chr &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; 在输出结果中，以下是每一部分的含义： ‘data.frame’：这是 df 的数据类型，即数据框（data frame）。 3 obs. of 3 variables：这表示 df 有3个观察值（即行）和3个变量（即列）。 接下来的部分列出了数据框的每一个变量（列）： name : chr “John” “Mary” “Peter”： name 表示 df 的一个变量是 name，: chr 表示 name 变量的数据类型是字符型 (character)，接着的 “John” “Mary” “Peter” 是 name 变量的前几个观察值。 age : num 25 32 45：age 表示 df 的另一个变量是 age，: num 表示 age 变量的数据类型是数值型 (numeric)，接着的 25 32 45 是 age 变量的前几个观察值。 gender: chr “Male” “Female” “Male”：gender 表示 df 的另一个变量是 gender，: chr 表示 gender 变量的数据类型是字符型 (character)，接着的 “Male” “Female” “Male” 是 gender 变量的前几个观察值。 # nrow()：用于计算数据框的行数。 nrow(df) #（输出df的行数。） # ncol()：用于计算数据框的列数。 ncol(df) #（输出df的列数。） # head()：用于查看数据框的前几行数据。 head(df, n = 2) #（输出df的前10行数据。） # tail()：用于查看数据框的后几行数据。 tail(df, n = 2) #（输出df的后5行数据。） # unique()：用于去重并输出数据框中唯一的值。 unique(df$gender) #（输出df中gender列的唯一值。） 矩阵和数据框都是二维数据结构。有时候，我们可能需要将数据框转换为矩阵格式，以便于进行某些特定的数据处理（比如进行归一化处理）。很多机器学习算法（例如SVM、KNN等）在训练时，需要数据以矩阵形式输入。此外，在文本分析中，词频-文档矩阵通常非常稀疏，也就是说，矩阵中的大部分元素都是零（因为每个文档只包含词汇表中的一小部分词语）。而在R中，数据框不支持稀疏数据，如果我们尝试将一个大的稀疏矩阵存储为数据框，那么它将占用大量的内存。相反，词频-文档矩阵通常以稀疏矩阵的形式存储，这种格式只存储非零元素，从而大大节省了内存。在文本分析中，我们经常需要对词频-文档矩阵进行各种矩阵运算，例如矩阵乘法、转置等。在R中，数据框不支持这些矩阵运算，如果我们尝试对数据框执行这些操作，那么我们需要先将其转换为矩阵。 2.3.5 数组（array） 数组是一个可以存储具有相同数据类型元素的多维数据结构。可以使用array()函数创建数组。 # 创建一个一维数组： data &lt;- 1:10 array1 &lt;- array(data, dim = c(10)) print(array1) # 在这个示例中，我们首先创建了一个从1到10的向量 data，然后我们创建了一个一维数组 array1，并将这个向量的数据存入数组。输出的结果会是1到10的一维数组。 # 创建一个二维数组： data &lt;- 1:12 array2 &lt;- array(data, dim = c(3, 4)) print(array2) # 在这个示例中，我们创建了一个二维数组 array2，它有3行4列。输出的结果会是一个3行4列的二维数组，元素值从1到12。 # 创建一个三维数组： data &lt;- 1:24 array3 &lt;- array(data, dim = c(2, 3, 4)) print(array3) # 在这个示例中，我们创建了一个三维数组 array3，它有2个2行3列的面。输出的结果会是一个2行3列，共有2个面的三维数组，元素值从1到24。 在数据科学实践中，尽管数组可能不如数据框或列表那么常用，但它们在某些特定的情况下是非常有用的。例如，数据以多维形式出现时，数组可以是理想的数据结构。例如，不同时间、不同地点收集到了大气温度数据，可以存储在一个三维数组中，其中一个维度代表时间，另一个维度代表经度，第三个维度代表纬度。在这种情况下，使用数组可以使数据的处理和分析更加直观和方便。 Dimensions Homogenous Heterogeneous 1D Vector List 2D Matrix Data frame nD Array 在R语言中，有五种主要的数据结构：向量（vector）、矩阵（matrix）、数组（array）、列表（list）和数据框（data frame）。这些数据结构是R编程中非常常用的，可以帮助我们有效地组织和处理数据。每种数据结构都有其特定的用途，适用于处理不同类型和维度的数据。 "],["第三章-数据整理.html", "Chapter 3 第三章 数据整理 3.1 数据与变量 3.2 数据清洁 3.3 变量筛选 3.4 数据塑形 3.5 数据整合", " Chapter 3 第三章 数据整理 数据整理（Data wrangling）是指对原始数据进行一系列的处理，以消除数据中错误，选择合适的数据，调整数据格式，为后续的探索性数据分析做好准备。数据加工主要包括数据清理（Data Cleaning）、变量筛选、数据合成（Data Integration）、数据塑形（Data reshaping）等。在学习数据处理之前，我们需要对数据与变量的类型有所了解。 3.1 数据与变量 3.1.1 结构型数据和非结构型数据 根据数据的组织方式，数据科学中将数据分为结构型数据（Structured Data）和非结构型数据（Unstructured Data）。 结构型数据是指以明确定义的数据结构和数据模式组织的数据，其数据元素之间存在预定义的关系和属性，以便于存储、检索和分析。结构型数据具有固定的格式和字段，通常以表格、数据库、CSV（逗号分隔值）文件等形式存储。每个数据项都有预定义的类型，例如整数、浮点数、日期、文本等，且数据之间可以通过键值或主键进行关联。 非结构型数据是指没有固定格式和明确定义的数据，数据元素之间缺乏预定义的关系和属性，不易于直接存储和分析的数据形式。常见的非结构型数据有文本、图像、音频、视频等内容，其结构和含义需要进一步的处理和解析。这些数据通常存储在文档、图像文件、日志、社交媒体帖子等中。 示例： 文本数据：社交媒体帖子、新闻文章、电子邮件等。 图像数据：照片、绘画、图表等。 音频数据：录音、音乐等。 在语言研究中，语言本体数据包括文本数据（含口语转录的文本）和语音数据。语言使用者的社会人口学数据也是语言研究的重要组成部分。在实验语言学或心理语言学研究中，数据还包括语言使用者加工语言的行为数据和神经活动数据。语言研究涉及的数据类型十分丰富，同时也为数据处理提出了巨大的挑战。 语言研究中，实验数据通常是结构性的。语音数据及标注，还有相应的声学指标，如时长，元音的共振峰，响音部分的基频等一般也是以结构性数据形式存在。结构化数据是较为容易存储，处理的。 文本数据比如博客，文章，小说等是非结构化的，不能直接用数据科学中的数据探索分析方法或者数据建模和数据可视化。文本数据可以通过分词，词频统计，特征提取等方法转化为结构性数据。只有当转化为结构性数据之后，进一步加入元数据，我们才能够来讨论变量之间的关系。在语言研究中，语料库数据通常是非结构性的，对语料库进行量化分析需要我们提取相应信息并转化成为结构性的数据。 3.1.2 质性数据和量化数据 根据数据的统计学特性，数据可以分为质性数据和量化数据。质性数据是指不能够用数字来表示，也不能够进行简单的数学计算，而仅仅用来描述某个事物的范畴属性。比如，语音学中一个音是元音，还是辅音。一个词的词性，名词，动词。一句话所具有的语用功能。 量化数据是可以用数字来描述并且可以进行简单的数学运算。进一步可以分为离散数据（Discrete）和连续数据（ Continuous）。离散数据只能用某一些数值代表，比如参与实验的人数，实验中刺激的个数。连续数据（ Continuous）可以有无限的数据数值代表，比如实验任务的反应时。 在科学研究中，为了探究一些问题，我们通过实验、调查、观察或其他数据收集方法获得的数据。数据通常用于描述和分析研究中的变量，并用于回答研究问题或测试假设。数据是对变量的测量或观察结果。 3.1.3 变量 变量是指任何可以被测量或计数的特征，用于描述不同个体或实物的特征，比如年龄、性别、企业收入和支出、出生国家、班级成绩、眼睛颜色和车辆类型都是常见的变量。变量之所以称为变量，是因为在一个总体（population）或数据集中，其值可能会在不同的个体之间有所变化，或者随着时间的推移可能会发生变化。例如，一个人的身高可以随着时间的推移而增长，一个企业的收入和支出可以随着季度或年份的变化而变化。 变量根据其描述性统计特性分为四种不同层次，分别为定类（nominal）、定序(ordinal)、定距（interval）及定比（ratio）变量。同时，定类和定序变量为类别变量（categorical），定距和定比变量为数值变量（numberic）。 3.1.4 类别变量 定性变量描述事物的“特征”、“类型”或“范畴”，通常使用非数字值表示。分类变量通常是排他的性，即一个个体属于同一变量中的一个类别。分类变量应该是详尽的，包括所有可能的类别。 类别变量可以进一步分为定类和定序两种。定类变量描述一个类别，无法按逻辑顺序进行组织。常见的该类变量包括性别、企业类型、眼睛颜色、宗教和品牌。我们可以描述其频数（率），或者一组数据中的众数 （出现频数最高的）。 定序变量描述一个有顺序类别。比如，调查问卷中，喜欢、一般、厌恶，这3个选项就构成了有顺序数据。定序数据可以按数据顺序排列，因此除了众数之外，我们还可以描述其中位数，也就是将一组数据按一定顺序排列，其中处于中间位置的数据。定序变量的类别相互比较高低大小，但不一定能建立各个类别之间的数字差异。换句话说，变量水平之间的间隔是未知的。 在一些情况下，我们可以为有序变量的不同水平分配数字，实现定序变量转化为数值变量，但我们需要注意这些变量不是数值型的。例如，“非常同意”和“中立”不能平均为“同意”，即使我们将“非常同意”分配为5，将“中立”分配为3。 3.1.5 数值型变量 数值型变量描述了一个可测量的数量，其中数字之间的间隔是相等的。比如，1千克和2千克之间的间隔与3千克和4千克之间的间隔相等。 定距数据是一种数值性变量，一种具有等距间隔的变量，其中相邻数值之间的差异是相等的。比如气温。定距数据可以进行加减。 定比数据和定距数据的区别在于，定比数据有一个零点。比如，年龄、体重、身高都属于定比数据，不存在负值。定比数据除了可以加减运算以外，还可以乘除运算。我们可以说一个人的年龄是另外一个人的两倍。 数值型变量可以进一步分为连续（continuous）变量和离散（discrete）变量两个子类。 离散变量由整数值组成，不能取介于两个值之间的值。常见的离散变量有：车辆数量、家庭子女数量。它们都以整数单位衡量。 连续变量可以取一定集合范围内任意一个值。连续变量的观测值可以包括仪器测量允许的最小值。连续变量的例子包括身高、时间、年龄和温度。 变量类型将决定（1）统计分析的方法；（2）我们如何使用统计数据和图表总结数据。 数据层级 趋中性描述 变异性描述 数学运算 举例 定类 众数 不能 是否相等，集合关系 实验反应选项、词性、语音类别 定序 众数、中位数 不能 排序、比较 李克特量表问题数据 定距 众数、中位数、算数平均值 极差、方差、标准差 加减 温度 定比 众数、中位数、几何平均值 极差、方差、标准差 乘除 年龄、体重、身高 不同的变量需要以不同的数据类型存储在R中。名义变量和定序变量可以存储为字符类型（character）或因子（factors，具有级别）。数值型数据存储为数字，可以是整数（integer）、实数（real）、小数（decimal）。一个数据集（dataset）可以包含一个或者多个变量，这些变量可能有一种或者多种不同的类型。 3.2 数据清洁 数据清理（Data Cleaning）是数据科学和数据分析中的重要步骤，指的是对原始数据进行检查或修正，处理数据中的错误、缺失、异常和重复值等问题，从而得到干净、可靠的数据集，使数据更适合后续的分析和建模工作。 library(tidyverse) # 创建含有错误值的数据 set.seed(123) # 设置随机种子以确保可重复性 df_errors &lt;- data.frame(Student_ID = 1:100, Listening_Score = sample(-5:100, 100, replace = TRUE), Speaking_Score = sample(0:999, 100, replace = TRUE), Reading_Score = sample(0:105, 100, replace = TRUE), Writing_Score = sample(0:100, 100, replace = TRUE), Age = sample(c(18:40, NA), 100, replace = TRUE), Gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 100, replace = TRUE)) 3.2.1 任务-处理异常值 异常值是指与其他观测值明显不同的异常点，可能是数据录入错误或者异常情况。可以选择删除异常值或者采用合适的方法进行处理。 筛除学生听力分数小于0的数据。 筛除学生阅读分数超过满分100分的数据。 筛除年龄缺失的数据 df.clean = df_errors %&gt;% filter(Listening_Score &gt; 0 | Reading_Score &lt; 100)%&gt;% filter(!is.na(Age)) 3.3 变量筛选 当变量过多时，我们可以选择部分变量组成新的数据框。 3.3.1 任务-变量筛选 选择所有女生的听力数据 选择所有男生的阅读数据 df.girl = df_errors %&gt;% filter(Gender == &quot;Female&quot;)%&gt;% select(Listening_Score) df.boy = df_errors %&gt;% filter(Gender == &quot;Male&quot;)%&gt;% select(Reading_Score) 3.4 数据塑形 数据重塑（Data Reshaping）是指将原始数据在行和列之间进行转换，从而改变数据的排列方式，以便于进行特定的数据分析或建模。tidyr是一个常用的数据整理包，主要用于处理数据的宽格式和长格式之间的转换。 Tidy data（整洁数据）是Hadley Wickham在2014年提出的一种数据组织原则，其目标是使数据集易于分析、可读性强，并且方便进行数据操作和可视化。在整洁数据中，每个变量都是一个列，每个观察值都是一个行，每个数据集都是一个表格。整洁数据遵循三个基本规则： 例子：学生考试成绩数据集 Student Subject Score Alice Math 85 Alice English 78 Bob Math 92 Bob English 88 在上面的例子中，每个变量都在数据集的一列中表示（例如，学生、科目、分数在学生考试成绩数据集中是列名），每个观察值在数据集的一行中表示（例如，每个学生的考试成绩是一行）。又称为长数据（Long data）。 相对而言，宽数据（Wide data）中每一行代表一个观察单元，不同属性或测量结果被放置在同一行的不同列中。 如下： 学生姓名 数学成绩 英语成绩 Alice 85 78 Bob 92 88 长数据和宽数据的选择取决于具体的数据分析需求和使用场景。在某些情况下，长数据更适合进行数据处理和可视化，而在其他情况下，宽数据更便于进行数据呈现。它们可以相互转换。 # 生成一个二语学术写作数据集 set.seed(123) # 设置随机种子，以确保结果可复现 academic_writing_data &lt;- tibble( student_id = 1:50, # 学生ID native_language = sample(c(&quot;Chinese&quot;, &quot;Spanish&quot;, &quot;French&quot;, &quot;Russian&quot;, &quot;German&quot;, &quot;Italian&quot;), 50, replace = TRUE), # 母语 second_language = sample(c(&quot;English&quot;, &quot;Chinese&quot;, &quot;Spanish&quot;, &quot;French&quot;, &quot;German&quot;, &quot;Italian&quot;), 50, replace = TRUE), # 二语 essay1_length = sample(200:2000, 50, replace = TRUE), # 论文1长度 essay2_length = sample(200:2000, 50, replace = TRUE), # 论文2长度 essay3_length = sample(200:2000, 50, replace = TRUE), # 论文3长度 academic_performance = sample(60:100, 50, replace = TRUE), # 学术成绩 writing_speed = sample(200:800, 50, replace = TRUE), # 写作速度（单词/分钟） language_anxiety = sample(1:5, 50, replace = TRUE), # 语言焦虑水平（1-5分） writing_errors = sample(0:30, 50, replace = TRUE), # 写作错误数 research_experience = sample(c(TRUE, FALSE), 50, replace = TRUE), # 是否有研究经验 self_evaluation = sample(c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Fair&quot;, &quot;Poor&quot;), 50, replace = TRUE) # 自我评价 ) 3.4.1 任务-数据塑形 把essay1_length、essay2_length和essay3_length三列汇总到一列中 把gathered_data_1转回宽格式 # 把essay1_length、essay2_length和essay3_length三列汇总到一列中 gathered_data_1 &lt;- gather(academic_writing_data, essay, length, essay1_length:essay3_length) # 把gathered_data_1转回宽格式 spreaded_data_1 &lt;- spread(gathered_data_1, essay, length) 3.5 数据整合 如果数据来自不同的源头，可能需要将多个数据集合并成一个整体数据集，进行数据整合和关联分析，即数据整合 （Data Integration）。 我们模拟了以下2个数据集： - 学生信息数据集（Student_Info）：包含学生ID、性别、年龄等学生信息。 - 语言测试数据集（Language_Test）：包含学生ID、听说读写分数等语言测试结果。 在这里，我们将把不同数据集中的学生ID作为核心进行合并。 3.5.1 任务-数据整合 合并学生信息数据集和语言测试数据集，创建一个包含学生信息和语言测试结果的数据集。 # 创建学生信息数据集 student_info &lt;- data.frame(Student_ID = 1:100, Gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 100, replace = TRUE), Age = sample(18:25, 100, replace = TRUE)) # 创建语言测试数据集 language_test &lt;- data.frame(Student_ID = 1:100, Listening_Score = sample(0:100, 100, replace = TRUE), Speaking_Score = sample(0:100, 100, replace = TRUE), Reading_Score = sample(0:100, 100, replace = TRUE), Writing_Score = sample(0:100, 100, replace = TRUE)) # 使用inner_join函数将学生信息数据集和语言测试数据集合并 merged_data &lt;- inner_join(student_info, language_test, by = &quot;Student_ID&quot;) "],["第四章-探索性数据分析.html", "Chapter 4 第四章 探索性数据分析 4.1 数据转换 4.2 数据可视化", " Chapter 4 第四章 探索性数据分析 探索性数据分析（Exploratory Data Analysis，简称EDA）通过可视化和统计手段来帮助人们理解数据中的模式和特征，发现数据中隐藏的关联性和趋势，为进一步的数据建模和分析提供基础,帮助数据科学家和分析师提出假设或问题。 探索性数据分析过程中会不断反复使用以下几个工具： 数据转换（Data Transformation）：对数据集中的数值变量进行变换、重构，进行描述性统计值计算（包括数据的趋中性和分布）。 数据可视化：通过绘制条形图、直方图、散点图、箱线图、密度图等，将数据可视化，更好地展示数据的趋中性和分布。 数据建模：通过统计模型验证数据之间的关系。 4.1 数据转换 对原始数据进行变换、重构的常见操作有： 标准化（Standardization）：将不同特征的数据缩放到相同的尺度。 归一化（Normalization）：将数据缩放到[0, 1]范围内。 对数化（Logarithm）：将数据进行对数变换，使得数据更加符合线性关系或满足某些假设。 此处，我们模拟创建一个包含200个汉语词汇的数据集，包含词频、抽象度、意象度、词汇起始习得年龄、词汇难度、词汇情感评分等变量。这里的数据均为模拟数据。 library(tidyverse) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) # 创建含有真实汉语词汇的数据集 set.seed(123) # 设置随机种子以确保可重复性 df_vocabulary &lt;- data.frame(Word = c(&quot;苹果&quot;, &quot;梨子&quot;, &quot;香蕉&quot;, &quot;草莓&quot;, &quot;桃子&quot;, &quot;西瓜&quot;, &quot;橙子&quot;, &quot;柚子&quot;, &quot;葡萄&quot;, &quot;芒果&quot;, &quot;书&quot;, &quot;笔&quot;, &quot;课本&quot;, &quot;教室&quot;, &quot;黑板&quot;, &quot;学生&quot;, &quot;老师&quot;, &quot;考试&quot;, &quot;作业&quot;, &quot;校园&quot;, &quot;美食&quot;, &quot;旅游&quot;, &quot;音乐&quot;, &quot;电影&quot;, &quot;运动&quot;, &quot;阅读&quot;, &quot;绘画&quot;, &quot;写作&quot;, &quot;摄影&quot;, &quot;游戏&quot;, &quot;猫&quot;, &quot;狗&quot;, &quot;兔子&quot;, &quot;鱼&quot;, &quot;鸟&quot;, &quot;大象&quot;, &quot;狮子&quot;, &quot;熊猫&quot;, &quot;猴子&quot;, &quot;蛇&quot;, &quot;早晨&quot;, &quot;中午&quot;, &quot;下午&quot;, &quot;晚上&quot;, &quot;春天&quot;, &quot;夏天&quot;, &quot;秋天&quot;, &quot;冬天&quot;, &quot;天气&quot;, &quot;季节&quot;, &quot;篮球&quot;, &quot;足球&quot;, &quot;乒乓球&quot;, &quot;网球&quot;, &quot;羽毛球&quot;, &quot;游泳&quot;, &quot;滑雪&quot;, &quot;跑步&quot;, &quot;健身&quot;, &quot;瑜伽&quot;), Frequency = sample(1:1000, 60, replace = TRUE), Abstractness = sample(1:5, 60, replace = TRUE), Imagery = sample(1:5, 60, replace = TRUE), Age_of_Acquisition = sample(3:12, 60, replace = TRUE), Difficulty = sample(1:5, 60, replace = TRUE), Emotion_Score = sample(1:5, 60, replace = TRUE)) 4.1.1 任务-数据转化 计算每个词汇的情感得分与抽象度的乘积作为新特征Emotion_Abstractness_Product。 计算每个词汇的词频与意象度的差作为新特征Frequency_Imagery_Difference。 对词频特征进行Z-score标准化，并创建新特征Frequency_Zscore。 对词汇抽象度进行Min-Max归一化，并创建新特征Abstractness_Normalized 对词频特征进行对数化，并创建新特征Frequency_Log。 # 使用mutate函数创建新特征 df_vocabulary %&gt;% mutate(Emotion_Abstractness_Product = Emotion_Score * Abstractness, Frequency_Imagery_Difference = Frequency - Imagery, Frequency_Zscore = scale(Frequency), Abstractness_Normalized = (Abstractness - min(Abstractness)) / (max(Abstractness) - min(Abstractness)), Frequency_Log = log(Frequency)) ## Word Frequency Abstractness Imagery Age_of_Acquisition Difficulty Emotion_Score Emotion_Abstractness_Product ## 1 苹果 415 4 1 11 2 2 8 ## 2 梨子 463 5 2 6 5 2 10 ## 3 香蕉 179 2 1 12 1 2 4 ## 4 草莓 526 1 2 9 1 5 5 ## 5 桃子 195 1 5 11 2 4 4 ## 6 西瓜 938 3 3 9 1 4 12 ## 7 橙子 818 1 4 12 2 5 5 ## 8 柚子 118 5 4 6 5 4 20 ## 9 葡萄 299 1 1 10 1 1 1 ## 10 芒果 229 2 4 11 3 2 4 ## 11 书 244 4 1 11 2 1 4 ## 12 笔 14 4 3 11 2 2 8 ## 13 课本 374 3 4 7 5 3 9 ## 14 教室 665 1 3 9 4 2 2 ## 15 黑板 602 2 5 8 1 3 6 ## 16 学生 603 1 4 3 5 1 1 ## 17 老师 768 2 4 12 2 4 8 ## 18 考试 709 4 4 12 5 2 8 ## 19 作业 91 5 1 3 2 3 15 ## 20 校园 953 5 2 12 2 5 25 ## 21 美食 348 3 3 3 4 5 15 ## 22 旅游 649 1 4 12 3 3 3 ## 23 音乐 989 4 3 7 4 2 8 ## 24 电影 355 1 1 9 3 5 5 ## 25 运动 840 1 5 7 3 4 4 ## 26 阅读 26 3 5 12 3 2 6 ## 27 绘画 519 4 2 11 5 1 4 ## 28 写作 426 1 3 6 3 4 4 ## 29 摄影 649 3 5 8 2 1 3 ## 30 游戏 766 5 1 4 3 4 20 ## 31 猫 211 3 4 3 1 5 15 ## 32 狗 932 2 2 7 5 4 8 ## 33 兔子 590 5 4 11 1 2 10 ## 34 鱼 593 5 5 6 4 2 10 ## 35 鸟 555 3 5 5 2 4 12 ## 36 大象 871 2 5 11 2 3 6 ## 37 狮子 373 2 5 3 4 5 10 ## 38 熊猫 844 2 1 4 1 2 4 ## 39 猴子 143 4 2 6 4 3 12 ## 40 蛇 544 2 1 12 5 3 6 ## 41 早晨 490 2 2 3 5 3 6 ## 42 中午 621 4 5 7 3 1 4 ## 43 下午 775 4 5 7 5 1 4 ## 44 晚上 905 1 1 11 5 3 3 ## 45 春天 937 3 2 10 3 4 12 ## 46 夏天 842 3 5 9 1 4 12 ## 47 秋天 23 1 4 11 3 5 5 ## 48 冬天 923 3 2 7 4 2 6 ## 49 天气 956 5 2 4 2 2 10 ## 50 季节 309 2 3 12 5 4 8 ## 51 篮球 135 3 1 8 1 2 6 ## 52 足球 821 2 1 9 4 1 2 ## 53 乒乓球 923 5 5 11 3 4 20 ## 54 网球 224 5 5 3 1 3 15 ## 55 羽毛球 166 3 3 7 2 1 3 ## 56 游泳 217 4 2 7 3 5 20 ## 57 滑雪 290 4 5 10 3 3 12 ## 58 跑步 989 4 5 7 1 4 16 ## 59 健身 581 5 3 9 2 2 10 ## 60 瑜伽 72 3 3 6 1 5 15 ## Frequency_Imagery_Difference Frequency_Zscore Abstractness_Normalized Frequency_Log ## 1 414 -0.371413868 0.75 6.028279 ## 2 461 -0.212354843 1.00 6.137727 ## 3 178 -1.153454071 0.25 5.187386 ## 4 524 -0.003589874 0.00 6.265301 ## 5 190 -1.100434396 0.00 5.273000 ## 6 935 1.361666752 0.50 6.843750 ## 7 814 0.964019191 0.00 6.706862 ## 8 114 -1.355591581 1.00 4.770685 ## 9 298 -0.755806510 0.00 5.700444 ## 10 225 -0.987767587 0.25 5.433722 ## 11 243 -0.938061642 0.75 5.497168 ## 12 11 -1.700219467 0.75 2.639057 ## 13 370 -0.507276784 0.50 5.924256 ## 14 662 0.457018551 0.00 6.499787 ## 15 597 0.248253581 0.25 6.400257 ## 16 599 0.251567311 0.00 6.401917 ## 17 764 0.798332708 0.25 6.643790 ## 18 705 0.602822657 0.75 6.563856 ## 19 90 -1.445062282 1.00 4.510860 ## 20 951 1.411372697 1.00 6.859615 ## 21 345 -0.593433756 0.50 5.852202 ## 22 645 0.403998876 0.00 6.475433 ## 23 986 1.530666966 0.75 6.896694 ## 24 354 -0.570237648 0.00 5.872118 ## 25 835 1.036921244 0.00 6.733402 ## 26 21 -1.660454711 0.50 3.258097 ## 27 517 -0.026785982 0.75 6.251904 ## 28 423 -0.334962841 0.00 6.054439 ## 29 644 0.403998876 0.50 6.475433 ## 30 765 0.791705248 1.00 6.641182 ## 31 207 -1.047414721 0.50 5.351858 ## 32 930 1.341784374 0.25 6.837333 ## 33 586 0.208488825 1.00 6.380123 ## 34 588 0.218430014 1.00 6.385194 ## 35 550 0.092508287 0.50 6.318968 ## 36 866 1.139646864 0.25 6.769642 ## 37 368 -0.510590514 0.25 5.921578 ## 38 843 1.050176163 0.25 6.738152 ## 39 141 -1.272748339 0.75 4.962845 ## 40 543 0.056057260 0.25 6.298949 ## 41 488 -0.122884142 0.25 6.194405 ## 42 616 0.311214445 0.75 6.431331 ## 43 770 0.821528815 0.75 6.652863 ## 44 904 1.252313673 0.00 6.807935 ## 45 935 1.358353023 0.50 6.842683 ## 46 837 1.043548704 0.50 6.735780 ## 47 19 -1.670395900 0.00 3.135494 ## 48 921 1.311960807 0.50 6.827629 ## 49 954 1.421313886 1.00 6.862758 ## 50 306 -0.722669213 0.25 5.733341 ## 51 134 -1.299258177 0.50 4.905275 ## 52 820 0.973960380 0.25 6.710523 ## 53 918 1.311960807 1.00 6.827629 ## 54 219 -1.004336236 1.00 5.411646 ## 55 163 -1.196532557 0.50 5.111988 ## 56 215 -1.027532343 0.75 5.379897 ## 57 285 -0.785630077 0.75 5.669881 ## 58 984 1.530666966 0.75 6.896694 ## 59 578 0.178665258 1.00 6.364751 ## 60 69 -1.508023146 0.50 4.276666 对于数据集中的连续型变量，我们常常会根据不同组别进行描述性统计。通常我们从两个方面描述连续型变量：中心趋势（Measures of Central Tendency）和分布。 中心趋势度量可以由众数（Mode）、中位数（Median）和均值（Mean），每个指标都描述了分布中不同的典型值或中心值。均值（Mean）是数据集中每个观测值的值之和除以观测值的数量。这也被称为算术平均值。均值适用于连续和离散数值数据。但是均值无法计算分类数据，因为无法对这些值进行求和。由于均值包括分布中的每个值，因此受到异常值和偏斜分布的影响。总体均值是用希腊字母μ（读作“mu”）表示的。当从样本中计算均值时，用符号x̅（读作X-bar）表示。 中位数（Median）是数据按升序或降序排列时的中间值。中位数将分布分成两半（中位数值的两侧各有50％的观测值）。在具有奇数个观测值的分布中，中位数是中间值。当分布中有偶数个观测值时，中位数是两个中间值的平均值。中位数对于异常值和偏斜数据的影响较小，通常在分布不对称时作为首选的中心趋势度量。对于分类名义数据，无法确定中位数，因为它们无法进行逻辑排序。 众数（Mode）是分布中出现最频繁的值。与中位数和均值相比，众数对于数值和分类（非数值）数据都适用。 在某些分布中，众数可能无法很好地反映分布的中心。 &gt; 54, 54, 54, 55, 56, 57, 57, 58, 58, 60, 60 上述数据集中的分布中心是57岁，但众数却较低，为54岁。 同一个数据集中可能存在多个众数，称为双峰分布或多峰分布。多个众数的存在限制了众数对于描述分布的中心或典型值的能力，因为无法确定单个值来描述中心。在某些情况下，特别是在数据连续的情况下，分布可能根本没有众数（即所有值都不同）。在这种情况下，更好地考虑使用中位数或均值，或者将数据分组为适当的间隔，并找到众数组。 除了中心趋势之外，数值型变量的另一个描述维度是分布。 比如下面这两组数据的中心趋势相同但是二者的离散程度不同 集合1: 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8 集合2: 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11 dataset1 = c(4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 8) dataset2 = c(1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11) dataset = data.frame(dataset1, dataset2) dataset = gather(dataset, type, data) ggplot(dataset, aes(data, fill = type)) + geom_bar(position=position_dodge()) range(dataset1) ## [1] 4 8 quantile(dataset1) ## 0% 25% 50% 75% 100% ## 4 5 6 7 8 quantile(dataset2) ## 0% 25% 50% 75% 100% ## 1.00 3.75 6.00 8.25 11.00 var(dataset1) ## [1] 1.272727 sqrt(var(dataset1)) ## [1] 1.128152 sd(dataset1) ## [1] 1.128152 对于数据分布的度量指标有极差（Range）、四分位数（Quartiles）、四分位距（Interquartile Range，IQR）、方差（variance） 和标准差（standard deviation）。 极差是最大值和最小值之间的差异。四分位数（Quartiles）将有序数据集分为四个等份。四分位距（Interquartile Range，IQR）是上四分位数（Q3）和下四分位数（Q1）之间的差值，用于描述数据集从最低到最高排序的中间50％的值。与极差相比，IQR通常被视为更好的散布度量，因为它不受异常值的影响。方差和标准差是衡量每个观察数据值与均值的接近程度的度量。在具有较小散布的数据集中，所有值都非常接近均值，导致方差和标准差较小。当数据集更为分散时，值离均值的距离较远，导致方差和标准差较大。方差和标准差越小，均值就越能代表整个数据集。因此，如果数据集的所有值都相同，则标准差和方差为零。 标准差是方差的平方根。正态分布的标准差使我们能够计算置信区间。在正态分布中，约68％的值在均值的一个标准差内，约95％的分数在均值的两个标准差内。 五数概括（Five-number summary）是一组描述性统计量，样本最小值（最小观测值）、第一四分位数、中位数（中间值）、第三四分位数、样本最大值（最大观测值）。五数概括提供了数据观测分布的简洁概述。报告这五个数字避免了需要决定最适合的摘要统计量。 通常我们会基于一个分组变量来计算另一个变量的描述性统计指标。 4.1.2 任务-描述性统计 计算男女生各门成绩的均分和标准差 df.score = data.frame(Student_ID = 1:100, Listening_Score = sample(-5:100, 100, replace = TRUE), Speaking_Score = sample(0:999, 100, replace = TRUE), Reading_Score = sample(0:105, 100, replace = TRUE), Writing_Score = sample(0:100, 100, replace = TRUE), Gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 100, replace = TRUE)) # 计算每种母语的学习者的平均学习时长 df.mean.sd &lt;- df.score %&gt;% filter(Listening_Score &gt; 0 | Reading_Score &lt; 100)%&gt;% group_by(Gender) %&gt;% summarise(Listening.mean = mean(Listening_Score), Listening.sd = sd(Listening_Score)) 4.2 数据可视化 上面我们已经对于数据进行了一些转换操作，并通过描述性统计指标对数据中的变量有所了解。我们可以通过图表对数据进行可视化来进一步揭示数据之间的关系。数据可视化过程中需要考虑到数据中变量的类型和数量。 4.2.1 单变量数据可视化 描述分类变量的最佳方式是通过频率。频率可以用不同的方式表示。绝对频率描述一个特定变量出现的次数,就是计数。相对频率描述一个特定变量出现的次数与该变量的总值之间的关系。比如，在语言研究中，我们最常见的分类变量就是词汇或者语法类别。在语料库研究中我们会统计词汇的相对整个语料库总词数的频率。在写作研究中，我们会统计学生名词、动词等不同词类的使用次数。 常见的显示频率分布的方法包括频率表和条形图。频率表是一种简单直接的方式，用于显示特定值或特征出现的次数。条形图则可以基于频率表进行可视化，使得结果更加便于展示。这里我们使用janeaustenr这个包里所包含的简奥斯丁小说数据来构建小说中的词频表，并基于词频表产生条形图。 #用于数据 library(janeaustenr) #用于分词 library(tidytext) library(dplyr) library(stringr) austen.emma.word &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% unnest_tokens(word, text)%&gt;% count(word, sort = TRUE) %&gt;% #筛选Emma这个小说 filter(book == &quot;Emma&quot;)%&gt;% #显示前20个高频词 head(20) 4.2.1.1 条形图 条形图是一种图表类型，其中每个列（可以是垂直或水平绘制）的高度（如果是垂直条形图）或长度（如果是水平条形图）表示一个分类变量的频率（计数）。每个条之间都包含有间隔，可以按任意顺序排列，而不影响数据。 我们根据上述Emma小说最高频的20个词，绘制条形图。 library(ggplot2) austen.emma.word %&gt;% #给高频词排序 mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) 连续变量的描述包括中心趋势和分布。在可视化中我们可以使用箱线图、小提琴图和直方图来显示这两个方面的不同特征。 4.2.1.2 箱线图和小提琴图 箱线图通过从盒子（也称为“须”）延伸垂直线来表示上下四分位数外的变异性。异常值会以单独的点显示。盒子的不同部分之间的间隔表示数据的离散程度。箱线图通常绘制六个数据点，最小值（排除异常值）、第一四分位数、中位数、第三四分位数、最大值（排除异常值）、异常值。 小提琴图结合了箱线图和密度图的特点，可以更直观地展示数据的分布情况。在小提琴图中，箱线图表示五数概括，而密度图展示数据的概率密度分布。 # `languageR` 包中包含了多个数据集，供用户在语言学研究和数据分析中使用。其中`lexdec`:包含了英语词汇决策任务的数据，用于研究词汇的判断和决策过程。 library(languageR) data(&quot;lexdec&quot;) ggplot(lexdec, aes(x=&quot;RT&quot;, y=RT))+ geom_boxplot() #violin plot ggplot(lexdec, aes(x=&quot;RT&quot;, y=RT))+ geom_violin() 4.2.1.3 直方图 直方图展示了数据集中所有观察值的分布情况。柱的高度显示了特定数值范围的频数。柱通常具有相等的宽度，每个柱所代表的数值必须是互斥且完整的，之间没有空隙，每个观察值只能属于一个柱。 直方图和柱状图有一些区别：柱状图适用于展示不同类别变量，每个柱子代表一个类别，高度表示该类别的频数或频率。直方图适用于展示连续变量的分布情况，将数据分成若干连续的区间，每个区间称为一个“柱”，高度表示该区间内数据的频数或频率。 柱状图的X轴通常显示不同的类别标签，例如产品名称、地区等。直方图的X轴显示连续变量的数值范围，例如温度、成绩等。在柱状图中，各个柱体之间通常有间隔，以区分不同的类别。在直方图中，各个柱体紧密相邻，没有间隔，因为它们代表连续的数值区间。 # histogram ggplot(lexdec)+ geom_histogram(aes(RT), binwidth = 0.1) 4.2.1.4 密度图 密度图（Density Plot）也可以展示数据分布，它通过平滑数据点来显示数据在不同区间的密度，提供数据分布的连续视图。与直方图不同，密度图通过使用核密度估计（Kernel Density Estimation, KDE）等方法对数据进行平滑处理，形成一条连续的曲线。可以在同一图上绘制多条密度曲线，便于比较不同组数据的分布差异。 ggplot(lexdec, aes(RT))+ geom_freqpoly(binwidth = 0.1) 4.2.2 多变量数据 变异性（Covariation）用来描述一个变量，协变性描述的是两个或多个变量之间的相互作用。 4.2.2.1 瓦片图（tile plot） 对于两个分类变量，我们可以使用交叉表（contingency table）来描述两个分类变量之间的协变性，显示各自的频率。 table(lexdec$NativeLanguage,lexdec$Sex) ## ## F M ## English 553 395 ## Other 553 158 lexdec%&gt;% count(Class, Correct)%&gt;% ggplot(aes(Class, Correct))+ geom_tile(aes(fill=n)) ggplot(lexdec)+ geom_count(aes(Class, Correct)) 4.2.2.2 分类变量+连续变量 如果既包含分类变量也包含连续变量，可以通过将分类变量视为组别来可视化，其与连续变量之间的协变性。我们可以通过柱状图加误差线的方法来表示不同组之间均值和变异性的差别。也可以通过箱型图和小提琴图来表示。此外我们还可以通过密度图来表示不同的分布特征。 #bar plot lexdec%&gt;% group_by(PrevType)%&gt;% summarise(mean = mean(RT))%&gt;% ggplot(aes(PrevType, mean))+ geom_bar(stat=&quot;identity&quot;) lexdec%&gt;% group_by(PrevType)%&gt;% summarise(mean = mean(RT), sd = sd(RT))%&gt;% ggplot(aes(PrevType, mean, fill = PrevType))+ geom_bar(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = .2, size = 0.7, position = position_dodge(.9)) lexdec%&gt;% ggplot(., aes(x=PrevType, y=RT))+ geom_boxplot() #violin plot lexdec%&gt;% ggplot(., aes(x=PrevType, y=RT))+ geom_violin() # 密度图 ggplot(lexdec, aes(x = RT, y = ..density..))+ geom_freqpoly(aes(color = PrevType), binwidth = 0.1) 如果变量均为连续变量，我们最常见的方式是使用散点图来可视化两个连续变量之间的关系。 4.2.2.3 两个连续变量 散点图 ggplot(lexdec)+ geom_point(aes(RT, Frequency)) "],["第五章-语言实验数据分析.html", "Chapter 5 第五章 语言实验数据分析 5.1 数据导入 5.2 数据整理 5.3 数据转化 5.4 可视化", " Chapter 5 第五章 语言实验数据分析 实验是心理语言学常用的研究方法。对于实验数据的基本分析，包括数据导入，数据整理，数据转换，数据可视化和数据建模。本章以语音感知领域的一个实验为例，介绍在R语言环境中行为实验中数据分析的过程。 研究背景 成年人对言语语音的感知受到母语语音系统的影响。对于母语中没有，而外语中存在的音位对立，人们感知起来会存在困难。因此，对于母语和外语语音感知关系的研究对于外语学习和教学来说非常重要。常用的实验方法或者任务是跨语言感知实验。此类实验中，被试会听到外语语音，然后需要选择与之最相似的母语语音，并且为这种相似度评分。通过对于母语和外语映射关系的分析，结合相关理论，如感知同化模型，我们可以预测不同外语音位对立的感知难度。声调语言占全世界语言的70%，但是相关的跨语言感知研究却很有限，下面的分析基于Chen et al 2020中的部分数据，分析汉语母语者跨语言感知泰语声调的模式。 5.1 数据导入 为了方便数据读取和存储，建议设置工作路径。这里可以将工作路径设置在配套数据文件夹ch5中。实验是使用E-prime进行编程获取的感知数据，单个人的原始数据可以用E-prime进行合成导出成为txt纯文本格式。首先我们使用R的导入函数进行导入得到一个数据框。 library(tidyverse) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) cm13 &lt;- read.delim(&quot;data/ch5/cm13.txt&quot;) head(cm13) ## ExperimentName Subject Session ## 1 Assimilation_2b 101 1 ## 2 Assimilation_2b 101 1 ## 3 Assimilation_2b 101 1 ## 4 Assimilation_2b 101 1 ## 5 Assimilation_2b 101 1 ## 6 Assimilation_2b 101 1 ## Clock.Information ## 1 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## 2 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## 3 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## 4 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## 5 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## 6 &lt;?xml version=1.0?&gt;\\\\n&lt;Clock xmlns:dt=urn:schemas-microsoft-com:datatypes&gt;&lt;Description dt:dt=string&gt;E-Prime Primary Realtime Clock&lt;/Description&gt;&lt;StartTime&gt;&lt;Timestamp dt:dt=int&gt;0&lt;/Timestamp&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/StartTime&gt;&lt;FrequencyChanges&gt;&lt;FrequencyChange&gt;&lt;Frequency dt:dt=r8&gt;2742343&lt;/Frequency&gt;&lt;Timestamp dt:dt=r8&gt;15765959310&lt;/Timestamp&gt;&lt;Current dt:dt=r8&gt;0&lt;/Current&gt;&lt;DateUtc dt:dt=string&gt;2017-12-26T06:39:05Z&lt;/DateUtc&gt;&lt;/FrequencyChange&gt;&lt;/FrequencyChanges&gt;&lt;/Clock&gt;\\\\n ## DataFile.Basename Display.RefreshRate ExperimentVersion Group RandomSeed RuntimeCapabilities RuntimeVersion ## 1 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## 2 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## 3 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## 4 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## 5 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## 6 Assimilation_2b-101-1 60.015 1.0.0.62 1 -1413574928 Professional 2.0.10.353 ## RuntimeVersionExpected SessionDate SessionStartDateTimeUtc SessionTime StudioVersion Block explist explist.Cycle ## 1 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 1 NA NA ## 2 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 2 NA NA ## 3 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 3 NA NA ## 4 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 4 NA NA ## 5 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 5 NA NA ## 6 2.0.10.353 12-26-2017 26-Dec-17 6:39:05 AM 17:39:05 2.0.10.248 6 NA NA ## explist.Sample filename.Block. praclist praclist.Cycle praclist.Sample pracSlide1.ACC pracSlide1.CRESP ## 1 NA Thai_F4_maa241-5 2 1 1 0 NA ## 2 NA Thai_F4_maa45-4 15 1 2 0 NA ## 3 NA Thai_F4_maa45-3 5 1 3 0 NA ## 4 NA Thai_F4_mii33-5 9 1 4 0 NA ## 5 NA Thai_F4_mii315-6 18 1 5 0 NA ## 6 NA Thai_F4_mii45-3 20 1 6 0 NA ## pracSlide1.DurationError pracSlide1.OnsetDelay pracSlide1.OnsetTime pracSlide1.OnsetToOnsetTime pracSlide1.RESP ## 1 0 -1 146763 0 2 ## 2 0 0 155761 0 5 ## 3 0 -1 164758 0 6 ## 4 0 -1 173756 0 7 ## 5 0 -1 182754 0 7 ## 6 0 0 191752 0 7 ## pracSlide1.RT pracSlide1.RTTime pracSoundOut1.ACC pracSoundOut1.CRESP pracSoundOut1.DurationError ## 1 2590 149353 1 NA 369 ## 2 1992 157753 0 NA 0 ## 3 2584 167342 0 NA 0 ## 4 2443 176199 0 NA 0 ## 5 2329 185083 0 NA 0 ## 6 2480 194232 0 NA 0 ## pracSoundOut1.OnsetDelay pracSoundOut1.OnsetTime pracSoundOut1.RESP pracSoundOut1.RT pracSoundOut1.RTTime ## 1 -1 142763 0 0 ## 2 -1 151761 h 2823 154584 ## 3 -1 160759 g 3714 164473 ## 4 -1 169756 f 2269 172025 ## 5 -1 178754 h 1286 180040 ## 6 -1 187752 g 1549 189301 ## Procedure.Block. Running.Block. speaker.Block. syllable.Block. token.Block. tone.Block. Trial f1List1 ## 1 pracproc praclist F4 maa 5 241 NA NA ## 2 pracproc praclist F4 maa 4 45 NA NA ## 3 pracproc praclist F4 maa 3 45 NA NA ## 4 pracproc praclist F4 mii 5 33 NA NA ## 5 pracproc praclist F4 mii 6 315 NA NA ## 6 pracproc praclist F4 mii 3 45 NA NA ## f1List1.Cycle f1List1.Sample f1List2 f1List2.Cycle f1List2.Sample f2List1 f2List1.Cycle f2List1.Sample f2List2 ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA ## f2List2.Cycle f2List2.Sample filename.Trial. Procedure.Trial. Running.Trial. Slide1.ACC Slide1.CRESP ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## Slide1.DurationError Slide1.OnsetDelay Slide1.OnsetTime Slide1.OnsetToOnsetTime Slide1.RESP Slide1.RT ## 1 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA ## Slide1.RTTime SoundOut1.ACC SoundOut1.CRESP SoundOut1.DurationError SoundOut1.OnsetDelay SoundOut1.OnsetTime ## 1 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA ## SoundOut1.RESP SoundOut1.RT SoundOut1.RTTime speaker.Trial. syllable.Trial. token.Trial. tone.Trial. ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA # library(readr) # assimilation &lt;- read_csv(&quot;~/Nutstore Files/310_Tutorial/Language data science/data/ch5/assimilation.csv&quot;) # # head(assimilation) 此时我们得到一个1820行，84列的数据框，里面包含了13名汉语普通话母语者感知不同泰语声调，并将其同化为普通话四个声调的数据。 5.2 数据整理 在正式实验之前，我们让被试进行了几个试次的练习。因此，首先我们需要去掉被试在练习时的数据。原始数据中每列的名称受到E-prime编程和运行的影响，有很多列是没有用的。为了方便后续数据处理，我们选择需要的列（即刺激和对于的选择和反应时数据），并去除被试没有作答的试次。 assim.clean = cm13%&gt;% # 除去练习数据 filter(., Procedure.Block. != &quot;pracproc&quot;) %&gt;% # 选择需要的列并将变量名修改 select(., subject = &quot;Subject&quot;, stimuli = &quot;tone.Trial.&quot;, response = &quot;SoundOut1.RESP&quot;, response_rt = &quot;SoundOut1.RT&quot;, rating = &quot;Slide1.RESP&quot;, rating_rt = &quot;Slide1.RT&quot;)%&gt;% # 除去被试未作答的试次 filter(., response !=&quot;&quot;)%&gt;% # 反应时为刺激播放后1000毫秒后的被试反应，原始数据从刺激播放开始记录，因此进行调整。 mutate(., response_rt = response_rt-1000)%&gt;% # 将被试的按键反应改写成相应的母语声调类别，修改刺激名称 mutate(response =dplyr:: recode(response, f = &quot;M55&quot;, g = &quot;M35&quot;, h = &quot;M214&quot;, j = &quot;M51&quot;), stimuli = dplyr:: recode(stimuli, &quot;33&quot; = &quot;T33&quot;, &quot;21&quot; = &quot;T21&quot;, &quot;45&quot; = &quot;T45&quot;, &quot;315&quot; = &quot;T315&quot;, &quot;241&quot; = &quot;T241&quot;)) # mutate(n = 1)%&gt;% # group_by(language, subject, stimuli, response)%&gt;% # summarise(cat = sum(n), # response_rt = mean(response_rt, na.rm = TRUE), # rating = mean(rating, na.rm = TRUE), # #rating_rt = mean(rating_rt,na.rm = TRUE) # )%&gt;% # group_by(subject, stimuli)%&gt;% # mutate(percent = cat/sum(cat))%&gt;% # ungroup()%&gt;% # mutate() # tbl.cat = cat.final%&gt;% # mutate(stimuli = as_factor(stimuli), # response = as_factor(response), # stimuli = fct_relevel(stimuli, &quot;T45&quot;,&quot;T33&quot;,&quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;), # response = fct_relevel(response, # &quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;, # &quot;NV44&quot;,&quot;NV22&quot;,&quot;NV35&quot;,&quot;NV21&quot;,&quot;NV415&quot;,&quot;NV214&quot;, # &quot;SV44&quot;,&quot;SV22&quot;,&quot;SV35&quot;,&quot;SV21&quot;,&quot;SV214&quot;))%&gt;% # group_by(stimuli,response)%&gt;% # summarise(cat.mean = round(sum(percent)/13, 3)*100, # rate.mean = round(mean(rating,na.rm = TRUE),1))%&gt;% # gather(temp, score, ends_with(&quot;.mean&quot;)) %&gt;% # unite(temp1, stimuli, temp, sep = &quot;_&quot;)%&gt;% # spread(temp1, score)%&gt;% # #change the order of columns # select(&quot;response&quot; , &quot;T45_cat.mean&quot;, &quot;T45_rate.mean&quot;, # &quot;T33_cat.mean&quot; , &quot;T33_rate.mean&quot;, # &quot;T21_cat.mean&quot;,&quot;T21_rate.mean&quot;, # &quot;T315_cat.mean&quot;, &quot;T315_rate.mean&quot;, # &quot;T241_cat.mean&quot;, &quot;T241_rate.mean&quot;) 5.3 数据转化 下面我们希望能够先计算每个人在听到泰语五个声调时的选择模式和反应时。然后基于此计算出汉语普通话组的选择模式和反应时。 assim.individual = assim.clean %&gt;% #为每个反应计数 mutate(n = 1)%&gt;% # 计算每个人每个泰语声调及对应汉语声调反应的次数、评分和反应时均值 group_by(subject, stimuli, response)%&gt;% summarise(cat = sum(n), response_rt = mean(response_rt, na.rm = TRUE), rating = mean(rating, na.rm = TRUE))%&gt;% # 计算每个人每个泰语声调及对应汉语声调反应的比例 group_by(subject, stimuli)%&gt;% mutate(percent = cat/sum(cat))%&gt;% ungroup() ## `summarise()` has grouped output by &#39;subject&#39;, &#39;stimuli&#39;. You can override using the `.groups` argument. assim.group = assim.individual%&gt;% # mutate(stimuli = as_factor(stimuli), # response = as_factor(response), # stimuli = fct_relevel(stimuli, &quot;T45&quot;,&quot;T33&quot;,&quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;), # response = fct_relevel(response, # &quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;))%&gt;% group_by(stimuli,response)%&gt;% # 计算每组反应平均值和评分均值 summarise(cat.mean = round(sum(percent)/13, 3)*100, rate.mean = round(mean(rating,na.rm = TRUE),1))%&gt;% gather(temp, score, ends_with(&quot;.mean&quot;)) %&gt;% # 将刺激和对应的反应均值和评分均值组合 unite(temp1, stimuli, temp, sep = &quot;_&quot;)%&gt;% # 将表格进行转换，得到最终需要的感知同化模式图 spread(temp1, score)%&gt;% #change the order of columns select(&quot;response&quot; , &quot;T45_cat.mean&quot;, &quot;T45_rate.mean&quot;, &quot;T33_cat.mean&quot; , &quot;T33_rate.mean&quot;, &quot;T21_cat.mean&quot;,&quot;T21_rate.mean&quot;, &quot;T315_cat.mean&quot;, &quot;T315_rate.mean&quot;, &quot;T241_cat.mean&quot;, &quot;T241_rate.mean&quot;) ## `summarise()` has grouped output by &#39;stimuli&#39;. You can override using the `.groups` argument. 5.4 可视化 上述表格包含两个分类变量（泰语刺激的声调类型以及汉语反应的声调类型）和两个连续变量（选择的百分比以及评分）。两个分类变量是自变量，两个连续变量是因变量，我们可以根据因变量不同分别作图。首先，我们可以为感知同化模式画一个堆叠柱状图。普通柱状图的横纵表表示一个分类变量，为了让柱状图可以表示两个分类变量，我们需要在每个柱子中加入不同的颜色来区分。 assim.individual %&gt;% group_by(stimuli,response)%&gt;% summarise(percent = round(sum(percent)/13, 3)*100)%&gt;% filter(percent&gt;1)%&gt;% ggplot(aes(fill=response, y=percent, x=stimuli)) + geom_bar( stat=&quot;identity&quot;)+ scale_fill_manual( values=c(&quot;M55&quot; = &quot;black&quot;, &quot;M35&quot;=&quot;gray50&quot;, &quot;M214&quot;=&quot;gray75&quot;, &quot;M51&quot;=&quot;white&quot;), name=&quot;Mandarin&quot;, breaks=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;))+ geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;)+ xlab(&quot;Thai tones (stimuli)&quot;)+ ylab(&quot;Percentage of choice (%)&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, 101))+ labs(title = &quot;Perceputal assimialtion of Thai tones by Mandarin listeners&quot;)+ theme_classic()+ theme(legend.title = element_text(size=12, face=&quot;bold&quot;))+ theme(legend.text = element_text(size = 12, face = &quot;bold&quot;))+ theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10)) ## `summarise()` has grouped output by &#39;stimuli&#39;. You can override using the `.groups` argument. 两个分类变量的另一种可视化方法是热力图。我们可以用泰语的刺激作为横轴，普通话声调类别作为纵轴，用色块的深浅表示评分的高低。 assim.individual %&gt;% group_by(stimuli,response)%&gt;% summarise(percent = round(sum(percent)/13, 3)*100, # 计算评分的均值 rate.mean = round(mean(rating,na.rm = TRUE),1))%&gt;% mutate(text.color = (stimuli== response))%&gt;% # 构建xy轴 ggplot(aes(stimuli,response))+ # 构建热力图 geom_tile(aes(fill = rate.mean))+ # 将评分作为标记填入，并设置相应的颜色、大小 geom_text(aes(label = round(rate.mean, 1)), color = &quot;red&quot;, size = 4) + scale_colour_manual(values=c(&quot;black&quot;, &quot;white&quot;))+ scale_x_discrete(name = &quot;Thai stimuli (%)&quot;)+ scale_y_discrete(name = &quot;Mandarin responses (%)&quot;)+ scale_fill_gradient(name=&quot;%&quot;,low = &quot;white&quot;, high = &quot;black&quot;) + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=8), axis.title.x = element_text(face=&quot;bold&quot;, size=10), axis.title.y = element_text(face=&quot;bold&quot;, size=10), #axis.title.y = element_blank(), axis.text.y = element_text(face=&quot;bold&quot;, size=8), #axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.ticks.x = element_blank()) ## `summarise()` has grouped output by &#39;stimuli&#39;. You can override using the `.groups` argument. "],["第六章-文本数据分析.html", "Chapter 6 第六章 文本数据分析 6.1 词汇特征 6.2 短语分析 6.3 句子分析", " Chapter 6 第六章 文本数据分析 文本数据属于非结构化数据，通常需要通过分词、词性赋码等自然语言处理方法，将文本数据进行结构化。 在语言学研究中，语料库研究与文本最为相关。Tognini-Bonelli（2001）提出基于语料库（corpus-based）和语料库驱动（corpus-driven）的区分。基于语料库的研究将语料库用作验证研究者直觉或检查小型数据集中语言的频率和/或合理性的例子来源。研究者不会质疑预先存在的传统描述单元和类别。语料库驱动的分析则是一种更具归纳性的过程：语料库本身就是数据，分析过程中通过记录语料库中的模式来表达语言中的规律性（和例外情况）。语料库驱动的分析倾向于只使用关于语法结构的最低限度的理论预设。 在词汇层面的语料库研究中，我们一般会进行词频统计（Word Frequency Count），即统计词汇在语料库中的出现频率，以了解词汇的使用频率和分布情况，构建词表。其次，与参考语料库比较，我们可以进行关键词提取（Keyword Extraction）即识别出在特定语境或文本类型中特别频繁出现的词汇。 再者，如果对于某些特定词汇感兴趣，我们可以通过共现分析（concordance Analysis）研究哪些词汇经常一起出现，以了解词汇之间的关系和语境。 最后，通过collocation anlysis我们可以分析词组搭配。 除了对文本中的词汇进行分析，也可以通过词汇来分析文本的风格。词汇丰富度和可读性分析可以帮我们比较不同文本之间词汇复杂性的差异。在分析不同翻译文本的特征是我们常常用到。此外，通过文本中最高频的几百个词的频率特征我们可以解锁不同作家的创作指纹。这样的分析可以帮我们鉴定一些归属存疑的文本作品。 在R语言中常用的文本处理的程序包有tidytext和quenteda。他们各有不同的功能，同时彼此之间可以相互转化。 library(tidyverse) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) library(quanteda) #install.packages(&quot;readtext&quot;) require(readtext) library(DT) library(DiagrammeR) library(FactoMineR) library(factoextra) library(flextable) library(GGally) library(ggdendro) library(igraph) library(network) library(Matrix) library(quanteda.textstats) library(quanteda.textplots) library(tm) library(sna) library(udpipe) grViz(&quot; digraph text_workflow { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10, rankdir = LR] # 节点 node [shape = box, style = unfilled, fontname = Helvetica] 文本分析[shape = folder]; 词汇分析; 短语分析; 句子分析; node [shape = oval, style = filled, fontname = Helvetica] 词频分析; 关键词分析; 搭配分析; 词汇丰富度; 信息熵; 情感分析; 心理语言学特征 ; 词汇语义相似度 ; 上下文分析; 搭配分析; 词性分析; 句子依存分析; 可读性分析; # 边线 文本分析 -&gt;词汇分析 文本分析 -&gt;短语分析 文本分析 -&gt;句子分析 词汇分析-&gt; 词频分析; 词汇分析-&gt; 关键词分析; 词汇分析-&gt; 词汇丰富度; 词汇分析-&gt; 信息熵; 词汇分析-&gt; 情感分析; 词汇分析-&gt; 心理语言学特征 ; 词汇分析-&gt; 词汇语义相似度 ; 短语分析-&gt;上下文分析; 短语分析-&gt;搭配分析; 句子分析-&gt;词性分析; 句子分析-&gt;句子依存分析; 句子分析-&gt;可读性分析; } &quot;) 6.1 词汇特征 grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10, rankdir = LR] # 节点 node [shape = box, style = unfilled, fontname = Helvetica] 导入readtext[label = &#39;导入 \\n readtext&#39;, shape = folder, fillcolor = Beige]; 构建语料库corpus[label = &#39;构建语料库 \\n corpus&#39;]; 分词tokens[label = &#39;分词 \\n tokens&#39;]; 分词unnest_tokens[label = &#39;分词 \\n unnest_tokens&#39;, color = red]; dfm ; node [shape = oval, style = filled, fontname = Helvetica] 词频textstat_frequency[label = &#39;词频 \\n textstat_frequency&#39;]; KWIC[label = &#39;KWIC \\n kwic&#39;] ; 搭配textstat_collocations[label = &#39;搭配 \\n textstat_collocations&#39;]; 关键词分析textstat_keyness[label = &#39;关键词分析 \\n textstat_keyness&#39;] ; # 边线 导入readtext -&gt;构建语料库corpus-&gt;分词tokens-&gt; dfm -&gt; 词频textstat_frequency dfm -&gt; 关键词分析textstat_keyness 分词tokens -&gt; KWIC 分词tokens -&gt; 搭配textstat_collocations 构建语料库corpus-&gt;分词unnest_tokens 分词unnest_tokens -&gt; 词频textstat_frequency 分词unnest_tokens -&gt; 关键词分析textstat_keyness } &quot;) 6.1.1 词频提取 词频是语料库语言学最基本的概念。频率可以以原始数据的形式给出，例如在某个文本中，某个词出现了58次；或者可以以百分比或比例的形式给出，某个词在每百万词中出现602.91次。这使得可以在不同大小的语料库之间进行比较。 通过对语料库中每个词进行词频统计编制的词表可以用来生成关键词列表。 quanteda package # quanteda detectives.raw &lt;- readtext(&quot;data/ch6/detectives/*txt&quot;) corps.detectives = corpus(detectives.raw) summary(corpus(corps.detectives), 5) ## Corpus consisting of 2 documents, showing 2 documents: ## ## Text Types Tokens Sentences ## Dee.txt 6010 79276 3686 ## Sherlock.txt 6313 51368 2736 docvars(corps.detectives, &quot;book&quot;) &lt;- names(corps.detectives) tidy.corps.detectives = tidytext::tidy(corps.detectives) # make a dfm dfm_detectives &lt;- corps.detectives %&gt;% tokens(remove_punct = TRUE) %&gt;% tokens_remove(stopwords(&quot;en&quot;)) %&gt;% dfm()%&gt;% dfm_group(groups = book) print(dfm_detectives) ## Document-feature matrix of: 2 documents, 8,566 features (35.77% sparse) and 1 docvar. ## features ## docs chapter_0101 judge dee appointed magistrate chang-ping people crowd tribunal report ## Dee.txt 1 697 542 12 82 41 117 36 110 60 ## Sherlock.txt 0 4 0 0 0 0 18 5 2 4 ## [ reached max_nfeat ... 8,556 more features ] topfeatures(dfm_detectives, 20) ## judge said dee one now man old two upon shall mrs will time young room case back ma come ## 701 553 542 326 315 300 225 222 217 207 204 198 197 179 177 167 166 160 158 ## see ## 156 library(&quot;quanteda.textstats&quot;) library(&quot;quanteda.textplots&quot;) dfm_detectives.freq &lt;- textstat_frequency(dfm_detectives, n = 20, groups = book) ggplot(dfm_detectives.freq, aes(x = frequency, y = reorder(feature, frequency))) + geom_point() + labs(x = &quot;Frequency&quot;, y = &quot;Feature&quot;)+ facet_wrap(~ group, scales = &quot;free&quot;) 6.1.2 关键词提取 在语料库语言学中，关键词分析（keyword analysis）是一种通过与参考语料库进行比较，识别出在特定语料库中出现频率异常高（正关键词）或异常低（负关键词）的词语的方法。 关键词（Keyword）是指在一个文本或语料库中，与参考语料库相比，出现频率显著高于或低于预期的词语。通常，使用统计测试（如对数似然检验或卡方检验）来比较两个词表，以得出关键词。 参考语料库（Reference corpus）是一个平衡且代表性的语料库，通常用于关键词分析中提供参考词表。在关键词分析中，通过与参考语料库的比较，可以发现哪些词在特定语料库中是关键的。 在关键词分析中，一个词可能仅仅因为在某些文本中频率极高，而被误认为是关键词。为了确认一个词是否真的具有代表性，可以使用分布图查看这个词在整个语料库中的分布，或者计算在多个文本中均为关键词的词汇列表，以避免因不均匀分布导致的偏差。 6.1.2.1 quanteda package # Calculate keyness and determine Trump as target group tstat_keyness &lt;- textstat_keyness(dfm_detectives , target = &quot;Dee.txt&quot;) tstat_keyness%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # Plot estimated word keyness textplot_keyness(tstat_keyness) 6.1.2.2 tidytext texts_df = detectives.raw%&gt;% unnest_tokens(word, text)%&gt;% mutate(doc_id = dplyr::recode(doc_id , &quot;Dee.txt&quot; = &quot;text1&quot;, &quot;Sherlock.txt&quot; = &quot;text2&quot;))%&gt;% group_by(doc_id,word)%&gt;% summarise(freq = n())%&gt;% spread(doc_id, freq, fill = 0)%&gt;% dplyr::rename(token = &quot;word&quot;) ## `summarise()` has grouped output by &#39;doc_id&#39;. You can override using the `.groups` argument. stats_tb2 = texts_df %&gt;% dplyr::mutate(text1 = as.numeric(text1), text2 = as.numeric(text2)) %&gt;% dplyr::mutate(C1 = sum(text1), C2 = sum(text2), N = C1 + C2) %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(R1 = text1+text2, R2 = N - R1, O11 = text1, O12 = R1-O11, O21 = C1-O11, O22 = C2-O12) %&gt;% dplyr::mutate(E11 = (R1 * C1) / N, E12 = (R1 * C2) / N, E21 = (R2 * C1) / N, E22 = (R2 * C2) / N) %&gt;% dplyr::select(-text1, -text2) head(stats_tb2)%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) assoc_tb3 = stats_tb2 %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract descriptives dplyr::mutate(ptw_target = O11/C1*1000, ptw_ref = O12/C2*1000) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract keyness measures dplyr::mutate(phi = sqrt((X2 / N)), MI = log2(O11 / E11), t.score = (O11 - E11) / sqrt(O11), PMI = log2( (O11 / N) / ((O11+O12) / N) * ((O11+O21) / N) ), DeltaP = (O11 / R1) - (O21 / R2), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) )), G2 = 2 * ((O11+ 0.001) * log((O11+ 0.001) / E11) + (O12+ 0.001) * log((O12+ 0.001) / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22)), # traditional keyness measures RateRatio = ((O11+ 0.001)/(C1*1000)) / ((O12+ 0.001)/(C2*1000)), RateDifference = (O11/(C1*1000)) - (O12/(C2*1000)), DifferenceCoefficient = RateDifference / sum((O11/(C1*1000)), (O12/(C2*1000))), OddsRatio = ((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) ), LLR = 2 * (O11 * (log((O11 / E11)))), RDF = abs((O11 / C1) - (O12 / C2)), PDiff = abs(ptw_target - ptw_ref) / ((ptw_target + ptw_ref) / 2) * 100, SignedDKL = sum(ifelse(O11 &gt; 0, O11 * log(O11 / ((O11 + O12) / 2)), 0) - ifelse(O12 &gt; 0, O12 * log(O12 / ((O11 + O12) / 2)), 0))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5), type = ifelse(E11 &gt; O11, &quot;antitype&quot;, &quot;type&quot;), phi = ifelse(E11 &gt; O11, -phi, phi), G2 = ifelse(E11 &gt; O11, -G2, G2)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;) %&gt;% # arrange by G2 dplyr::arrange(-G2) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;upp&quot;, &quot;low&quot;, &quot;op&quot;, &quot;t.score&quot;, &quot;z.score&quot;, &quot;Rws&quot;))) %&gt;% dplyr::relocate(any_of(c(&quot;token&quot;, &quot;type&quot;, &quot;Sig_corrected&quot;, &quot;O11&quot;, &quot;O12&quot;, &quot;ptw_target&quot;, &quot;ptw_ref&quot;, &quot;G2&quot;, &quot;RDF&quot;, &quot;RateRatio&quot;, &quot;RateDifference&quot;, &quot;DifferenceCoefficient&quot;, &quot;LLR&quot;, &quot;SignedDKL&quot;, &quot;PDiff&quot;, &quot;LogOddsRatio&quot;, &quot;MI&quot;, &quot;PMI&quot;, &quot;phi&quot;, &quot;X2&quot;, &quot;OddsRatio&quot;, &quot;DeltaP&quot;, &quot;p&quot;, &quot;E11&quot;, &quot;O21&quot;, &quot;O22&quot;))) head(assoc_tb3)%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) 6.1.3 上下文中的关键词索引（KWIC） 索引（Concordance）是指在语料库中按照字母顺序排列的搜索模式索引，显示该搜索模式在每个上下文中的出现。上下文中的关键词索引（Key-Word-In-Context Concordance，简称KWIC）是一种显示关键词在语料库中实际使用情况的工具或方法。具体来说，KWIC 索引会将目标关键词放在中心位置，并显示该词在文本中的所有出现位置，同时显示它前后的一定数量的单词或词组。这种排列方式可以帮助研究者观察关键词在不同上下文中的用法和意义。 例如，如果你想研究英语中“education”一词的用法，KWIC 索引会将所有包含“education”的句子或句段列出，展示该词左右的上下文。这样，研究者就可以一目了然地看到“education”在不同语境下的使用方式，并分析其频率、搭配、语法特征以及语义变化。 KWIC 索引在语料库语言学中被广泛应用，因为它能够帮助研究者深入理解词语在实际语言使用中的行为，而不仅仅是关注词频等表面数据。 toks_corpus_detectives &lt;- corps.detectives%&gt;% tokens(remove_punct = TRUE)%&gt;% tokens_remove(stopwords(&quot;en&quot;)) kwic(toks_corpus_detectives , pattern = &quot;murder&quot;) %&gt;% head()%&gt;% as.data.frame() ## docname from to pre keyword post ## 1 Dee.txt 283 283 difficult cases tells people commit murder able live end days odour ## 2 Dee.txt 1156 1156 dangerous criminal hand apparently important murder case course decide merits statement ## 3 Dee.txt 1217 1217 Mile Village respectfully greet Honour murder falls jurisdiction morning saw bodies ## 4 Dee.txt 1417 1417 living along road heard double murder Knowing Judge Dee&#39;s reputation great ## 5 Dee.txt 1578 1578 apart question whether Koong guilty murder right remove two bodies place ## 6 Dee.txt 1782 1782 slightest evidence struggle let alone murder committed Judge Dee thinking Koong ## pattern ## 1 murder ## 2 murder ## 3 murder ## 4 murder ## 5 murder ## 6 murder 平行语料库共现分析 # 导入 law_ce &lt;- read_csv(&quot;data/ch6/law.csv&quot;, col_names = TRUE) ## Rows: 155 Columns: 2 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): cn, en ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # 检索中文 law_ce%&gt;% filter(str_detect(cn, &quot;当事人&quot;))%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # put it at the top of the table law_ce%&gt;% mutate(count = str_count(cn, &quot;当事人&quot;))%&gt;% filter(count &gt; 0) ## # A tibble: 41 × 3 ## cn en count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1         第一条     为了保护合同当事人的合法权益，维护社会经济秩序，促进社会主义现代化建设，制定本法… Arti… 1 ## 2         第三条     合同当事人的法律地位平等，一方不得将自己的意志强加给另一方。 Arti… 1 ## 3         第四条     当事人依法享有自愿订立合同的权利，任何单位和个人不得非法干预。 Arti… 1 ## 4         第五条     当事人应当遵循公平原则确定各方的权利和义务。 Arti… 1 ## 5         第六条     当事人行使权利、履行义务应当遵循诚实信用原则。 Arti… 1 ## 6         第七条     当事人订立、履行合同，应当遵守法律、行政法规，尊重社会公德，不得扰乱社会经济秩序… Arti… 1 ## 7         第八条     依法成立的合同，对当事人具有法律约束力。当事人应当按照约定履行自己的义务，不得擅… Arti… 2 ## 8         第九条     当事人订立合同，应当具有相应的民事权利能力和民事行为能力。当事人依法可以委托代理… Arti… 2 ## 9         第十条     当事人订立合同，有书面形式、口头形式和其他形式。法律、行政法规规定采用书面形式的… Arti… 2 ## 10         第十二条     合同的内容由当事人约定，一般包括以下条款： Arti… 1 ## # ℹ 31 more rows ##上下排版的文本如何构建平行语料库 trans_corpus &lt;- read_csv(&quot;data/ch6/英汉对照文本.csv&quot;, col_names = FALSE) ## Rows: 104 Columns: 1 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): X1 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. row.no = nrow(trans_corpus) trans.new = trans_corpus%&gt;% mutate(key = rep(c(&quot;E&quot;,&quot;C&quot;), (row.no/2)), id = rep(1:(row.no/2),each=2))%&gt;% spread(key, X1) trans.new %&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # put it at the top of the table #search trans.new%&gt;% filter(str_detect(E, &quot;good&quot;))%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # put it at the top of the table 6.1.3.1 词汇文本分布 kwic(toks_corpus_detectives , pattern = &quot;murder&quot;) %&gt;% textplot_xray() textplot_xray( kwic(toks_corpus_detectives, pattern = &quot;murder&quot;), kwic(toks_corpus_detectives, pattern = &quot;judge&quot;), kwic(toks_corpus_detectives, pattern = &quot;police&quot;), scale = &quot;absolute&quot;) ## Warning: Use of `x$ntokens` is discouraged. ## ℹ Use `ntokens` instead. 6.2 短语分析 6.2.1 上下文共现 # 对于词组 kwic(toks_corpus_detectives, pattern = phrase(&quot;commit crime&quot;)) %&gt;% head()%&gt;% as.data.frame() ## docname from to pre keyword post ## 1 Dee.txt 2626 2627 bamboo still faced original problem commit crime thus ruminated Sergeant Hoong set ## pattern ## 1 commit crime 6.2.2 搭配分析 参考https://ladal.edu.au/coll.html#Identifying_collocations_using_kwics 搭配分析（collocation analysis）是语料库语言学中的一种方法，用于研究词汇之间的共现关系，即在自然语言中某些词语经常一起出现的现象。搭配分析通过识别和分析这些词语的共现频率和模式，可以揭示出词语之间的语义或语法关系。 搭配（Collocation）：搭配是指两个或多个词在特定的上下文中经常一起出现。例如，在英语中，“strong”常常与“tea”搭配，而“powerful”则很少与“tea”搭配。类似地，“make a decision”和“take a photo”也是常见的搭配。 搭配强度（Collocational Strength）：搭配强度是衡量词语之间共现关系的紧密程度。它通常通过统计方法计算，比如互信息（Mutual Information，MI）或t检验（t-score）等。高搭配强度表明这些词语经常一起出现，并且比随机出现的可能性大得多。 搭配范围（Collocational Range）：搭配范围指的是词语在多大范围内（比如在前后几个词内）出现的频率。例如，“make”在前面紧跟着“decision”或“mistake”时，是一个常见的搭配，这个范围通常称为“window size”。 搭配分析的应用： 1. 语言学习： 搭配分析有助于语言学习者理解哪些词语经常一起使用，从而提高语言的自然性和流利度。例如，学习者可以通过搭配分析了解“do homework”比“make homework”更自然。 词典编纂： 词典编纂者使用搭配分析来确定哪些词语组合应该被列为固定搭配，从而为使用者提供更准确的词语用法信息。 语义分析： 通过分析一个词的搭配，可以推测其语义。例如，“dark”通常与“night”搭配，而“dark”与“mood”搭配时，可以暗示“mood”是消极的。 我们需要区分以下概念： 搭配 (collocations)：指的是显著彼此吸引且常常一起出现的单词（但不一定是相邻的），例如 black 和 coffee。 n元语法 (n-grams)：指的是相邻的单词组合，比如 This is、is a 和 a sentence，这些双词组 (bi-grams) 组成了句子 This is a sentence。 这样的单词配对或组合表现出一定的自然性，并且倾向于形成重复的模式。它们在语言习得、学习、流利度和使用中起着至关重要的作用，并且有助于自然和惯用的思想表达。一个典型的搭配 (collocation) 例子是 Merry Christmas，因为 merry 和 Christmas 比单词随机组合时更频繁地一起出现。其他搭配的例子包括 strong coffee、make a decision 或 take a risk。对于语言学习者来说，识别和理解搭配 (collocations) 是至关重要的，因为这可以增强他们生成地道且符合语境的语言的能力。 识别搭配的词对（w1 和 w2，即搭配 (collocations)）并确定其关联强度（衡量单词彼此吸引的强度）是基于词对在列联表中的共现频率（见下表，O 代表观察频率）。 搭配分析是通过统计方法研究词语之间的共现关系，揭示语言使用中的规律性和隐含语义。它在语言研究、教育和自然语言处理等领域具有广泛的应用。 toks_corpus_detectives%&gt;% #提取包含大写字母的专有名词 tokens_select(pattern = &quot;^[A-Z]&quot;, valuetype = &quot;regex&quot;, case_insensitive = FALSE, padding = TRUE) %&gt;% textstat_collocations(min_count = 5, size = 3, tolower = FALSE) ## collocation count count_nested length lambda z ## 1 Mrs Bee Mrs 9 0 3 2.2882883 1.36389991 ## 2 Mr Joseph Stangerson 5 0 3 0.1344819 0.06212732 ## 3 Six Mile Village 31 0 3 -1.9089729 -0.64638713 ## 4 Mr Sherlock Holmes 7 0 3 -3.2894384 -1.32239444 ## 5 Old Mr Hua 6 0 3 -2.9579926 -1.39906678 ## 6 Joong Chiao Tai 26 0 3 -6.0275679 -2.11984329 ## 7 Salt Lake City 8 0 3 -7.4619815 -2.49201790 ## 8 Halliday&#39;s Private Hotel 5 0 3 -9.1967484 -2.63498363 ## 9 Ma Joong Chiao 26 0 3 -6.6497622 -2.66229470 ## 10 Chiao Tai Ma 5 0 3 -8.0053802 -2.79391406 ## 11 Tai Ma Joong 5 0 3 -7.7757162 -3.08229949 ## 12 Dragon Boat Festival 5 0 3 -10.8061495 -3.32192975 ## 13 Warden Ho Kai 29 0 3 -8.5422313 -3.38635450 ## 14 Bee Mrs Djou 7 0 3 -4.8244261 -3.77890236 ## 15 Excellency Judge Dee 5 0 3 -6.6410263 -4.11215680 collocation.3wd &lt;- textstat_collocations(corps.detectives, size = 3, tolower = FALSE) 从这个列联表中，我们可以计算出如果单词之间没有任何吸引或排斥关系时的期望频率（见下表，E 表示期望频率）。 观察频率 (Observed Frequency) w2 出现 w2 未出现 合计 w1 出现 O11 O12 R1 w1 未出现 O21 O22 R2 合计 C1 C2 N 期望频率 (Expected Frequency) w2 出现 w2 未出现 合计 w1 出现 E11 = (R1 * C1) / N E12 = (R1 * C2) / N R1 w1 未出现 E21 = (R2 * C1) / N E22 = (R2 * C2) / N R2 合计 C1 C2 N 通过这个列联表，期望频率表示在单词之间没有吸引或排斥的情况下，它们一起出现的频率。 关联度量使用上面列联表中的频率信息来评估单词之间的吸引或排斥强度。因此，关联度量是用来量化词语在搭配 (collocation) 中关系强度和显著性的统计指标。这些度量帮助评估两个单词是否比随机出现更频繁地一起出现。在搭配分析中，常用的几种关联度量包括： Gries的关联度量 (Gries’ AM)：Gries的AM（Gries 2022）可能是基于条件概率的最佳关联度量。关于其计算方式，请参考Gries (2022)。与其他关联度量相比，它有三个主要优点： 1. 它考虑到单词1和单词2之间的关联并非对称的（单词1可能比单词2更强烈地吸引单词2，反之亦然），在某种意义上，它非常类似于ΔP。 2. 它不受频率影响，而其他关联度量则会受到影响（这是一个严重的问题，因为关联度量应该反映关联强度而不是频率）。 3. 它是归一化的，因为它考虑到不同元素的值范围不同（一些单词可以有非常高的值，而其他单词则不能）。 ΔP (delta P)：ΔP（Ellis 2007；Gries 2013）是一种基于条件概率的关联度量，它在MS中隐含 (Gries 2013, 141)。ΔP有两个优点：它考虑到单词1和单词2之间的关联并非对称的（单词1可能比单词2更强烈地吸引单词2，反之亦然），并且它不受频率影响（这是一个严重的问题，因为关联度量应该反映关联强度而不是频率）（见Gries 2022）。 Δ𝑃1 = 𝑃(𝑤1|𝑤2) = (𝑂11 / 𝑅1) − (𝑂21 / 𝑅2) Δ𝑃2 = 𝑃(𝑤2|𝑤1) = (𝑂11 / 𝐶1) − (𝑂21 / 𝐶2) 逐点互信息 (PMI)：PMI 衡量两个单词共同出现的可能性，与它们分别独立出现的可能性相比。较高的PMI分数表明较强的关联。 PMI(𝑤1,𝑤2) = log2(𝑃(𝑤1∩𝑤2) / (𝑃(𝑤1) ⋅ 𝑃(𝑤2))) 对数似然比 (LLR)：LLR 将观察到的单词组合出现的可能性与基于单词个体频率的期望可能性进行比较。较高的LLR值表明更显著的关联（其中𝑂𝑖是观察频率，𝐸𝑖是每个组合的期望频率）。 LLR(𝑤1,𝑤2) = 2 ∑(𝑂𝑖 − 𝐸𝑖)^2 / 𝐸𝑖 Dice系数：该度量考虑了单词的共现情况，计算两个单词的重叠与它们各自频率之和的比率。Dice系数的范围是0到1，值越高表明关联越强。 Dice(𝑤1,𝑤2) = 2 × freq(𝑤1∩𝑤2) / (freq(𝑤1) + freq(𝑤2)) 卡方检验 (Chi-Square)：卡方检验衡量单词共现的观察频率与期望频率之间的差异。较高的卡方值表明更显著的关联（其中𝑂𝑖是观察频率，𝐸𝑖是每个组合的期望频率）。 𝜒²(𝑤1,𝑤2) = ∑(𝑂𝑖 − 𝐸𝑖)^2 / 𝐸𝑖 t-检验 (t-Score)：t-检验基于观察频率和期望频率之间的差异，并进行标准差归一化。较高的t-分数表明较强的关联。 t-Score(𝑤1,𝑤2) = (freq(𝑤1∩𝑤2) − expected_freq(𝑤1∩𝑤2)) / √freq(𝑤1∩𝑤2) 互信息 (MI)：MI 衡量根据一个单词的出现，另一个单词出现的不确定性减少了多少。较高的MI值表明较强的关联（其中𝑃(𝑤1∩𝑤2)是联合概率，𝑃(𝑤1)和𝑃(𝑤2)是个体概率）。 MI(𝑤1,𝑤2) = log2(𝑃(𝑤1∩𝑤2) / (𝑃(𝑤1) ⋅ 𝑃(𝑤2))) 最小敏感度 (Minimum Sensitivity, MS)：当w1和w2总是一起出现且从不分开时，最小敏感度为1。当w1和w2从未一起出现时，最小敏感度为0。较高的最小敏感度表示两个单词在双词组中的依赖性更强（Pedersen 1998）。 MS = 𝑚𝑖𝑛(𝑃(𝑤1|𝑤2), 𝑃(𝑤2|𝑤1)) 这些关联度量帮助研究人员和语言分析师识别有意义且具有统计显著性的搭配，从而辅助从语料库中提取相关信息，并提高语言学研究中搭配分析的准确性。 6.2.2.1 基于句子提取搭配 # set options options(stringsAsFactors = F) options(scipen = 999) options(max.print=1000) # read in text # text &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/cdo.rda&quot;, &quot;rb&quot;)) %&gt;% # #合并不同行 # paste0(collapse = &quot; &quot;) %&gt;% # #除去多余空格 # stringr::str_squish() %&gt;% # stringr::str_remove_all(&quot;- &quot;) # text %&gt;% # # concatenate the elements in the &#39;text&#39; object # paste0(collapse = &quot; &quot;) %&gt;% # # separate possessives and contractions # stringr::str_replace_all(fixed(&quot;&#39;&quot;), fixed(&quot; &#39;&quot;)) %&gt;% # stringr::str_replace_all(fixed(&quot;’&quot;), fixed(&quot; &#39;&quot;)) %&gt;% sentences =corps.detectives%&gt;% # split text into sentences tokenizers::tokenize_sentences() %&gt;% # unlist sentences unlist() %&gt;% # remove non-word characters stringr::str_replace_all(&quot;\\\\W&quot;, &quot; &quot;) %&gt;% stringr::str_replace_all(&quot;[^[:alnum:] ]&quot;, &quot; &quot;) %&gt;% # remove superfluous white spaces stringr::str_squish() %&gt;% # convert to lower case and save in &#39;sentences&#39; object tolower() head(sentences) ## [1] &quot;chapter 0101 judge dee is appointed magistrate of chang ping the people crowd his tribunal to report grievances&quot; ## [2] &quot;although all people hanker after a magistrate s office few realise all that is involved in solving criminal cases tempering severity by lenience as laid down by our law makers and avoiding the extremes advocated by crafty philosophers&quot; ## [3] &quot;one upright magistrate means the happiness of a thousand families the one word justice means the peace of the entire population&quot; ## [4] &quot;the exemplary conduct of judge dee magistrate of chang ping is placed here on record for the edification of the reading public&quot; ## [5] &quot;in the end as a general rule no criminal escapes the laws of the land&quot; ## [6] &quot;but it is up to the judge to decide who is guilty and who is innocent&quot; # tokenize the &#39;sentences&#39; data using quanteda package coll_basic = sentences %&gt;% quanteda::tokens() %&gt;% # create a document-feature matrix (dfm) using quanteda quanteda::dfm() %&gt;% # create a feature co-occurrence matrix (fcm) without considering trigrams quanteda::fcm(tri = FALSE)%&gt;% # tidy the data using tidytext package tidytext::tidy() %&gt;% # rearrange columns for better readability dplyr::relocate(term, document, count) %&gt;% # rename columns for better interpretation dplyr::rename(w1 = 1, w2 = 2, O11 = 3) head(coll_basic)%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # calculate the total number of observations (N) colldf = coll_basic %&gt;% dplyr::mutate(N = sum(O11)) %&gt;% # calculate R1, O12, and R2 dplyr::group_by(w1) %&gt;% dplyr::mutate(R1 = sum(O11), O12 = R1 - O11, R2 = N - R1) %&gt;% dplyr::ungroup(w1) %&gt;% # calculate C1, O21, C2, and O22 dplyr::group_by(w2) %&gt;% dplyr::mutate(C1 = sum(O11), O21 = C1 - O11, C2 = N - C1, O22 = R2 - O21) head(colldf )%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # reduce and complement data colldf_redux = colldf %&gt;% # determine Term dplyr::filter(w1 == &quot;crime&quot;, # set minimum number of occurrences of w2 (O11+O21) &gt; 10, # set minimum number of co-occurrences of w1 and w2 O11 &gt; 5) %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(E11 = R1 * C1 / N, E12 = R1 * C2 / N, E21 = R2 * C1 / N, E22 = R2 * C2 / N) head(colldf_redux )%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) assoc_tb = colldf_redux %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract AM # 1. bias towards top left dplyr::mutate(btl_O12 = ifelse(C1 &gt; R1, 0, R1-C1), btl_O11 = ifelse(C1 &gt; R1, R1, R1-btl_O12), btl_O21 = ifelse(C1 &gt; R1, C1-R1, C1-btl_O11), btl_O22 = ifelse(C1 &gt; R1, C2, C2-btl_O12), # 2. bias towards top right btr_O11 = 0, btr_O21 = R1, btr_O12 = C1, btr_O22 = C2-R1) %&gt;% # 3. calculate AM dplyr::mutate(upp = btl_O11/R1, low = btr_O11/R1, op = O11/R1) %&gt;% dplyr::mutate(AM = op / upp) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;btr_O21&quot;, &quot;btr_O12&quot;, &quot;btr_O22&quot;, &quot;btl_O12&quot;, &quot;btl_O11&quot;, &quot;btl_O21&quot;, &quot;btl_O22&quot;, &quot;btr_O11&quot;))) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract association measures dplyr::mutate(phi = sqrt((X2 / N)), Dice = (2 * O11) / (R1 + C1), LogDice = log((2 * O11) / (R1 + C1)), MI = log2(O11 / E11), MS = min((O11/C1), (O11/R1)), t.score = (O11 - E11) / sqrt(O11), z.score = (O11 - E11) / sqrt(E11), PMI = log2( (O11 / N) / ((O11+O12) / N) * ((O11+O21) / N) ), DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)), DeltaP21 = (O11 / (O11 + O21)) - (O21 / (O12 + O22)), DP = (O11 / R1) - (O21 / R2), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) )), # calculate LL aka G2 G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;, # filter out instances where the w1 and w2 repel each other E11 &lt; O11) %&gt;% # arrange by DeltaP12 (association measure) dplyr::arrange(-DeltaP12) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;O12&quot;, &quot;O21&quot;, &quot;O22&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;, &quot;E11&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;upp&quot;, &quot;low&quot;, &quot;op&quot;, &quot;Rws&quot;))) head(assoc_tb)%&gt;% datatable(., filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) 6.2.2.2 基于目标词提取搭配 kwic_words &lt;- quanteda::tokens_select(tokens(corps.detectives, remove_numbers = TRUE, remove_punct = TRUE), pattern = &quot;kill&quot;, window = 5, selection = &quot;keep&quot;) %&gt;% unlist() %&gt;% # tabulate results table() %&gt;% # convert into data frame as.data.frame() %&gt;% # rename columns dplyr::rename(token = 1, n = 2) %&gt;% # add a column with type dplyr::mutate(type = &quot;kwic&quot;) corpus_words &lt;- corps.detectives %&gt;% # tokenize the corpus files quanteda::tokens( remove_numbers = TRUE, remove_punct = TRUE) %&gt;% # unlist the tokens to create a data frame unlist() %&gt;% as.data.frame() %&gt;% # rename the column to &#39;token&#39; dplyr::rename(token = 1) %&gt;% # group by &#39;token&#39; and count the occurrences dplyr::group_by(token) %&gt;% dplyr::summarise(n = n()) %&gt;% # add column stating where the frequency list is &#39;from&#39; dplyr::mutate(type = &quot;corpus&quot;) freq_df &lt;- dplyr::left_join(corpus_words, kwic_words, by = c(&quot;token&quot;)) %&gt;% # rename columns and select relevant columns dplyr::rename(corpus = n.x, kwic = n.y) %&gt;% dplyr::select(-type.x, -type.y) %&gt;% # replace NA values with 0 in &#39;corpus&#39; and &#39;kwic&#39; columns tidyr::replace_na(list(corpus = 0, kwic = 0)) stats_tb = freq_df %&gt;% dplyr::filter(corpus &gt; 0) %&gt;% dplyr::mutate(corpus = as.numeric(corpus), kwic = as.numeric(kwic)) %&gt;% dplyr::mutate(corpus= corpus-kwic, C1 = sum(kwic), C2 = sum(corpus), N = C1 + C2) %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(R1 = corpus+kwic, R2 = N - R1, O11 = kwic, O12 = R1-O11, O21 = C1-O11, O22 = C2-O12) %&gt;% dplyr::mutate(E11 = (R1 * C1) / N, E12 = (R1 * C2) / N, E21 = (R2 * C1) / N, E22 = (R2 * C2) / N) %&gt;% dplyr::select(-corpus, -kwic) assoc_tb2 = stats_tb %&gt;% # determine number of rows dplyr::mutate(Rws = nrow(.)) %&gt;% # work row-wise dplyr::rowwise() %&gt;% # calculate fishers&#39; exact test dplyr::mutate(p = as.vector(unlist(fisher.test(matrix(c(O11, O12, O21, O22), ncol = 2, byrow = T))[1]))) %&gt;% # extract AM # 1. bias towards top left dplyr::mutate(btl_O12 = ifelse(C1 &gt; R1, 0, R1-C1), btl_O11 = ifelse(C1 &gt; R1, R1, R1-btl_O12), btl_O21 = ifelse(C1 &gt; R1, C1-R1, C1-btl_O11), btl_O22 = ifelse(C1 &gt; R1, C2, C2-btl_O12), # 2. bias towards top right btr_O11 = 0, btr_O21 = R1, btr_O12 = C1, btr_O22 = C2-R1) %&gt;% # 3. calculate AM dplyr::mutate(upp = btl_O11/R1, low = btr_O11/R1, op = O11/R1) %&gt;% dplyr::mutate(AM = op / upp) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;btr_O21&quot;, &quot;btr_O12&quot;, &quot;btr_O22&quot;, &quot;btl_O12&quot;, &quot;btl_O11&quot;, &quot;btl_O21&quot;, &quot;btl_O22&quot;, &quot;btr_O11&quot;))) %&gt;% # extract x2 statistics dplyr::mutate(X2 = (O11-E11)^2/E11 + (O12-E12)^2/E12 + (O21-E21)^2/E21 + (O22-E22)^2/E22) %&gt;% # extract expected frequency dplyr::mutate(Exp = E11) %&gt;% # extract association measures dplyr::mutate(phi = sqrt((X2 / N)), MS = min((O11/C1), (O11/R1)), Dice = (2 * O11) / (R1 + C1), LogDice = log((2 * O11) / (R1 + C1)), MI = log2(O11 / E11), t.score = (O11 - E11) / sqrt(O11), z.score = (O11 - E11) / sqrt(E11), PMI = log2( (O11 / N) / ((O11+O12) / N) * ((O11+O21) / N) ), DeltaP12 = (O11 / (O11 + O12)) - (O21 / (O21 + O22)), DeltaP21 = (O11 / (O11 + O21)) - (O21 / (O12 + O22)), DP = (O11 / R1) - (O21 / R2), LogOddsRatio = log(((O11 + 0.5) * (O22 + 0.5)) / ( (O12 + 0.5) * (O21 + 0.5) )), # calculate LL aka G2 G2 = 2 * (O11 * log(O11 / E11) + O12 * log(O12 / E12) + O21 * log(O21 / E21) + O22 * log(O22 / E22))) %&gt;% # determine Bonferroni corrected significance dplyr::mutate(Sig_corrected = dplyr::case_when(p / Rws &gt; .05 ~ &quot;n.s.&quot;, p / Rws &gt; .01 ~ &quot;p &lt; .05*&quot;, p / Rws &gt; .001 ~ &quot;p &lt; .01**&quot;, p / Rws &lt;= .001 ~ &quot;p &lt; .001***&quot;, T ~ &quot;N.A.&quot;)) %&gt;% # round p-value dplyr::mutate(p = round(p, 5)) %&gt;% # filter out non significant results dplyr::filter(Sig_corrected != &quot;n.s.&quot;, # filter out instances where the w1 and w2 repel each other E11 &lt; O11) %&gt;% # arrange by phi (association measure) dplyr::arrange(-DeltaP12) %&gt;% # remove superfluous columns dplyr::select(-any_of(c(&quot;TermCoocFreq&quot;, &quot;AllFreq&quot;, &quot;NRows&quot;, &quot;O12&quot;, &quot;O21&quot;, &quot;O22&quot;, &quot;R1&quot;, &quot;R2&quot;, &quot;C1&quot;, &quot;C2&quot;, &quot;E11&quot;, &quot;E12&quot;, &quot;E21&quot;, &quot;E22&quot;, &quot;upp&quot;, &quot;low&quot;, &quot;op&quot;, &quot;Rws&quot;))) # sort the assoc_tb2 data frame in descending order based on the &#39;phi&#39; column assoc_tb2 %&gt;% dplyr::arrange(-phi) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # create a ggplot with &#39;token&#39; on the x-axis (reordered by &#39;phi&#39;) and &#39;phi&#39; on the y-axis ggplot(aes(x = reorder(token, phi, mean), y = phi)) + # add a scatter plot with points representing the &#39;phi&#39; values geom_point() + # flip the coordinates to have horizontal points coord_flip() + # set the theme to a basic white and black theme theme_bw() + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Association strength (phi)&quot; labs(x = &quot;Token&quot;, y = &quot;Association strength (phi)&quot;) # sort the assoc_tb2 data frame in descending order based on the &#39;phi&#39; column assoc_tb2 %&gt;% dplyr::arrange(-phi) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # create a ggplot with &#39;token&#39; on the x-axis (reordered by &#39;phi&#39;) and &#39;phi&#39; on the y-axis ggplot(aes(x = reorder(token, phi, mean), y = phi, label = phi)) + # add a bar plot using the &#39;phi&#39; values geom_bar(stat = &quot;identity&quot;) + # add text labels above the bars with rounded &#39;phi&#39; values geom_text(aes(y = phi - 0.005, label = round(phi, 3)), color = &quot;white&quot;, size = 3) + # flip the coordinates to have horizontal bars coord_flip() + # set the theme to a basic white and black theme theme_bw() + # set the x-axis label to &quot;Token&quot; and y-axis label to &quot;Association strength (phi)&quot; labs(x = &quot;Token&quot;, y = &quot;Association strength (phi)&quot;) 6.2.2.2.1 Dendrograms # sort the assoc_tb2 data frame in descending order based on the &#39;phi&#39; column top20colls &lt;- assoc_tb2 %&gt;% dplyr::arrange(-phi) %&gt;% # select the top 20 rows after sorting head(20) %&gt;% # extract the &#39;token&#39; column dplyr::pull(token) # inspect the top 20 tokens with the highest &#39;phi&#39; values top20colls ## [1] &quot;kill&quot; &quot;conceived&quot; &quot;crown&quot; &quot;prince&quot; &quot;withdraws&quot; &quot;instigated&quot; &quot;king&quot; &quot;begged&quot; ## [9] &quot;cooks&quot; &quot;hesitate&quot; &quot;intended&quot; &quot;shortly&quot; &quot;colleague&quot; &quot;complaint&quot; &quot;imagine&quot; &quot;afterwards&quot; ## [17] &quot;desire&quot; &quot;husband&quot; &quot;affairs&quot; &quot;cold&quot; # tokenize the &#39;sentences&#39; data using quanteda package keyword_fcm &lt;- sentences %&gt;% quanteda::tokens() %&gt;% # create a document-feature matrix (dfm) from the tokens quanteda::dfm() %&gt;% # select features based on &#39;top20colls&#39; and the term &quot;selection&quot; pattern quanteda::dfm_select(pattern = c(top20colls, &quot;kill&quot;)) %&gt;% # Create a symmetric feature co-occurrence matrix (fcm) quanteda::fcm(tri = FALSE) # inspect the first 6 rows and 6 columns of the resulting fcm keyword_fcm[1:6, 1:6] ## Feature co-occurrence matrix of: 6 by 6 features. ## features ## features affairs king desire afterwards cold kill ## affairs 0 0 1 0 0 0 ## king 0 1 0 2 0 2 ## desire 1 0 0 0 0 0 ## afterwards 0 2 0 0 0 1 ## cold 0 0 0 0 0 1 ## kill 0 2 0 1 1 1 # create a hierarchical clustering object using the distance matrix of the fcm as data hclust(dist(keyword_fcm), # use ward.D as linkage method method=&quot;ward.D2&quot;) %&gt;% # generate visualization (dendrogram) ggdendrogram() + # add title ggtitle(&quot;20 most strongly collocating terms of &#39;kill&#39;&quot;) 6.2.2.3 networkplot # create a network plot using the fcm quanteda.textplots::textplot_network(keyword_fcm, # set the transparency of edges to 0.8 for visibility edge_alpha = 0.8, # set the color of edges to gray edge_color = &quot;gray&quot;, # set the size of edges to 2 for better visibility edge_size = 2, # adjust the size of vertex labels # based on the logarithm of row sums of the fcm vertex_labelsize = log(rowSums(keyword_fcm))) 6.2.3 词汇多样性 词汇丰富度（Lexical Richness）是指在语言学和语言习得研究中，用来衡量一个人使用语言时，所表现出的词汇多样性和复杂程度的指标。它反映了一个人语言表达的广度和深度，具体而言，就是在一个语言片段中，使用多少不同的词汇以及这些词汇的复杂性。 词汇丰富度通常通过以下几个方面来衡量： 类型-标记比率（Type-Token Ratio, TTR）：这是最常见的词汇丰富度衡量指标。类型指的是不同的词汇种类，而标记指的是词汇的总数。类型-标记比率就是不同词汇种类数与总词汇数的比值。TTR 比值越高，表示词汇丰富度越高，但它会受到文本长度的影响。 词汇密度（Lexical Density）：衡量一个文本中实词（如名词、动词、形容词、副词）所占的比例。高词汇密度意味着文本中使用了较多的实词，相对内容较为丰富。 平均词长（Mean Word Length）：平均词长可以作为词汇复杂性的一种衡量方式，通常较长的词汇意味着较高的词汇丰富度。 稀有词汇比例（Proportion of Rare Words）：这个指标衡量的是在文本中使用的稀有或不常见词汇的比例。稀有词汇使用越多，词汇丰富度通常越高。 D值（D-measure）：这是一个更复杂的指标，试图通过建模解决 TTR 中的文本长度影响问题，提供一个更稳定的词汇丰富度衡量方式。 词汇丰富度的分析在二语习得、语言能力评估、心理语言学等领域中具有重要意义。通过衡量一个人词汇丰富度，可以判断其语言能力、表达能力、认知水平，甚至可以用来诊断语言障碍或认知功能衰退。 lexical.diversity &lt;- textstat_lexdiv(dfm_detectives, measure = &quot;all&quot;) lexical.diversity.df = lexical.diversity%&gt;% as.data.frame()%&gt;% # mutate(id = 1:n())%&gt;% # filter(id &gt;1 )%&gt;% select(&quot;document&quot;,&quot;TTR&quot;,&quot;C&quot;,&quot;R&quot;,&quot;K&quot;,&quot;D&quot;,&quot;Vm&quot;)%&gt;% gather(&quot;measures&quot;, &quot;values&quot;, -c(&quot;document&quot;))%&gt;% mutate(measures = as.factor(measures), measures = fct_relevel(measures, &quot;TTR&quot;,&quot;C&quot;,&quot;R&quot;,&quot;K&quot;,&quot;D&quot;,&quot;Vm&quot;)) lexical.diversity.df ## document measures values ## 1 Dee.txt TTR 0.153081395 ## 2 Sherlock.txt TTR 0.287095142 ## 3 Dee.txt C 0.820331293 ## 4 Sherlock.txt C 0.873835885 ## 5 Dee.txt R 28.392369184 ## 6 Sherlock.txt R 40.357040760 ## 7 Dee.txt K 18.328184154 ## 8 Sherlock.txt K 10.754253471 ## 9 Dee.txt D 0.001832872 ## 10 Sherlock.txt D 0.001075480 ## 11 Dee.txt Vm 0.040889983 ## 12 Sherlock.txt Vm 0.030818161 ## 移动窗口词汇丰富度 lexical.mattr &lt;- textstat_lexdiv(tokens(corps.detectives), measure = &quot;MATTR&quot;, MATTR_window = 500) lexical.mattr ## document MATTR ## 1 Dee.txt 0.4970508 ## 2 Sherlock.txt 0.5203928 6.2.4 文本可读性 readability &lt;- textstat_readability(corps.detectives, measure = &quot;all&quot;) readability.df = readability %&gt;% select(&quot;document&quot;,&quot;ARI&quot;,&quot;Flesch&quot;,&quot;FOG&quot;, &quot;Coleman.Liau.short&quot;,&quot;Dale.Chall&quot;,&quot;Spache&quot;)%&gt;% gather(&quot;measures&quot;, &quot;values&quot;, -c(&quot;document&quot;))%&gt;% mutate(measures = as.factor(measures), measures = fct_relevel(measures, &quot;ARI&quot;,&quot;Flesch&quot;,&quot;FOG&quot;, &quot;Coleman.Liau.short&quot;,&quot;Dale.Chall&quot;,&quot;Spache&quot;)) 6.2.5 文本信息熵 文本信息熵（Textual Information Entropy）是信息理论中的一个概念，用来衡量文本中信息的复杂性和不确定性。它是由克劳德·香农（Claude Shannon）在1948年提出的，并广泛应用于语言学、计算机科学和信息论等领域。 信息熵的基本概念： 不确定性（Uncertainty）：信息熵反映了一个系统的不确定性程度。如果某个事件的发生是完全确定的，那么其信息熵为零；相反，如果事件的发生非常不确定，其信息熵则较高。 概率分布（Probability Distribution）：信息熵的计算基于事件出现的概率分布。在文本处理中，这通常意味着计算某个字母、单词或符号在整个文本中出现的概率。 香农熵（Shannon Entropy）：这是信息熵最常用的形式，表示为： \\[ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) \\] 其中，\\(H(X)\\) 是信息熵，\\(P(x_i)\\) 是事件 \\(x_i\\) 发生的概率，\\(n\\) 是可能的事件总数。对于文本信息熵，事件通常是字符、单词或其他文本元素的出现。 文本信息熵在语言学中的应用： 通过计算文本的熵，可以分析文本的语言复杂性。熵值越高，意味着文本的信息量大、复杂性高；熵值越低，意味着文本更为简单、信息量较少。 textstat_entropy(dfm_detectives) ## document entropy ## 1 Dee.txt 10.71495 ## 2 Sherlock.txt 11.23666 6.2.6 文本情感分析 library(dplyr) library(stringr) library(tidytext) tidy_books &lt;- detectives.raw %&gt;% unnest_sentences(sentence, text)%&gt;% group_by(doc_id) %&gt;% mutate(linenumber = row_number()) %&gt;% ungroup() %&gt;% unnest_tokens(word, sentence) tidy.sent = tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(doc_id, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) ## Joining with `by = join_by(word)` library(ggplot2) ggplot(tidy.sent , aes(index, sentiment, fill = doc_id)) + geom_col(show.legend = FALSE) + facet_wrap(~doc_id, ncol = 2, scales = &quot;free_x&quot;) # 进一步看看哪些积极或者消极的词 bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% group_by(doc_id) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() ## Joining with `by = join_by(word)` bing_word_counts %&gt;% group_by(sentiment) %&gt;% slice_max(n, n = 10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(doc_id~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) 6.3 句子分析 6.3.1 依存句法标注 #Load the model # First load the model which you have downloaded or which you have stored somewhere on disk. # # udmodel &lt;- udpipe_download_model(language = &quot;chinese&quot;) ## Either give a file in the current working directory udmodel_eng &lt;- udpipe_load_model(file = &quot;data/ch6/english-ewt-ud-2.5-191206.udpipe&quot;) # udmodel_chn &lt;- udpipe_load_model(file = &quot;chinese-gsd-ud-2.5-191206.udpipe&quot;) # dee.raw &lt;- scan(file=&quot;judge_dee/01.txt&quot;, # what=&quot;character&quot;, # sep=&quot;\\n&quot;) dee.raw = detectives.raw %&gt;% filter(doc_id == &quot;Dee.txt&quot;) dee.depd &lt;- udpipe_annotate(udmodel_eng, x = dee.raw$text) dee.df &lt;- as.data.frame(dee.depd) table(dee.df$upos) ## ## ADJ ADP ADV AUX CCONJ DET INTJ NOUN NUM PART PRON PROPN PUNCT SCONJ SYM VERB X ## 4039 7057 5027 4235 2445 7638 69 12744 478 2057 8201 4015 10216 1896 6 10135 13 6.3.2 词性统计 library(lattice) stats &lt;- txt_freq(dee.df$upos) stats$key &lt;- factor(stats$key, levels = rev(stats$key)) barchart(key ~ freq, data = stats, col = &quot;cadetblue&quot;, main = &quot;UPOS (Universal Parts of Speech)\\n frequency of occurrence&quot;, xlab = &quot;Freq&quot;) 6.3.3 依存句法结构 library(textplot) # generate dependency plot dplot &lt;- textplot_dependencyparser(filter(dee.df, doc_id == &quot;doc1&quot;, paragraph_id == 2, sentence_id == 2), size = 3) # show plot dplot stats &lt;- subset(dee.df, upos %in% c(&quot;NOUN&quot;)) stats &lt;- udpipe::txt_freq(stats$token) stats$key &lt;- factor(stats$key, levels = rev(stats$key)) barchart(key ~ freq, data = head(stats, 20), col = &quot;cadetblue&quot;, main = &quot;Most occurring nouns&quot;, xlab = &quot;Freq&quot;) 6.3.4 基于词性的搭配提取 ## Using RAKE stats &lt;- keywords_rake(x = dee.df, term = &quot;lemma&quot;, group = &quot;doc_id&quot;, relevant = dee.df$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;)) stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword)) barchart(key ~ rake, data = head(subset(stats, freq &gt; 3), 20), col = &quot;cadetblue&quot;, main = &quot;Keywords identified by RAKE&quot;, xlab = &quot;Rake&quot;) ## Using Pointwise Mutual Information Collocations dee.df$word &lt;- tolower(dee.df$token) stats &lt;- keywords_collocation(x = dee.df, term = &quot;word&quot;, group = &quot;doc_id&quot;) stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword)) barchart(key ~ pmi, data = head(subset(stats, freq &gt; 3), 20), col = &quot;cadetblue&quot;, main = &quot;Keywords identified by PMI Collocation&quot;, xlab = &quot;PMI (Pointwise Mutual Information)&quot;) ## Using a sequence of POS tags (noun phrases / verb phrases) dee.df$phrase_tag &lt;- as_phrasemachine(dee.df$upos, type = &quot;upos&quot;) stats &lt;- keywords_phrases(x = dee.df$phrase_tag, term = tolower(dee.df$token), pattern = &quot;(A|N)*N(P+D*(A|N)*N)*&quot;, is_regex = TRUE, detailed = FALSE) stats &lt;- subset(stats, ngram &gt; 1 &amp; freq &gt; 3) stats$key &lt;- factor(stats$keyword, levels = rev(stats$keyword)) barchart(key ~ freq, data = head(stats, 20), col = &quot;cadetblue&quot;, main = &quot;Keywords - simple noun phrases&quot;, xlab = &quot;Frequency&quot;) library(igraph) library(ggraph) library(ggplot2) ## Co-occurrences allow to see how words are used either in the same sentence or next to each other. # This R package make creating co-occurrence graphs using the relevant Parts of Speech tags as easy as possible. # Nouns / adjectives used in same sentence cooc &lt;- cooccurrence(x = subset(dee.df, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;)), term = &quot;lemma&quot;, group = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;)) wordnetwork &lt;- head(cooc, 30) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;blue&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Cooccurrences within sentence&quot;, subtitle = &quot;Nouns &amp; Adjective&quot;) ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead cooc &lt;- cooccurrence(dee.df$lemma, relevant = dee.df$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;), skipgram = 1) head(cooc) ## term1 term2 cooc ## 1 young man 44 ## 2 private office 33 ## 3 silver piece 27 ## 4 next morning 24 ## 5 court hall 21 ## 6 raw silk 19 wordnetwork &lt;- head(cooc, 15) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc)) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + labs(title = &quot;Words following one another&quot;, subtitle = &quot;Nouns &amp; Adjective&quot;) ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead ## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;Arial Narrow&#39; not found, ## will use &#39;sans&#39; instead dee.df$id &lt;- unique_identifier(dee.df, fields = c(&quot;sentence_id&quot;, &quot;doc_id&quot;)) dtm &lt;- subset(dee.df, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;)) dtm &lt;- document_term_frequencies(dtm, document = &quot;id&quot;, term = &quot;lemma&quot;) dtm &lt;- document_term_matrix(dtm) dtm &lt;- dtm_remove_lowfreq(dtm, minfreq = 5) termcorrelations &lt;- dtm_cor(dtm) y &lt;- as_cooccurrence(termcorrelations) y &lt;- subset(y, term1 &lt; term2 &amp; abs(cooc) &gt; 0.2) y &lt;- y[order(abs(y$cooc), decreasing = TRUE), ] head(y) ## term1 term2 cooc ## 50151 burner incense 0.8614180 ## 174829 piece silver 0.8497441 ## 180729 musty smell 0.8330833 ## 25092 consideration favourable 0.7742093 ## 126705 office private 0.7355215 ## 153136 propriety rule 0.6539167 ## Define the identifier at which we will build a topic model dee.df$topic_level_id &lt;- unique_identifier(dee.df, fields = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;)) ## Get a data.frame with 1 row per id/lemma dtf &lt;- subset(dee.df, upos %in% c(&quot;NOUN&quot;)) dtf &lt;- document_term_frequencies(dtf, document = &quot;topic_level_id&quot;, term = &quot;lemma&quot;) head(dtf) ## doc_id term freq ## 1: 2 MAGISTRATE 1 ## 2: 3 people 1 ## 3: 3 Tribunal 1 ## 4: 3 Grievance 1 ## 5: 4 people 1 ## 6: 4 magistrate 1 ## Create a document/term/matrix for building a topic model dtm &lt;- document_term_matrix(x = dtf) ## Remove words which do not occur that much dtm_clean &lt;- dtm_remove_lowfreq(dtm, minfreq = 5) head(dtm_colsums(dtm_clean)) ## accusation adder admiration affair afternoon age ## 14 7 5 46 12 11 ############################################################ # Use dependency parsing output to get the nominal subject and the adjective of it stats &lt;- merge(dee.df, dee.df, by.x = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;head_token_id&quot;), by.y = c(&quot;doc_id&quot;, &quot;paragraph_id&quot;, &quot;sentence_id&quot;, &quot;token_id&quot;), all.x = TRUE, all.y = FALSE, suffixes = c(&quot;&quot;, &quot;_parent&quot;), sort = FALSE) stats &lt;- subset(stats, dep_rel %in% &quot;nsubj&quot; &amp; upos %in% c(&quot;NOUN&quot;) &amp; upos_parent %in% c(&quot;ADJ&quot;)) stats$term &lt;- paste(stats$lemma_parent, stats$lemma, sep = &quot; &quot;) stats &lt;- txt_freq(stats$term) library(wordcloud) wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, max.words = 100, random.order = FALSE, colors = c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;, &quot;#7570B3&quot;, &quot;#E7298A&quot;, &quot;#66A61E&quot;, &quot;#E6AB02&quot;)) ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : auspicious graveyard could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : obscure explanation could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : obscure dream could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : happy none could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : worth land could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : criminal fellow could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : overjoy ma could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : cheaper price could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : overjoy manager could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : alive manager could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : reasonable price could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : present position could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : lighted lantern could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : better none could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : bad market could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : willing waiter could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : bad reputation could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : content magistrate could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : strong fellow could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : involved money could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : greater danger could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : official magistrate could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : official judge could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : full people could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : successful expedition could not be fit ## on page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : light penalty could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : important ceremony could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : capable one could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : lenient authority could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : clean cup could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : clear case could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : busy cook could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : busy cooks could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : asleep master could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : regular feature could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : suspicious affair could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : nobody shape could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : strict magistrate could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : full compound could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : rash magistrate could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : formidable ignorance could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : opposite quarter could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : astonish crowd could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : clear coast could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : poor reasoning could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : ready constable could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : obtained confession could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : alive mother could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : worth dream could not be fit on page. It ## will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : able daughter could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : dark tribunal could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : lighte candle could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : confused part could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : alive husband could not be fit on page. ## It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : important thing could not be fit on ## page. It will not be plotted. ## Warning in wordcloud(words = stats$key, freq = stats$freq, min.freq = 5, : bare neck could not be fit on page. It ## will not be plotted. # install.packages(&quot;devtools&quot;) # install.packages(&quot;devtools&quot;) # devtools::install_github(&quot;timmarchand/dtagger&quot;) # library(dtagger) # ## basic example code # dtag.df = dtag_directory(path = &quot;data/ch6/detectives&quot;, # n = NULL, ST = FALSE, deflated = TRUE) # # # # Example text: # text &lt;- &quot;This is an example sentence to be tagged&quot; # # Example speech, tokenized: # speech &lt;- c(&quot;I&quot;,&quot;don&#39;t&quot;, &quot;know&quot; , &quot;erm&quot; ,&quot;,&quot;, &quot;whether&quot; , &quot;to&quot; , # &quot;include&quot; ,&quot;hesitation&quot; , &quot;markers&quot;, &quot;.&quot;) # # Initiate udpipe model # init_udpipe_model() # # udmodel_eng &lt;- udpipe_load_model(file = &quot;data/ch6/english-ewt-ud-2.5-191206.udpipe&quot;) # # Tag text # add_st_tags( udmodel_eng, text) # # Tag speech # add_st_tags(speech, st_hesitation = TRUE, tokenized = TRUE) # "],["第七章-语音数据分析.html", "Chapter 7 第七章 语音数据分析 7.1 语音库创建 7.2 时长提取 7.3 共振峰提取 7.4 基频分析", " Chapter 7 第七章 语音数据分析 语音数据在我们的生活中无处不在，从微信语音消息到智能音箱，不断提高着我们的生活质量。语音是声音的一种，是由人类的发音器官发出的声音。从声学上，语音是一种声波。对于声波我们可以分析它的时长和频率。常见的语音分析软件有Praat，可以提供各种各样的语音声学指标的提取。用Praat提取的结构化语音数据，经过数据整理，数据转化，可以像语言实验数据一样进行分析。 本章我们从数据科学的角度，介绍R语言中的一个语音库构建程序包EMU-R。 EMU语音库管理系统（the EMU Speech Database Management System，简称 EMU-SDMS）是集语音库建立、控制、检索、分析和管理为一体的多种软件工具的集合体。该系统最早由澳大利亚麦考瑞大学的语音学家Harrington开发的mu+系统(Harrington et al., 1993)，后来发展为EMU软件(Cassidy &amp; Harrington, 2001)。为了适应大数据时代的发展，兼容不同平台，Winkelmann等人将EMU软件的整体构想和功能保留，使用R语言及相关工具包作为依托，优化升级创建了新系统，即EMU-SDMS。由于R语言本身是开源免费的开发环境，EMU-SDMS系统也是完全免费供研究人员使用。EMU-SDMS系统包含三个软件部分：emuR工具包负责语音库构建和管理；wrassp工具包负责声学数据的处理；EMU-webApp负责语音数据的标注。 与目前的语音库管理系统相比，EMU-SDMS系统有三个优点。 第一，EMU-SDMS系统的数据结构既包含基于时间（time-based）语音信息，也包含层级性的语音信息。传统的语音标注系统和软件，如Praat必须也只能标注基于时间的语音信息。 第二，EMU-SDMS系统是第一个使用网页端作为语音标注界面的语音数据管理系统。网页端的标注界面可以单独使用，用户可以对自己加载的语音库进行标注。 第三，EMU-SDMS系统可以基于检索结果，在线提取语音库中的声学数据（如基频，共振峰）或发音生理数据（如电磁发音数据，electromagnetic articulography，简称 EMA）。传统的语音所有学数据处理流程是将所有数据提取和保存，再通过检索的方式获得。如果修改数据标注，或者修改检索条件，提取数据将会困难且容易出错。 7.1 语音库创建 语音库的构建离不开录音材料的准备、发音人的选取、录音等工作，本文重点在于如何基于语音数据构建语音库，因此对数据收集不做赘述。EMU-SDMS系统可以基于原始声音数据，直接创建语音库，然后用其EMU-webApp进行语音数据的标注，也可以基于已经标注的语音数据（声音文件和Textgrid文件）来创建语音库。如果是根据原始声音文件直接创建语音库，需要首先自定义语音库标注的层级，每一层的属性，以及层与层的关系。如果是根据已经标注的语音数据，程序包会默认将标注文件的层级设置为语音库的层级。 # load the package library(emuR) library(tidyverse) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) # 设置语音库的位置 corpusPath = &quot;data/ch7/speech_corp&quot; # # # 设置已经标注的语音文件 .wav 和标注文件.TextGrid files的位置 # path2folder = &quot;/Users/chenjuqiang/Desktop/AmE_Consonants/&quot; # # # 基于语音文件和对应标注文件构建 emuDB 语音库 # # 注意：这一步只需要一次，语音库建设完成后不需要重复构建 # convert_TextGridCollection(dir = path2folder, # dbName = &quot;AmE_Consonants&quot;, # targetDir = corpusPath) # # # 将语音库文件位置设置为一个变量 AMECpath = paste(corpusPath, &quot;/AmE_Consonants_emuDB&quot;,sep = &quot;&quot;) # 将语音库载入R语言环境 # 注意这一步每次都需要先运行 AmeC.corp = load_emuDB(AMECpath, verbose = FALSE) # 了解一下语音库 summary(AmeC.corp) ## ## ── Summary of emuDB ─────────────────────────────────────────────────────────────────────────────────────────────── ## Name: AmE_Consonants ## UUID: 6280f273-9c1b-4fae-ae01-99413dda6fb8 ## Directory: /Users/chenjuqiang/Nutstore Files/310_Tutorial/LanguageDS-e/data/ch7/speech_corp/AmE_Consonants_emuDB ## Session count: 1 ## Bundle count: 24 ## Annotation item count: 338 ## Label count: 338 ## Link count: 0 ## ## ── Database configuration ───────────────────────────────────────────────────────────────────────────────────────── ## ## ── SSFF track definitions ── ## ## data frame with 0 columns and 0 rows ## ── Level definitions ── ## name type nrOfAttrDefs attrDefNames ## token SEGMENT 1 token; ## word SEGMENT 1 word; ## segment SEGMENT 1 segment; ## closure SEGMENT 1 closure; ## burst SEGMENT 1 burst; ## aspiration SEGMENT 1 aspiration; ## ── Link definitions ── ## data frame with 0 columns and 0 rows 我们可以看到，构建的语音库由24个语音文件（Bundle count），在这些文件中共有338个标注信息。每个文件有6层，分别为token, word, segment, closure, burst, aspiration。需要注意的是，我们如果使用Praat对语音文件进行标注，最好再标注工作开始之前确定好标注层数和名称，并用脚本生成对于的标注文件，确保语音库中所有标注文件的层数和名称（包括大小写）是完全相同的。否则会出现无法构建语音库的情况。 7.1.1 检索相关语音信息 EMU-SDMS系统的最大优点是拥有强大的检索系统，可以完成固定语音元素查找（例如某个元音），基于正则表达式的模糊查找，以及不同层级多重条件限定的复杂查找。其中，不同层级多重条件限定的复杂查找，可以回答研究问题，比如元音的高低是否收到所在语音环境包括辅音，音节位置，甚至句中位置的影响。在传统的语音库数据处理中，这种检索实现很难，需要复杂的步骤，因为传统的语音库标注中每个层级之间是没有关联的。EMU-SDMS系统的灵活检索，使得语音库研究的数据探索更加高效。 EMU-SDMS系统的另一个优点是基于检索结果的声学分析。传统的语音库数据提取完成后会保存下来，占据很大的空间，特别是语音库标注修改过程中版本控制很难。EMU-SDMS系统可以基于检索结果直接通过wrassp程序包进行声学分析，可以快速的得到所研究的语音目标的声学特征，而不要对于整个语料库进行声学分析。这使得语音分析的流程更加高效。 EMU-SDMS系统可以无缝和R语言的其他数据处理，可视化，建模的程序包对接，不需要产生不必要的中间数据，减少版本或人工操作错误的可能性。 下面我们分别提取语音中的三个常见指标，时长、基频和共振峰。 # 通过检索函数检索语音库的内容 query(emuDBhandle = AmeC.corp, query = &quot;segment =~ .*&quot;)%&gt;% head() ## # A tibble: 6 × 16 ## labels start end db_uuid session bundle start_item_id end_item_id level attribute start_item_seq_idx ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 0 74.9 6280f273-9c1b-4fa… 0000 azure 7 7 segm… segment 1 ## 2 &quot;\\\\ae&quot; 74.9 191. 6280f273-9c1b-4fa… 0000 azure 8 8 segm… segment 2 ## 3 &quot;\\\\zh&quot; 191. 305. 6280f273-9c1b-4fa… 0000 azure 9 9 segm… segment 3 ## 4 &quot;u&quot; 305. 498. 6280f273-9c1b-4fa… 0000 azure 10 10 segm… segment 4 ## 5 &quot;\\\\sw&quot; 498. 651. 6280f273-9c1b-4fa… 0000 azure 11 11 segm… segment 5 ## 6 &quot;&quot; 651. 735. 6280f273-9c1b-4fa… 0000 azure 12 12 segm… segment 6 ## # ℹ 5 more variables: end_item_seq_idx &lt;int&gt;, type &lt;chr&gt;, sample_start &lt;int&gt;, sample_end &lt;int&gt;, sample_rate &lt;int&gt; 7.2 时长提取 # 检索语音库中的鼻音和爆破音 nasals = query(AmeC.corp, query = &quot;segment==n&quot;)%&gt;% mutate(type = &quot;nasals&quot;) plosives = query(AmeC.corp, &quot;segment==b|p|t|d|k|g&quot;)%&gt;% mutate(type = &quot;plosives&quot;) #计算各自的时长 duration.df = rbind(nasals,plosives)%&gt;% mutate(duration = end-start)%&gt;% select(labels, type, duration, bundle) duration.df ## # A tibble: 10 × 4 ## labels type duration bundle ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 n nasals 148. chin ## 2 n nasals 165. gin ## 3 n nasals 53.7 nigh ## 4 b plosives 18.5 buy ## 5 d plosives 17.1 die ## 6 g plosives 21.2 guy ## 7 k plosives 82.2 kite ## 8 t plosives 231. kite ## 9 p plosives 97 pie ## 10 t plosives 74.5 tie 7.3 共振峰提取 # 元音ai在Praat中的标注是“a\\\\ic”, vwl = query(AmeC.corp, query = &quot;segment== a\\\\ic&quot;) # 提取共振峰信息 # 所得表格中最后为共振峰信息，T1、T2、T3、T4 vwl.fm = get_trackdata(AmeC.corp, vwl, onTheFlyFunctionName = &quot;forest&quot;, resultType = &quot;emuRtrackdata&quot;) ## Warning in get_trackdata(AmeC.corp, vwl, onTheFlyFunctionName = &quot;forest&quot;, : The emusegs/emuRsegs object passed in refers to bundles with in-homogeneous sampling rates in their audio files! Here is a list of all refered to bundles incl. their sampling rate: ## session bundle media_file sample_rate md5_annot_json ## 1 0000 buy buy.wav 20000 59639695211954ddb3e36ff5978900e7 ## 2 0000 die die.wav 20000 893d54219221b71cef3fdb6f7b5e3d98 ## 3 0000 fie fie.wav 20000 bea76ca68ea7e633942ae47b297c6b8f ## 4 0000 guy guy.wav 20000 16488b8b385da79c6e4243fbb0bb0eb3 ## 5 0000 high high.wav 20000 ec2a22f18c2854e68b0851e79c7fcfb3 ## 6 0000 kite kite.wav 20000 e8c91461c11d0c80435ddf54e56afcbd ## 7 0000 lie lie.wav 20000 09954d57f5b94a7c0d5763f154712ad5 ## 8 0000 my my.wav 20000 1c5333b5c56ff005e37c1bd249b71819 ## 9 0000 nigh nigh.wav 20000 1a9efacfafe1718d9b51322a3d71af14 ## 10 0000 pie pie.wav 20000 b3d05e6277fe8892e7117fe25ef48460 ## 11 0000 rye rye.wav 20000 d8d76184b99f972d62290432c571e2ca ## 12 0000 shy shy.wav 20000 359535c49c56d59f8a99aed2b78dd288 ## 13 0000 sigh sigh.wav 20000 28b3d66baf0232553a744a8fcfffe498 ## 14 0000 thigh thigh.wav 20000 470573b53a3e657a7400f3769304665d ## 15 0000 thy thy.wav 20000 ab204ada14eaee2ad3ae12c0954fb849 ## 16 0000 tie tie.wav 20000 7f03f2dbafe1e8125eef39bbf4b92997 ## 17 0000 vie vie.wav 20000 eb769439797466b5903acb51fac43694 ## 18 0000 why why.wav 22050 357bff9530cf2f5145a46e3f059865f2 ## ## INFO: applying forest to 18 segments/events ## | | | 0% | |====== | 6% | |============ | 11% | |================== | 17% | |======================= | 22% | |============================= | 28% | |=================================== | 33% | |========================================= | 39% | |=============================================== | 44% | |==================================================== | 50% | |========================================================== | 56% | |================================================================ | 61% | |====================================================================== | 67% | |============================================================================ | 72% | |================================================================================== | 78% | |======================================================================================== | 83% | |============================================================================================= | 89% | |=================================================================================================== | 94% | |=========================================================================================================| 100% # load package library(ggplot2) # 第一共振峰 ggplot(vwl.fm ) + aes(x = times_rel, y = T1, col = labels, group = sl_rowIdx) + geom_line() + labs(x = &quot;Duration (ms)&quot;, y = &quot;F1 (Hz)&quot;) # 将共振峰信息进行时长归一化 td_vowels_norm = normalize_length(vwl.fm) ggplot(td_vowels_norm) + aes(x = times_norm, y = T1, col = labels, group = labels) + geom_smooth() + labs(x = &quot;Duration (normalized)&quot;, y = &quot;F1 (Hz)&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; # 下面我们提取共振峰中最稳定的部分，中间点 td_vowels_midpoint = td_vowels_norm %&gt;% filter(times_norm == 0.5) # 计算元音在一、二共振峰的中心点 td_centroids = td_vowels_midpoint %&gt;% group_by(labels) %&gt;% summarise(T1 = mean(T1), T2 = mean(T2)) # 画元音的位置图 ggplot(td_vowels_midpoint, aes(x = T2, y = T1, colour = labels, label = labels)) + geom_text(data = td_centroids) + stat_ellipse() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position=&quot;none&quot;) 7.4 基频分析 基频是语音中音高的声学基础。所有响音（如元音和鼻音）都有基频。我们说话的语调也是由基频的变化体现的。此外，一些语言如汉语使用基频区分不同词的意思，又称声调。 vwl.f0 = get_trackdata(AmeC.corp, vwl, onTheFlyFunctionName = &quot;ksvF0&quot;, resultType = &quot;emuRtrackdata&quot;) ## Warning in get_trackdata(AmeC.corp, vwl, onTheFlyFunctionName = &quot;ksvF0&quot;, : The emusegs/emuRsegs object passed in refers to bundles with in-homogeneous sampling rates in their audio files! Here is a list of all refered to bundles incl. their sampling rate: ## session bundle media_file sample_rate md5_annot_json ## 1 0000 buy buy.wav 20000 59639695211954ddb3e36ff5978900e7 ## 2 0000 die die.wav 20000 893d54219221b71cef3fdb6f7b5e3d98 ## 3 0000 fie fie.wav 20000 bea76ca68ea7e633942ae47b297c6b8f ## 4 0000 guy guy.wav 20000 16488b8b385da79c6e4243fbb0bb0eb3 ## 5 0000 high high.wav 20000 ec2a22f18c2854e68b0851e79c7fcfb3 ## 6 0000 kite kite.wav 20000 e8c91461c11d0c80435ddf54e56afcbd ## 7 0000 lie lie.wav 20000 09954d57f5b94a7c0d5763f154712ad5 ## 8 0000 my my.wav 20000 1c5333b5c56ff005e37c1bd249b71819 ## 9 0000 nigh nigh.wav 20000 1a9efacfafe1718d9b51322a3d71af14 ## 10 0000 pie pie.wav 20000 b3d05e6277fe8892e7117fe25ef48460 ## 11 0000 rye rye.wav 20000 d8d76184b99f972d62290432c571e2ca ## 12 0000 shy shy.wav 20000 359535c49c56d59f8a99aed2b78dd288 ## 13 0000 sigh sigh.wav 20000 28b3d66baf0232553a744a8fcfffe498 ## 14 0000 thigh thigh.wav 20000 470573b53a3e657a7400f3769304665d ## 15 0000 thy thy.wav 20000 ab204ada14eaee2ad3ae12c0954fb849 ## 16 0000 tie tie.wav 20000 7f03f2dbafe1e8125eef39bbf4b92997 ## 17 0000 vie vie.wav 20000 eb769439797466b5903acb51fac43694 ## 18 0000 why why.wav 22050 357bff9530cf2f5145a46e3f059865f2 ## ## INFO: applying ksvF0 to 18 segments/events ## | | | 0% | |====== | 6% | |============ | 11% | |================== | 17% | |======================= | 22% | |============================= | 28% | |=================================== | 33% | |========================================= | 39% | |=============================================== | 44% | |==================================================== | 50% | |========================================================== | 56% | |================================================================ | 61% | |====================================================================== | 67% | |============================================================================ | 72% | |================================================================================== | 78% | |======================================================================================== | 83% | |============================================================================================= | 89% | |=================================================================================================== | 94% | |=========================================================================================================| 100% # 此时T1表示的是基频FO ggplot(vwl.f0 ) + aes(x = times_rel, y = T1, col = labels, group = sl_rowIdx) + geom_line() + labs(x = &quot;Duration (ms)&quot;, y = &quot;F0 (Hz)&quot;) # 将基频信息进行时长归一化 f0_norm = normalize_length(vwl.f0) ggplot(f0_norm) + aes(x = times_norm, y = T1, col = labels, group = labels) + geom_smooth() + labs(x = &quot;Duration (normalized)&quot;, y = &quot;F0 (Hz)&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; "],["第八章-统计建模.html", "Chapter 8 第八章 统计建模 8.1 因变量和自变量均为分类变量 8.2 因变量为连续变量而自变量为分类变量 8.3 因变量为连续变量，自变量为分类变量 8.4 因变量与自变量均为连续变量", " Chapter 8 第八章 统计建模 在之前的章节中，我们介绍了数据的描述性统计和可视化方法。这些方法可以帮助我们对数据本身的基本趋势有所了解。然而在科学研究中，我们收集到的数据是我们研究问题中研究对象的样本。如何将我们在样本数据上观察到的结果推广到总体中去呢？这个时候我们需要借助推断性统计（inferential statistics）。 统计建模中，我们不仅需要明确变量是分类变量还是连续变量，还有明确变量之间的关系（因变量和自变量）。 因变量（Dependent Variable）是研究中被测量或观察的变量，它是研究者感兴趣的结果。因变量的变化被假设为由自变量的变化引起的。在实验或研究中，因变量通常是研究者希望解释或预测的现象。 自变量（Independent Variable） 自变量是研究中被操纵或分类的变量，它是研究者用来影响因变量的因素。自变量是研究中主动改变或控制的条件，用于测试其对因变量的影响。在一个研究咖啡因对学习成绩影响的实验中，咖啡因的摄入量就是自变量，学生的学习成绩就是因变量。 此外，在选择统计模型时我们还要考虑数据本身的特点是否满足一些模型的需要。参数统计检验（Parametric Statistical Tests）和非参数统计检验（Non-Parametric Statistical Tests）是统计分析中两类主要的方法，它们在假设前提和应用场景上有所不同。 参数统计检验，通常假设数据来自一个正态分布（即正态性假设），并且数据为连续型。这类检验依赖于数据参数（如均值和标准差），因此得名为参数统计检验。常见的参数检验：t检验（t-test，用于比较两个样本均值之间的差异）、方差分析（ANOVA，用于比较多个样本均值之间的差异）、线性回归（Linear Regression，用于分析两个或多个变量之间的线性关系）。 在假设满足的情况下，参数统计检验通常具有更高的统计效率（即更强的检验能力）。如果假设不满足，结果可能不可靠。 非参数统计检验不依赖于数据的特定分布假设，适用于数据不满足参数统计检验的假设前提的情况，特别是当数据为分类数据时。 常见的非参数检验：Mann-Whitney U检验（用于比较两组独立样本的秩次）、Wilcoxon符号秩检验（用于比较两组配对样本的秩次）、 斯皮尔曼相关系数（Spearman’s Rank Correlation，用于分析两个变量之间的秩次相关性）。 非参数统计检验能处理小样本和异常值。但是统计效率通常低于参数统计检验，可能需要更大的样本量来达到相同的统计检验能力。 图8.1 统计建模 library(tidyverse) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) 8.1 因变量和自变量均为分类变量 如果统计建模的因变量和自变量均为分类变量，我们可以使用卡方检验（Chi-Square Test）。卡方检验是一种常用的非参数统计检验方法，通过比较观察频数和期望频数来判断变量之间是否存在显著关系。 卡方独立性检验用于检验两个分类变量之间是否存在关联，通常适用于二维列联表（contingency table）。它回答的问题是：一个变量的分类是否依赖于另一个变量的分类。 library(languageR) data(&quot;lexdec&quot;) tbl= table(lexdec$Correct, lexdec$PrevType) tbl ## ## nonword word ## correct 825 769 ## incorrect 30 35 # 卡方检验 chisq.test(tbl) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tbl ## X-squared = 0.57663, df = 1, p-value = 0.4476 8.2 因变量为连续变量而自变量为分类变量 当因变量为分类变量而自变量为连续变量时，我们可以使用逻辑回归（Logistic Regression）。逻辑回归是一种广泛使用的统计模型，特别适用于处理分类问题，尤其是二分类问题。它通过建立自变量和因变量之间的关系来预测因变量的类别概率。逻辑回归通过一个逻辑函数（Logistic Function），也称为Sigmoid函数，将自变量的线性组合转换为一个概率值。它输出的结果是一个在0到1之间的概率值，用来表示某个事件发生的可能性。 #fit logistic regression model model &lt;- glm(Correct ~ Frequency+FamilySize, family=&quot;binomial&quot;, data=lexdec) #disable scientific notation for model summary options(scipen=999) #view model summary summary(model) ## ## Call: ## glm(formula = Correct ~ Frequency + FamilySize, family = &quot;binomial&quot;, ## data = lexdec) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5767 -0.3081 -0.2394 -0.1848 3.0482 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5871 0.5070 -1.158 0.247 ## Frequency -0.6266 0.1407 -4.453 0.00000845 *** ## FamilySize 0.2064 0.2541 0.812 0.417 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 548.57 on 1658 degrees of freedom ## Residual deviance: 518.48 on 1656 degrees of freedom ## AIC: 524.48 ## ## Number of Fisher Scoring iterations: 6 caret::varImp(model) ## Overall ## Frequency 4.4533767 ## FamilySize 0.8123094 8.3 因变量为连续变量，自变量为分类变量 如果自变量只有两层，可以使用t检验（T-tests）。t检验是一种常见的统计检验方法，用于比较两个样本均值之间的差异。单样本t检验（One-Sample T-test）用于比较一个样本的均值与一个已知的总体均值之间的差异。应用前提是，数据是从正态分布的总体中抽取的。样本是独立随机抽取的。比如，假设某学校声称其学生的平均IQ为100。我们抽取一个班级的学生进行IQ测试，想验证这个班级的平均IQ是否显著不同于100。 独立样本t检验（Independent Samples T-test），用于比较两个独立样本的均值是否显著不同。应用前提是两个样本来自正态分布的总体。两个样本是独立的，即每个样本中的观测值不依赖于另一个样本的观测值。 两个样本的方差相等（同方差性假设）。当不满足同方差性假设时，可以使用Welch’s t检验。 比如，我们想比较男性和女性的平均工资，抽取了一组男性和女性的工资数据，检验两者的平均工资是否有显著差异。 配对样本t检验（Paired Samples T-test）用于比较两个相关样本的均值是否显著不同。通常用于前后测量或配对设计。应用前提是配对差值（即每对观测值的差值）来自正态分布。样本是成对的，每对数据之间有某种相关性。比如，我们想比较一组学生在参加培训前后的考试成绩，检查培训是否对考试成绩有显著影响。 # 检测数据是否符合正态分布，P&lt;0.05 不符合正态分布 shapiro.test(lexdec$RT) ## ## Shapiro-Wilk normality test ## ## data: lexdec$RT ## W = 0.94738, p-value &lt; 0.00000000000000022 plot(density(lexdec$RT)) # 参与者母语是否是英语 t.test(RT ~ NativeLanguage, data=lexdec) ## ## Welch Two Sample t-test ## ## data: RT by NativeLanguage ## t = -13.184, df = 1267.7, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true difference in means between group English and group Other is not equal to 0 ## 95 percent confidence interval: ## -0.1790089 -0.1326335 ## sample estimates: ## mean in group English mean in group Other ## 6.318309 6.474130 # 汇报Mann–Whitney U test 更好 wilcox.test(RT ~ NativeLanguage, data=lexdec) ## ## Wilcoxon rank sum test with continuity correction ## ## data: RT by NativeLanguage ## W = 212168, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true location shift is not equal to 0 当自变量有两个以上，或者自变量超过两个水平，我们可以使用方差分析（Analysis of Variance, ANOVA）。方差分析通过分析数据中的变异来源，确定不同组之间的均值是否有显著差异。ANOVA的主要优势在于它能够同时比较多个组的均值，而无需进行多次t检验，从而减少第一类错误的概率。 单因素方差分析（One-Way ANOVA）：用于比较一个因子（自变量）下的多个组（水平）的均值是否存在显著差异。 比如比较不同教学方法对学生成绩的影响。 双因素方差分析（Two-Way ANOVA）：用于比较两个因子（自变量）及其交互作用对因变量的影响。 适用场景：研究不同教学方法和不同学习时间对学生成绩的影响。 重复测量方差分析（Repeated Measures ANOVA）：用于处理同一组受试者在不同时间点或不同条件下的测量数据。比如，同一组学生在不同教学方法下的成绩变化等。 ANOVA 的基本原理是通过将总变异分解为组间变异和组内变异，来判断不同组的均值是否存在显著差异。 在发现整体组间均值存在显著差异后，进一步确定哪些具体组之间存在差异需要使用事后比较（Post-Hoc Comparisons）。因为ANOVA只告诉我们至少有一组均值不同，但不能明确指出哪些组之间存在差异。事后比较中，我们要控制多重比较带来的第一类错误（即假阳性）。常见的方法有Tukey’s HSD（Honestly Significant Difference）检验、Bonferroni检验等。 lex.aov = aov (RT ~ PrevType + Class, data = lexdec) summary(lex.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## PrevType 1 1.71 1.7144 29.891 0.0000000527 *** ## Class 1 0.01 0.0119 0.208 0.648 ## Residuals 1656 94.98 0.0574 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(lex.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = RT ~ PrevType + Class, data = lexdec) ## ## $PrevType ## diff lwr upr p adj ## word-nonword -0.06432275 -0.08739882 -0.04124667 0.0000001 ## ## $Class ## diff lwr upr p adj ## plant-animal -0.005397916 -0.02861424 0.01781841 0.6484245 8.4 因变量与自变量均为连续变量 当我们为两个或以上连续型变量建模时，我们可以使用相关分析和线性回归。 相关分析确定一个变量是否随着另一个变量的变化而系统地变化。它不指定哪个变量是因变量，哪个变量是自变量。比较合适观察数据集中哪些变量彼此相关。Pearson, Spearman 和 Kendall 是三种常见的相关性检验方法，主要用于衡量两个变量之间的相关程度。尽管它们都用于相关性分析，但它们有不同的假设和适用场景。 Pearson相关系数（Pearson Correlation Coefficient）是最常用的相关性测量方法，用于衡量两个变量之间的线性关系。假设前提：数据是连续的，符合正态分布，数据之间存在线性关系。数据是独立的。简单易计算，结果易于解释。在数据符合正态分布且有线性关系时，效果最好。对异常值敏感。不能有效处理非线性关系。 Spearman相关系数（Spearman Rank Correlation Coefficient）是基于数据排序的非参数检验方法，用于衡量两个变量之间的单调关系。假设前提有数据是连续的或离散的。数据不需要符合正态分布。数据之间存在单调关系。适用于衡量两个变量之间的单调关系，例如学生成绩排名和运动成绩排名的关系。对异常值不敏感。可以处理非线性关系。对于完全线性关系，效率不如Pearson相关系数。 cor.test( ~ RT + Frequency, data = lexdec, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: RT and Frequency ## t = -9.4587, df = 1657, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2715046 -0.1801720 ## sample estimates: ## cor ## -0.2263358 cor.test( ~ RT + Frequency, data = lexdec, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: RT and Frequency ## z = -9.5751, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.1581668 cor.test( ~ RT + Frequency, data = lexdec, method = &quot;spearman&quot;) ## Warning in cor.test.default(x = mf[[1L]], y = mf[[2L]], ...): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: RT and Frequency ## S = 937771196, p-value &lt; 0.00000000000000022 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.2322793 线性回归指定一个变量为自变量，另一个为因变量。所得模型用线性关系来描述这些变量之间的关系。线性回归是参数检验，假设残差的正态性、同方差性和独立性，以及两个变量之间的线性关系。如果模型中有多个区间/比例类型的自变量，那么线性回归将扩展为多元回归。如果自变量是分类变量，那么线性回归将变成单因素方差分析。可以使用 lm 函数来执行线性回归，这与我们用于方差分析的函数相同。 lm.model = lm(RT ~ Frequency, data = lexdec) summary(lm.model) ## ## Call: ## lm(formula = RT ~ Frequency, data = lexdec) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.55407 -0.16153 -0.03494 0.11699 1.08768 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.588778 0.022296 295.515 &lt;0.0000000000000002 *** ## Frequency -0.042872 0.004533 -9.459 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2353 on 1657 degrees of freedom ## Multiple R-squared: 0.05123, Adjusted R-squared: 0.05066 ## F-statistic: 89.47 on 1 and 1657 DF, p-value: &lt; 0.00000000000000022 lm模型对象的summary函数包括对模型参数（截距和斜率）的估计，以及模型的R平方值和模型的p值。 如何解读模型？ 模型产生了截距的系数（6.5）和斜率的系数（-0.04）； 每个系数还伴随着其他三个数字：其标准误差、t值和p值。p值告诉我们系数是否显著不同于零。 如果预测变量的系数为零，则预测变量与因变量之间没有任何关系，因此作为预测变量是毫无价值的。为了确定系数是否显著不同于零，从而具有潜在的预测能力，进行双侧t检验，使用t值和相关的自由度。 t值本身是系数除以其标准误差得到的值。这个标准误差是系数估计的确定程度的衡量。标准误差越小，估计周围的置信区间就越小，接受区域中包含零的可能性就越小，因此系数可能为零的概率也越小。 残差标准误差是模型不成功的度量；它衡量了我们无法通过预测变量处理的因变量的变异性。模型越好，其残差标准误差就越小。 多重R平方为0.8115。这个R平方是平方相关系数r²，它在0到1的范围内量化了模型解释的方差的比例。 "],["第九章-机器学习建模基础.html", "Chapter 9 第九章 机器学习建模基础 9.1 监督学习 9.2 非监督学习", " Chapter 9 第九章 机器学习建模基础 对于某一个观测对象收集多个不同的变量（或者叫特征）形成的数据称为多元数据（Multivariate Data） 。例如，在一个关于学生成绩的研究中，你可能会同时记录每个学生的多门课程成绩（如数学、语文、英语等），这些不同课程的成绩就构成了多变量数据。 多元数据的分析可以通过统计建模，也可以通过机器学习建模。 在本章中，我们讨论了两种发现多变量数据集结构的方法。在一种方法中，我们试图通过观测值的分组来发现数据中的结构。这些技术是无监督的（unsupervised），因为我们并不预先规定应该存在什么样的分组。我们将在“聚类（clustering）”这个标题下讨论这些技术。另一种方法中，我们理论上知道存在哪些分组，问题在于数据是否支持这些分组。这第二类技术可以被描述为监督式的（supervised），因为这些技术在分析数据时使用的是由分析师强加于数据的分组。我们将这些技术称为分类方法（methods for classification）。 机器学习（Machine Learning）是人工智能（Artificial Intelligence, AI）的一个重要分支，其主要目标是使计算机通过学习经验数据自动改进和发展。机器学习使计算机从数据中学习模式和规律，从而做出预测或决策，而无需显式地编程。机器学习主要分为2种主要类型，监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。 根据学习方式和目标任务的不同，它们各自解决不同类型的问题。监督学习是一种通过已标记的训练数据来训练模型，从而预测或估计新数据的输出。在监督学习中，训练数据包含了输入特征和相应的标签。模型学习从输入到标签之间的映射关系，以便对未知数据进行预测。监督学习的任务可以分为分类（Classification）任务即预测离散类别标签，如垃圾邮件检测、疾病诊断。典型算法包括支持向量机（SVM）、逻辑回归、决策树、随机森林等。回归（Regression）任务则是预测连续数值，如房价预测、销售预测。典型算法包括，线性回归、多项式回归、支持向量回归（SVR）等。 无监督学习使用未标记的数据，没有预先定义的输出，其目标是发现数据中的隐藏结构或模式。这种学习方法通常用于探索数据的内在关系，发现数据的聚类、降维或异常检测等特征。常见的无监督学习包括聚类分析（Clustering）和降维（Dimensionality Reduction）。聚类分析将数据集划分为不同的组（簇），使每个组内的数据点相似度最大化，而组间的相似度最小化。典型算法包括K均值聚类、层次聚类等。降维减少数据的特征维度，保留最重要的信息，以便更好地可视化和理解数据，同时降低计算复杂度。典型算法包括主成分分析（PCA）、因子分析、独立成分分析（ICA）等。 9.1 监督学习 下面将用一个二语语音研究的例子讲解如何使用支持向量机（SVM）和随机森林来对中国英语学习者的焦点语音实现进行自动评估。 传统的统计建模（如广义线性模型）在大多数先前研究中用于调查信息焦点的语音实现，这些研究检查整个数据集，并通过离散指标检测焦点类型的效果。通常，基于不同的声学测量构建几个模型，以测试不同焦点和语言组的效果，但很难得到全面的图景。而人类听众则逐一听取话语并据此作出判断。这个过程可以通过机器学习算法来模拟，这些算法输入一组特征并基于这些特征输出分类结果。 支持向量机（SVM）和随机森林模型被用于基于不同的声学测量集分类鼻音元音和口音元音。我们理解，计算感知与人类感知并不完全相同，但计算感知在语音研究中的优势在于它纯粹基于声学特征，因此可以剔除句法或词汇的影响。此外，与识别元音或辅音不同，仅仅给听众一些目标词并测试他们如何识别不同的焦点是困难的。机器学习算法可以在没有上下文的情况下识别不同的焦点，从而从感知的角度接近这个问题。 焦点是话语中提供信息的部分，与句子的背景信息形成对比。为了进一步解释焦点在话语中的作用，以下是具体例子： 广泛焦点（Broad Focus, BF） A: 你明天有什么计划？ B: 我想去纽约。 在这个例子中，B的整个回答都在提供新信息，因此整个句子是焦点。 狭窄焦点（Narrow Focus, NF） A: Karel想带你去哪里？ B: 他想带我去纽约。 在这个例子中，焦点是“纽约”，因为这是B回答A的问题“哪里”的特定信息。 纠正焦点（Corrective Focus, CF） A: 你妈妈想送你去芝加哥吗？ B: 不，她想送我去纽约。 在这个例子中，焦点也是“纽约”，但这次是为了纠正A的问题中的错误信息“芝加哥”。通过这些例子可以看出，焦点的范围和功能可以根据对话中的具体信息需求和信息状态有所不同。 不同语言的母语者使用不同语音手段（如音高重音分布、短语边界、音高范围和持续时间）来表达不同焦点条件下的意义。本研究采用两种机器学习模型，支持向量机（SVM）和随机森林（Random Forest），来探讨基于语音特征的不同类型信息焦点。我们还比较了美国英语母语者（AE）与低熟练度的中国英语学习者（CE1）和高熟练度的中国英语学习者（CE2）在分类准确性和不同语音特征排名方面的差异。此外，我们使用AE的语音数据训练了两个模型，并用CE1和CE2的数据进行测试，以模拟AE听者如何感知非母语者在不同信息焦点下的语音产出。 我们在本研究中使用了两种机器学习分类器，随机森林（RandomForests）和支持向量机（SVM）。随机森林是一种机器学习模型，由一组在数据的随机子集上训练和测试的决策树组成。在本研究中，我们在所有随机森林模型中使用了500棵树，如[9]所述。随机森林分类器检查每棵决策树的输出，并产生分类准确率。此外，随机森林分类器为每个模型提供了直接的特征重要性度量。本研究中使用了R语言中的randomForest包[10]来构建分类器。 然而，随机森林为了其可解释性在分类中牺牲了一些准确性。为了弥补这一点，我们使用了SVM，这在机器学习领域中整体表现良好。SVM通过找到最佳分离数据中不同类别的线来工作。当数据不是线性可分时，可以使用核函数来处理非线性关系。在本研究中，使用了“径向基函数”（radial）核。然而，使用核函数时我们无法得到特征权重。因此，我们主要依赖随机森林来指示每个语音特征的重要性。 此外，由于数据集相对较小（从机器学习的角度来看），我们使用了“10折交叉验证”，即对数据运行10次分析，每次使用不同的9/10数据作为“训练”集，剩余的1/10作为“测试”集。本研究中使用了R语言中的e1071包[11]来构建SVM分类器。 9.1.1 数据导入 library(tidyverse) library(languageR) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) library(cluster) library(factoextra) # install.packages(&#39;caTools&#39;) library(caTools) set.seed(123) # 美国英语本族语者数据 datasetAE &lt;- read_csv(&quot;data/ch9/datasetAE.CSV&quot;) # 中国英语学习者大一 datasetCE1 = read_csv(&quot;data/ch9/datasetCE1.CSV&quot;) # 中国英语学习者大三 datasetCE2 = read_csv(&quot;data/ch9/datasetCE2.CSV&quot;) datasetAE$focus_type = as.factor(datasetAE$focus_type) datasetCE1$focus_type = as.factor(datasetCE1$focus_type) datasetCE2$focus_type = as.factor(datasetCE2$focus_type) # install.packages(&#39;e1071&#39;) library(e1071) 9.1.2 构建SVM模型 svm_AE = svm(formula = focus_type ~ ., data = datasetAE, #regression or classification type = &#39;C-classification&#39;, kernel = &#39;radial&#39;, cost = &quot;2&quot;, cross = 10) # 交叉验证（Cross Validation）是一种用于评估和验证机器学习模型性能的方法，通过将数据集划分为多个子集，交替进行训练和测试，以确保模型在不同的数据集上都具有良好的泛化能力。最常见的方法是K折交叉验证（K-Fold Cross Validation），将数据集划分为K个大小大致相同的子集，每个子集轮流作为验证集，其余子集作为训练集，重复K次后将结果平均，作为模型的最终性能评估指标。交叉验证可以充分利用数据，提高模型评估的可靠性，防止过拟合问题，是机器学习和数据挖掘中广泛应用的技术。 summary(svm_AE) ## ## Call: ## svm(formula = focus_type ~ ., data = datasetAE, type = &quot;C-classification&quot;, kernel = &quot;radial&quot;, ## cost = &quot;2&quot;, cross = 10) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 2 ## ## Number of Support Vectors: 174 ## ## ( 58 61 55 ) ## ## ## Number of Classes: 3 ## ## Levels: ## bro cor nar ## ## 10-fold cross-validation on training data: ## ## Total Accuracy: 42.24599 ## Single Accuracies: ## 50 42.10526 36.84211 33.33333 42.10526 31.57895 55.55556 52.63158 36.84211 42.10526 svm_AE$tot.accuracy ## [1] 42.24599 svm_AE$accuracies ## [1] 50.00000 42.10526 36.84211 33.33333 42.10526 31.57895 55.55556 52.63158 36.84211 42.10526 9.1.3 random forest library(randomForest) set.seed(123) #AE RF_AE = randomForest(focus_type ~ ., ntree = 500, data = datasetAE, importance = TRUE) importance(RF_AE) ## bro cor nar MeanDecreaseAccuracy MeanDecreaseGini ## Odur 0.5140828 2.1340207 -2.1872216 0.35538483 5.317048 ## Vdur 0.0429139 1.1813533 2.4925858 2.07368177 5.604926 ## Cdur 4.2245241 1.3416428 1.8691133 4.42392485 8.415981 ## Rdur 3.1940865 -1.5403027 1.6942768 2.09952920 5.386566 ## Sylldur -0.7632086 0.7226848 0.1116193 -0.07377943 5.040385 ## Wdur 0.4031192 1.2760254 0.5353081 1.25499400 6.202501 ## O1Hexc 3.2166992 1.3994444 -0.3550104 2.58880098 5.439555 ## Fallexc -0.3784383 4.4299155 6.2340216 6.18100973 6.970555 ## Nucleusfallexc 5.0580749 2.0103623 -2.1740778 2.88325662 7.725118 ## H_N2exc 0.1552991 2.3525700 1.0984669 1.94682400 5.702467 ## pd 0.4784784 0.8331123 -0.6137476 0.71097692 4.687184 ## Pd_rime 3.8104797 -0.2959070 0.3361559 2.10364327 5.994991 ## Falldur 1.1514921 0.2380824 8.1123263 6.18477738 9.538895 ## Risedur 3.9233410 1.3787518 2.9428743 5.15879783 6.624772 ## Hf0_Ef0 1.7389798 3.0422496 3.2327149 4.67262290 5.265986 ## L1_Hexc 2.5726081 0.1309762 3.1346224 3.35089696 5.677258 ## O1f0_Ef0 1.0765409 2.7237252 1.0950456 2.70134078 4.712215 ## L1f0_Ef0 0.9964942 4.8822834 -1.4506798 2.70266139 5.059596 ## L2f0_Ef0 0.6635268 1.0752717 -1.6661622 0.51157262 4.394975 ## Risesp 0.1264366 -0.1587295 2.0082139 0.94940044 5.059293 ## Fallsp -0.1783400 3.6014926 4.0233803 4.18582556 5.154706 varImpPlot(RF_AE, n.var= 5, main =&quot;AE&quot;) # 在构建随机森林的过程中，每棵决策树都是通过对原始训练数据进行有放回抽样（Bootstrap Sampling）得到的子样本训练出来的。剩下样本则没有被该树用于训练，这些样本就是袋外样本。利用那些没有进行训练的数据来进行预测。 RF_AE$confusion ## bro cor nar class.error ## bro 26 22 15 0.5873016 ## cor 21 30 13 0.5312500 ## nar 14 17 29 0.5166667 为了模拟美国英语（AE）听众如何感知由中国英语（CE1和CE2）学习者发出的焦点目标词语，使用所有AE数据特征训练了一个支持向量机（SVM）模型，并分别使用两个中国英语学习者组的数据进行测试。 #Cross langauge preception #svm y_predCE1 = predict(svm_AE, newdata = datasetCE1[-1]) y_predCE2 = predict(svm_AE, newdata = datasetCE2[-1]) #output the data #install.packages(&quot;caret&quot;) library(caret) conf_ce1 = confusionMatrix(as.factor(datasetCE1$focus_type), y_predCE1) conf_ce2 = confusionMatrix(as.factor(datasetCE2$focus_type), y_predCE2) CE1.TEST = as.data.frame(conf_ce1$table)%&gt;% group_by(Reference)%&gt;% mutate(accuracy = Freq/sum(Freq), LANG = &quot;CE1&quot;) CE2.TEST = as.data.frame(conf_ce2$table)%&gt;% group_by(Reference)%&gt;% mutate(accuracy = Freq/sum(Freq), LANG = &quot;CE2&quot;) # 总体准确率在CE1和CE2两组中均高于随机水平，且两组之间的差异较小。在每个组内，BF类词汇相对容易分类，而NF和CF类词汇则较难分类，反映在较低的准确率上。 rbind(CE1.TEST, CE2.TEST)%&gt;% filter(Prediction == Reference) ## # A tibble: 6 × 5 ## # Groups: Reference [3] ## Prediction Reference Freq accuracy LANG ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 bro bro 54 0.535 CE1 ## 2 cor cor 30 0.417 CE1 ## 3 nar nar 27 0.403 CE1 ## 4 bro bro 42 0.494 CE2 ## 5 cor cor 31 0.360 CE2 ## 6 nar nar 33 0.478 CE2 根据机器学习算法，仅基于目标词的语音特征对不同信息焦点进行分类的总体准确率并不高。这从感知的角度支持了目标词本身没有稳健的语音线索来区分广义、狭义和对比焦点的观点。然而，与持续时间相关的特征在重要性指数中的高排名与之前的文献一致。然而，目标词焦点中缺乏稳健的语音特征并不意味着语音信息不能足以建模信息焦点。相反，目标词以外的语音信息可能有助于区分不同的焦点。在文献[3]中，所有焦点后的词的f0峰值均低于这些词在中性焦点句子中的f0峰值。包含焦点后词的语音信息可能会提高分类准确率。 使用AE数据训练的SVM模型对第二语言学习者的发音进行分类，生成的准确率与对AE发音进行分类的结果相当。这表明尽管AE和CE之间存在语音差异，但两个CE组的目标词至少在SVM模型区分这些词的能力上与AE相当。两个CE组之间的差异非常小，表明英语水平并未改变CE学习者的焦点实现模式。 9.2 非监督学习 类别数据 连续数据 非距离矩阵 PCA, FA 距离矩阵 CA, cluster MDS, cluster 从数据的角度，非监督学习中的多元变量可以是连续型或者类别型。连续型变量中有一种特殊的是距离变量。据此我们介绍不同的非监督学习方法。 9.2.1 principal components analysis Xiaofei Lu 的句法复杂性测量是一套用于评估书面或口语语言中句法结构复杂性的量化指标。这些测量广泛应用于二语习得（SLA）研究和应用语言学领域，用于评估学习者在语言产出中的句法复杂性发展情况。 Measure Definition MLS (Mean Length of Sentence) The average number of words per sentence. MLT (Mean Length of T-unit) The average number of words per T-unit. MLC (Mean Length of Clause) The average number of words per clause. C/S (Clauses per Sentence) The ratio of the total number of clauses to the total number of sentences. VP/T (Verb Phrases per T-unit) The ratio of the total number of verb phrases to the total number of T-units. C/T (Clauses per T-unit) The ratio of the total number of clauses to the total number of T-units. DC/C (Dependent Clauses per Clause) The ratio of the total number of dependent clauses to the total number of clauses. DC/T (Dependent Clauses per T-unit) The ratio of the total number of dependent clauses to the total number of T-units. T/S (T-units per Sentence) The ratio of the total number of T-units to the total number of sentences. CT/T (Complex T-units per T-unit) The ratio of the total number of complex T-units (T-units with dependent clauses) to T-units. CP/C (Coordinate Phrases per Clause) The ratio of the total number of coordinate phrases to the total number of clauses. CP/T (Coordinate Phrases per T-unit) The ratio of the total number of coordinate phrases to the total number of T-units. CN/C (Complex Nominals per Clause) The ratio of the total number of complex nominals to the total number of clauses. CN/T (Complex Nominals per T-unit) The ratio of the total number of complex nominals to the total number of T-units. SWT/T (Sentence Weight per T-unit) The ratio of the total number of sentence weight (content words) to the total number of T-units. metrics_SCA &lt;- read_csv(&quot;data/ch9/metrics_SCA.CSV&quot;) ## Rows: 17 Columns: 16 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): text, Registers ## dbl (14): MLS, MLT, MLC, C_S, VP_T, C_T, DC_C, DC_T, T_S, CT_T, CP_T, CP_C, CN_T, CN_C ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sc.pr = prcomp(metrics_SCA[, c(-1,-16)]) names(sc.pr) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; round(sc.pr$sdev, 4) ## [1] 1.8901 0.3342 0.1426 0.0308 0.0217 0.0113 0.0104 0.0048 0.0028 0.0020 0.0003 0.0003 0.0001 0.0001 summary(sc.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 ## Standard deviation 1.8901 0.33416 0.14265 0.03081 0.02170 0.01131 0.01038 0.00483 0.002843 0.002039 0.0003102 ## Proportion of Variance 0.9639 0.03013 0.00549 0.00026 0.00013 0.00003 0.00003 0.00001 0.000000 0.000000 0.0000000 ## Cumulative Proportion 0.9639 0.99405 0.99954 0.99980 0.99993 0.99996 0.99999 1.00000 1.000000 1.000000 1.0000000 ## PC12 PC13 PC14 ## Standard deviation 0.0002823 0.0001049 0.00005446 ## Proportion of Variance 0.0000000 0.0000000 0.00000000 ## Cumulative Proportion 1.0000000 1.0000000 1.00000000 props = round((sc.pr$sdev^2/sum(sc.pr$sdev^2)), 3) barplot(props, col = as.numeric(props &gt; 0.05), xlab = &quot;principal components&quot;, ylab = &quot;proportion of variance explained&quot;) abline(h = 0.05) sc.pr$x[, 1:3] ## PC1 PC2 PC3 ## [1,] -5.6678755 0.39252022 0.18921251 ## [2,] -3.8203320 -0.47080364 -0.19827777 ## [3,] 0.3694866 0.30087970 -0.08225046 ## [4,] 0.6538607 0.15639171 0.26993685 ## [5,] 0.2809699 0.50212353 -0.12599412 ## [6,] 1.6470246 0.41218393 0.04205135 ## [7,] 0.3089003 0.23713423 -0.24996723 ## [8,] 0.4672870 -0.27998203 -0.13950075 ## [9,] 1.3891812 0.33306454 -0.04060635 ## [10,] -0.5097736 -0.07729285 -0.13145459 ## [11,] 0.6227071 -0.13369953 0.05080880 ## [12,] 1.1682836 -0.32690815 0.03747206 ## [13,] 1.4643628 -0.21359235 0.03742893 ## [14,] 0.4945817 -0.06416250 0.01680801 ## [15,] 0.4499698 0.15196303 0.09370500 ## [16,] 0.3906499 -0.43079805 0.19037147 ## [17,] 0.2907160 -0.48902178 0.04025630 library(lattice) super.sym = trellis.par.get(&quot;superpose.symbol&quot;) splom(data.frame(sc.pr$x[,1:3]), groups = metrics_SCA$Registers, panel = panel.superpose, key = list( title = &quot;texts in productivity space&quot;, text = list(c(&quot;trans&quot;, &quot;writing&quot;)), points = list(pch = super.sym$pch[1:2], col = super.sym$col[1:2]))) dim(sc.pr$rotation) ## [1] 14 14 sc.pr$rotation[1:10, 1:3] ## PC1 PC2 PC3 ## MLS -0.780741710 0.458316463 0.39524592 ## MLT -0.583352900 -0.382892409 -0.67206895 ## MLC -0.188837511 -0.718615536 0.56375761 ## C_S -0.050196255 0.185706377 -0.07528150 ## VP_T -0.061850409 0.080929317 -0.13012241 ## C_T -0.032757258 0.077000459 -0.17891375 ## DC_C -0.012533463 0.009274188 -0.01341515 ## DC_T -0.029067939 0.035833749 -0.06333909 ## T_S -0.008350435 0.066380134 0.07835067 ## CT_T -0.019437948 0.032647517 -0.03651945 biplot(sc.pr, scale = 0, var.axes = F, col = c(&quot;darkgrey&quot;, &quot;black&quot;), cex = c(0.9, 1.2)) sc.pr = prcomp(metrics_SCA[ ,c(-1,-16)], scale = T, center = T) biplot(sc.pr, var.axes = F, col = c(&quot;darkgrey&quot;, &quot;black&quot;), cex = c(0.6, 1), xlim = c(-0.42, 0.38)) # k-means clustering [assume 3 clusters] sc.pr$x[,1:3] ## PC1 PC2 PC3 ## [1,] -9.88302972 1.2148695 0.59163495 ## [2,] -6.63655938 -2.1956318 -0.39052086 ## [3,] 0.49126238 1.4827272 -0.18589389 ## [4,] 1.05595265 0.6008529 1.60050109 ## [5,] 0.35914950 2.4850187 -0.88108304 ## [6,] 2.56945811 1.6712688 0.06286037 ## [7,] 0.01189399 1.1950523 -1.15636304 ## [8,] 0.54275580 -1.1540620 -0.02722837 ## [9,] 1.69159168 1.6968221 0.07810719 ## [10,] -0.87660913 -0.1970229 -0.49937926 ## [11,] 1.22073048 -0.4672236 -0.02287285 ## [12,] 2.32603347 -1.5456573 -0.31040472 ## [13,] 3.02073678 -1.1120001 0.30338478 ## [14,] 0.82432037 -0.3313737 0.04813735 ## [15,] 1.01344821 0.7773963 0.73684529 ## [16,] 1.36478492 -1.7573993 0.58619223 ## [17,] 0.90407989 -2.3636371 -0.53391721 km = kmeans(sc.pr$x[,1:3], centers=2, nstart=5) # fviz_cluster(km, data = metrics_SCA[, c(-1,-16)], # palette = c(&quot;#2E9FDF&quot;, &quot;#00AFBB&quot;, &quot;#E7B800&quot;), # geom = &quot;point&quot;, # ellipse.type = &quot;convex&quot;, # ggtheme = theme_bw() # ) # km.df &lt;- data.frame(sc.pr$x[,1:3], # Cluster=km$cluster, # Register = metrics_SCA$Registers) # Coordinates of individuals ind.coord &lt;- as.data.frame(get_pca_ind(sc.pr)$coord) # Add clusters obtained using the K-means algorithm ind.coord$cluster &lt;- factor(km$cluster) # Add Species groups from the original data sett ind.coord$Registers &lt;- metrics_SCA$Registers ind.coord$text &lt;- metrics_SCA$text # Percentage of variance explained by dimensions eigenvalue &lt;- round(get_eigenvalue(sc.pr), 1) variance.percent &lt;- eigenvalue$variance.percent head(eigenvalue) ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 10.9 77.7 77.7 ## Dim.2 2.3 16.4 94.1 ## Dim.3 0.4 3.1 97.2 ## Dim.4 0.3 1.9 99.1 ## Dim.5 0.1 0.5 99.6 ## Dim.6 0.0 0.2 99.8 library(ggpubr) ggscatter(ind.coord, x = &quot;Dim.1&quot;, y = &quot;Dim.2&quot;, color = &quot;cluster&quot;, palette = &quot;npg&quot;, ellipse = TRUE, ellipse.type = &quot;convex&quot;, label = &quot;text&quot;, shape = &quot;Registers&quot;, size = 1.5, legend = &quot;right&quot;, ggtheme = theme_bw(), xlab = paste0(&quot;PC 1 (&quot;, variance.percent[1], &quot;% )&quot; ), ylab = paste0(&quot;PC 2 (&quot;, variance.percent[2], &quot;% )&quot; )) # ggplot(ggdata) + # geom_point(aes(x=PC1, y=PC2, # color=factor(Register)), # size=5, shape=20) + # # stat_ellipse(aes(x=PC1,y=PC2,fill=factor(Cluster)), # # geom=&quot;polygon&quot;, level=0.95, alpha=0.2) + # guides(color=guide_legend(&quot;Register&quot;),fill=guide_legend(&quot;Cluster&quot;)) 9.2.2 因子分析 主成分分析（Principal Components Analysis，简称PCA）的扩展方法是探索性因子分析（Exploratory Factor Analysis，简称EFA）。在主成分分析中，总方差被分配到各个主成分（PCs）中。因此，某个主成分解释的方差比例由该主成分的方差除以所有主成分的总方差得出。然而，在因子分析中，模型中加入了一个误差项，以考虑数据中可能存在的噪音。因此，不再有唯一的一组主成分（现在称为因子）和载荷。相反，通过因子旋转（Factor Rotation）技术，可以获得各种备选的因子（及其载荷）。因子旋转的目的是尽可能简化因子模型的解释。如果变量在少数几个因子上具有高载荷，并且在某一维度上的载荷要么很大要么接近于零，解释将变得更加直观。 sc.fac = factanal(metrics_SCA[ ,c(-1,-16)], factors = 3) loadings = loadings(sc.fac) plot(loadings, type = &quot;n&quot;, xlim = c(-0.4, 1)) text(loadings, rownames(loadings), cex = 0.8) sc.fac2 = factanal(metrics_SCA[ ,c(-1,-16)], factors = 3, rotation = &quot;promax&quot;) loadings2 = loadings(sc.fac2) plot(loadings2, type = &quot;n&quot;, xlim = c(-0.4, 1)) text(loadings2, rownames(loadings)) abline(h = -0.1, col = &quot;darkgrey&quot;) 9.2.3 Correspondence analysis 适用于频数数据。Correspondence analysis, MDS和hierarchical cluster analysis都基于distance matrix。Correspondence analysis是MDS的一种特殊情况。 在对应分析（Correspondence Analysis）中，我们将行向量（或列向量）视为“城市”的轮廓，并计算它们之间的距离。可以用多种方式计算向量之间的距离（或不相似度），在 dist() 函数的在线帮助页面中记录了多种选项。在对应分析中使用的距离度量是所谓的卡方距离（chi-squared distance）。给定一个包含20行和5列的列联表，对应分析构建了两个距离矩阵，一个是20x20的矩阵，指定行之间的距离，另一个是5x5的矩阵，指定列之间的距离。 对应分析的第二步是尽可能忠实地将这些距离表示在二维散点图中，即低维地图中。两行之间的距离越大，它们在行的地图中应该越远。同样，不相似的列应该相距较远，而相似的列在列的地图中应该彼此接近。在对应分析中，我们将行图和列图叠加，这类似于双标图（biplot）中主成分得分（PC scores）和这些主成分上的载荷的叠加。由于卡方距离度量，我们可以确保在合并的地图中行和列之间的接近程度尽可能好地近似于行和列之间的相关性。 library(stylo) # 导入文本数据 style.corpus &lt;- load.corpus(files = &quot;all&quot;, corpus.dir = &quot;data/ch6/detectives&quot;, encoding = &quot;UTF-8&quot;) # 对文本进行分词 tokenized.corpus &lt;- txt.to.words.ext(style.corpus, preserve.case = TRUE) # 根据研究目的我们可以设置一个词表去除一些干扰词。比如我们想排除文本中高频专有名词的干扰。 #proper.noun = c(&quot;DEE&quot;,&quot;Judge&quot;,&quot;Dee&quot;,&quot;&quot;) # clean.corpus = delete.stop.words(tokenized.corpus, # stop.words = proper.noun) # 一些研究认为代词对于计算文体风格有干扰，比如如果是一个大女主的小说，那么女性代词就会很多。但是这些不能代表作者风格，只能代表作品风格。 clean.corpus = delete.stop.words(tokenized.corpus, stop.words = stylo.pronouns(corpus.lang = &quot;English&quot;)) corpus.char.1 &lt;- txt.to.features(clean.corpus, ngram.size = 1, features = &quot;w&quot;) # 将数据划分成1000个词一份 sliced.corpus.char.1 &lt;- make.samples(corpus.char.1, sampling = &quot;normal.sampling&quot;, sample.size = 1000) # 提取词频特征 features1 &lt;- make.frequency.list(sliced.corpus.char.1) # 制作词频特征表 freqs1 &lt;- make.table.of.frequencies(sliced.corpus.char.1, features = features1) # Culling # 通过删选，用户可以指定特征在语料库中的出现比例，以决定其是否被纳入分析。在语料库中未达到指定比例的词语将被忽略。 culled.freqs1 &lt;- perform.culling(freqs1, culling.level = 80) wd.df = as.data.frame(culled.freqs1) detect.ca = corres.fnc(wd.df) summary(detect.ca, head = TRUE) ## ## Call: ## corres(wd.df) ## ## Eigenvalue rates: ## ## 0.1626131 0.1001058 0.06225465 0.05089965 0.04367982 0.04005806 ... ## ## Factor 1 ## ## coordinates correlations contributions ## the -0.225 0.708 0.215 ## and -0.066 0.090 0.009 ## to -0.023 0.007 0.001 ## of -0.138 0.216 0.035 ## a 0.034 0.018 0.002 ## I 0.601 0.798 0.442 ## ... ## ## Factor 2 ## ## coordinates correlations contributions ## the 0.010 0.001 0.001 ## and 0.055 0.063 0.010 ## to -0.154 0.302 0.082 ## of 0.076 0.067 0.018 ## a 0.024 0.009 0.001 ## I 0.130 0.037 0.034 ## ... plot(detect.ca) plot(detect.ca, #rlabels = oldFrenchMeta$Genre, #rcol = as.numeric(oldFrenchMeta$Genre), rcex = 0.4, extreme = 0.1, ccol = &quot;blue&quot;) 9.2.4 Multi-demensional scaling 多维尺度分析（Multidimensional Scaling, MDS）是一种用于在距离矩阵中发现结构的技术。类似于主成分分析（Principal Components Analysis, PCA），它是一种用于降维的技术，通常将数据降到二维或三维。与对应分析（Correspondence Analysis）类似，实际上对应分析是多维尺度分析的一种特殊情况，其核心思想是在一个平面（例如）中创建一个表示，使得平面中各点之间的距离尽可能准确地反映原始多维空间中各点之间的距离。 下面我们基于词频特征计算两个文本的文体风格特征并通过不同的非监督学习的方法对两个不同的文本进行分类。 wd.df.d = dist(wd.df, # create distance matrix method = &quot;euclidean&quot;) wd.mds = cmdscale( wd.df.d, k = 3) 9.2.5 Cluster 9.2.5.1 检测数据是否可以聚类 我们通过测试数据是否包含非随机性来评估数据是否“可聚类”（clusterable）。为此，我们计算Hopkins统计量，该统计量表示数据与随机分布的相似程度。 Hopkins值为0.5表明数据是随机的，且没有内在的聚类。 如果Hopkins统计量接近1，则说明数据高度可聚类。 Hopkins值为0表示数据是均匀分布的 (Aggarwal 2015, 158)。 # apply get_clust_tendency to cluster object clusttendency &lt;- get_clust_tendency(wd.df, # define number of points from sample space n = 9, gradient = list( # define color for low values low = &quot;steelblue&quot;, # define color for high values high = &quot;white&quot;)) clusttendency[1] ## $hopkins_stat ## [1] 0.6259924 9.2.5.2 数据预处理 # clust &lt;- na.omit(clust) # remove missing values # clean data # clusm &lt;- as.matrix(clus) # clust &lt;- t(clusm) # transpose data # clust &lt;- na.omit(clust) # remove missing values # clusts &lt;- scale(clust) # standardize variables # clusts &lt;- as.matrix(clusts) # convert into matrix wd.dfs &lt;- scale(wd.df) 9.2.5.3 计算距离矩阵 距离 clustd &lt;- dist(wd.dfs, # 计算距离 method = &quot;euclidean&quot;) # use euclidean (!) distance ## 计算距离的各种方法 # # create distance matrix (euclidean method: not good when dealing with many dimensions) # clustd &lt;- dist(clusts, method = &quot;euclidean&quot;) # # create distance matrix (maximum method: here the difference between points dominates) # clustd_maximum &lt;- round(dist(clusts, method = &quot;maximum&quot;), 2) # # create distance matrix (manhattan method: most popular choice) # clustd_manhatten &lt;- round(dist(clusts, method = &quot;manhattan&quot;), 2) # # create distance matrix (canberra method: for count data only - focuses on small differences and neglects larger differences) # clustd_canberra &lt;- round(dist(clusts, method = &quot;canberra&quot;), 2) # # create distance matrix (binary method: for binary data only!) # clustd_binary &lt;- round(dist(clusts, method = &quot;binary&quot;), 2) # # create distance matrix (minkowski method: is not a true distance measure) # clustd_minkowski &lt;- round(dist(clusts, method = &quot;minkowski&quot;), 2) # # distance method for words: daisy (other possible distances are &quot;manhattan&quot; and &quot;gower&quot;) # clustd_daisy &lt;- round(daisy(clusts, metric = &quot;euclidean&quot;), 2) 9.2.5.4 创建聚类 最常见的聚类方法称为ward.D或ward.D2。这两种连接函数都旨在最小化方差。也就是说，它们以一种使方差最小化的方式进行聚类（类似于普通最小二乘法设计中的回归线）。 # create cluster object cd &lt;- hclust(clustd, method=&quot;ward.D2&quot;) # display dendrogram plot(cd, hang = -1) # 创建cluster的方法 # # single linkage: cluster with nearest data point # cd_single &lt;- hclust(clustd, method=&quot;single&quot;) # # create cluster object (ward.D linkage) # cd_wardd &lt;- hclust(clustd, method=&quot;ward.D&quot;) # # create cluster object (ward.D2 linkage): # # cluster in a way to achieve minimum variance # cd_wardd2 &lt;- hclust(clustd, method=&quot;ward.D2&quot;) # # average linkage: cluster with closest mean # cd_average &lt;- hclust(clustd, method=&quot;average&quot;) # # mcquitty linkage # cd_mcquitty &lt;- hclust(clustd, method=&quot;mcquitty&quot;) # # median linkage: cluster with closest median # cd_median &lt;- hclust(clustd, method=&quot;median&quot;) # # centroid linkage: cluster with closest prototypical point of target cluster # cd_centroid &lt;- hclust(clustd, method=&quot;centroid&quot;) # # complete linkage: cluster with nearest/furthest data point of target cluster # cd_complete &lt;- hclust(clustd, method=&quot;complete&quot;) 9.2.6 确定最佳的聚类数量 现在，我们基于轮廓宽度（silhouette widths）来确定最佳的聚类数量。轮廓宽度显示了聚类内部相似性与聚类之间相似性的比率。如果轮廓宽度的值低于0.2，这表明聚类可能不合适（Levshina 2015, 311）。下面的函数显示了2到8个聚类的轮廓宽度值。 optclus &lt;- sapply(2:8, function(x) summary(silhouette(cutree(cd, k = x), clustd))$avg.width) optclus # inspect results ## [1] 0.05391875 0.04634888 0.04462229 0.04769065 0.03598193 0.03790021 0.04366335 # 最佳的聚类数量是具有最高轮廓系数（silhouette width）的聚类方案。我们将树状图剪切为最佳的聚类数量，并绘制结果。 optnclust &lt;- which(optclus == max(optclus)) # determine optimal number of clusters groups &lt;- cutree(cd, k=optnclust) # cut tree into optimal number of clusters plot(cd, hang = -1, cex = .75) # plot result as dendrogram #rect.hclust(cd, k=optnclust, border=&quot;red&quot;) # draw red borders around clusters 9.2.6.1 确定影响因素 在下一步中，我们旨在确定哪些因素对聚类特别重要——这一步骤类似于推理设计中测量效应大小的过程。 # which factors are particularly important celtic &lt;- wd.dfs[c(1,2),] others &lt;- wd.dfs[-c(1,2),] # calculate column means celtic.cm &lt;- colMeans(celtic) others.cm &lt;- colMeans(others) # calculate difference between celtic and other englishes diff &lt;- celtic.cm - others.cm sort(diff, decreasing = F) ## that but with it said I had He out ## -1.69321119 -1.33784741 -1.13644247 -1.12259843 -1.06083926 -0.95872635 -0.95424014 -0.83743853 -0.81340938 ## so s one this would the have has was ## -0.80697064 -0.73690866 -0.72158651 -0.71090834 -0.70899352 -0.64731768 -0.58718361 -0.56948068 -0.56758791 ## could for there when up The an from about ## -0.54005685 -0.52973281 -0.51499592 -0.37586913 -0.34621512 -0.32503076 -0.29963923 -0.28421738 -0.21697833 ## were is to no on not are been time ## -0.20560158 -0.16562718 -0.15749547 -0.09250448 -0.02314698 -0.02266190 0.01197825 0.07352595 0.08937754 ## at a some be man and as all in ## 0.13445889 0.20863330 0.28245668 0.38935834 0.40804644 0.41283703 0.61887339 0.72477609 0.97843397 ## of by who ## 1.55867980 1.56607051 3.22904485 plot(sort(diff), # y-values 1:length(diff), # x-values type= &quot;n&quot;, # plot type (empty) cex.axis = .75, # axis font size cex.lab = .75, # label font size xlab =&quot;Prototypical for Non-Celtic Varieties (Cluster 2) &lt;-----&gt; Prototypical for Celtic Varieties (Cluster 1)&quot;, # x-axis label yaxt = &quot;n&quot;, # no y-axis tick marks ylab = &quot;&quot;) # no y-axis label text(sort(diff), 1:length(diff), names(sort(diff)), cex = .75) # plot text into plot 9.2.6.2 检验分类是否合理 现在我们将通过使用自助法（bootstrapping）验证聚类解决方案，测试聚类是否合理。 library(pvclust) res.pv &lt;- pvclust(t(wd.dfs), # apply pvclust method to clus data method.dist=&quot;euclidean&quot;, # use eucledian distance method.hclust=&quot;ward.D2&quot;, # use ward.d2 linkage nboot = 100) # use 100 bootstrap runs ## Bootstrap (r = 0.5)... Done. ## Bootstrap (r = 0.58)... Done. ## Bootstrap (r = 0.69)... Done. ## Bootstrap (r = 0.79)... Done. ## Bootstrap (r = 0.9)... Done. ## Bootstrap (r = 1.0)... Done. ## Bootstrap (r = 1.08)... Done. ## Bootstrap (r = 1.19)... Done. ## Bootstrap (r = 1.29)... Done. ## Bootstrap (r = 1.4)... Done. plot(res.pv, cex = .75) pvrect(res.pv) library(gplots) # # install.packages(&quot;vcd&quot;) # install.packages(&quot;exact2x2&quot;) library(ape) # library(vcd) # library(exact2x2) plot(as.phylo(cd), # plot cluster object cex = 0.75, # .75 font size label.offset = .5) # .5 label offset # plot as unrooted tree plot(as.phylo(cd), # plot cluster object type = &quot;unrooted&quot;, # plot as unrooted tree cex = .75, # .75 font size label.offset = 1) # .5 label offset 9.2.6.3 类别型数据聚类分析 tone.phon &lt;- read.csv(&quot;data/ch9/tone-phonology.CSV&quot;) ## Warning in read.table(file = file, header = header, sep = sep, quote = quote, : incomplete final line found by ## readTableHeader on &#39;data/ch9/tone-phonology.CSV&#39; row.names(tone.phon)=tone.phon$language #tone.phon = select(tone.phon, -language) # convert into factors tone.phon &lt;- apply(tone.phon, 1, function(x){ x &lt;- as.factor(x) }) # clean data tone.phon.mtr &lt;- t(as.matrix(tone.phon)) # create distance matrix tone.phon.mtr.d&lt;- dist(tone.phon.mtr[,-1] , method = &quot;binary&quot;) # create a distance object with binary (!) distance # create cluster object (ward.D2 linkage) : cluster in a way to achieve minimum variance tone.phon.mtr.cd &lt;- hclust(tone.phon.mtr.d, method=&quot;ward.D2&quot;) # plot result as dendrogram plot(tone.phon.mtr.cd , hang = -1) # display dendogram 9.2.7 文本聚类分析 # 主成分分析 stylo(frequencies = culled.freqs1, analysis.type = &quot;PCR&quot;, custom.graph.title = &quot;Judge Dee and Sherlock&quot;, pca.visual.flavour = &quot;technical&quot;, write.png.file = TRUE, gui = FALSE) # 显示两本书在第一第二主成分空间的分布 stylo(frequencies = culled.freqs1, analysis.type = &quot;PCR&quot;, mfw.min = 100, mfw.max = 500, custom.graph.title = &quot;Judge Dee and Sherlock&quot;, write.png.file = FALSE, gui = FALSE) # 显示不同高频词的权重loadings stylo(frequencies = culled.freqs1, analysis.type = &quot;PCR&quot;, custom.graph.title = &quot;Judge Dee and Sherlock&quot;, pca.visual.flavour = &quot;loadings&quot;, write.png.file = FALSE, gui = FALSE) # cluster analysis stylo(frequencies = culled.freqs1, analysis.type = &quot;CA&quot;, write.png.file = FALSE, custom.graph.title = &quot;Judge Dee and Sherlock&quot;, gui = FALSE) "],["案例一行为实验数据分析.html", "Chapter 10 案例一、行为实验数据分析 10.1 研究背景 10.2 数据整理+描述性统计 10.3 数据可视化 10.4 统计建模 10.5 结论", " Chapter 10 案例一、行为实验数据分析 10.1 研究背景 在第五章中，我们分析了汉语普通话母语者是如何将泰语声调同化到母语声调中的。成年人对言语语音的感知受到母语语音系统的影响。本章中我们将结合相关理论，感知同化模型，预测不同外语音位对立的感知难度。并且通过声调区分感知实验来验证。感知同化模型预测了几种同化模式的感知区分难度。单类别同化（Single Category, SC）中，两个不同的目标音素被同化为一个母语音素类别。类别差异同化（Category Goodness, CG）两个目标音素被同化为同一个母语音素类别，但一个音素更接近于该类别的原型。两类别同化（Two Category, TC），两个目标音素被同化为两个不同的母语音素类别。其中TC的区分度要好于CG，CG要好于SC。 除了不同的同化类型之外，感知模式也会影响区分任务中的表现。如果任务的记忆负荷大、刺激材料由不同的说话人产出、语音环境比较复杂（例如对于声调实验来说，刺激不仅声调不同而且元音也不同），这时听者会使用音位感知模式。在这个模式中，听者会比较粗放地感知语音，一些语音细节被忽视。反之，听者会使用语音感知模式，比较精细地感知。 在实验中，我们让参与者判断两个声调是否相同，操纵了记忆负荷（两个声调呈现的时间间隔）、语音是否来自同一个人，以及声调所在的音节元音是否相同。 library(lme4) library(lsmeans) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) library(tidyverse) # 显示小数点后很多位 options(scipen=999) # 计算置信区间 library(rcompanion) # 改变y轴度量 library(scales) # 组合图 library(cowplot) 10.2 数据整理+描述性统计 # 我们先处理一下感知同化数据 # 我们可以将数据处理过程封装成一个函数 # 方便后面如果有不同实验组进行调用 cat.table = function(catdata){ catdata %&gt;% select(subject, isi, rating, stimuli, response, percentage)%&gt;% # add cgload if necessary group_by(isi, stimuli, response)%&gt;% summarise(cat.mean = round(sum(percentage)/16, 3)*100, rate.mean = round(mean(rating),1))%&gt;% gather(temp, score, ends_with(&quot;.mean&quot;)) %&gt;% unite(temp1, stimuli, temp, sep = &quot;_&quot;) %&gt;% spread(temp1, score)%&gt;% select(&quot;isi&quot;,&quot;response&quot;, &quot;T45_cat.mean&quot;,&quot;T45_rate.mean&quot;, &quot;T33_cat.mean&quot;, &quot;T33_rate.mean&quot;, &quot;T21_cat.mean&quot;,&quot;T21_rate.mean&quot;, &quot;T315_cat.mean&quot;, &quot;T315_rate.mean&quot;,&quot;T241_cat.mean&quot;, &quot;T241_rate.mean&quot;)-&gt;catdata2 return(catdata2) } # 感知同化数据 cm.cat.fnl = read.csv(&quot;data/ch10/md.cat.fnl.2020-08-04.CSV&quot;)%&gt;% mutate(isi = fct_relevel(isi, &quot;Low&quot;,&quot;High&quot;), response = as_factor(response), response = fct_relevel(response, &quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;)) md.cat.tbl = cat.table(cm.cat.fnl) ## `summarise()` has grouped output by &#39;isi&#39;, &#39;stimuli&#39;. You can override using the `.groups` argument. knitr::kable( md.cat.tbl, caption = &#39;Table 3 &#39;, booktabs = TRUE ) Table 10.1: Table 3 isi response T45_cat.mean T45_rate.mean T33_cat.mean T33_rate.mean T21_cat.mean T21_rate.mean T315_cat.mean T315_rate.mean T241_cat.mean T241_rate.mean Low M55 0.4 4.0 77.3 5.3 19.9 3.3 NA NA 21.6 4.2 Low M35 88.8 5.8 2.8 2.8 1.6 2.1 48.6 5.5 2.9 3.7 Low M214 10.3 4.3 0.7 3.3 25.8 3.7 51.2 5.4 0.4 3.5 Low M51 0.4 5.5 19.2 4.9 52.7 4.6 0.2 7.0 75.1 5.6 High M55 NA NA 84.7 5.2 26.4 3.2 0.2 7.0 28.1 5.0 High M35 85.0 5.3 0.2 5.0 1.1 3.3 44.2 5.1 0.2 2.0 High M214 14.7 4.8 1.7 5.9 6.3 3.8 55.6 5.5 0.5 6.0 High M51 0.2 2.0 13.4 4.4 66.2 4.0 NA NA 71.2 5.1 我们结合了感知同化数据和母语选项之间的重合关系对两种不同的记忆负荷下的感知区分表现进行了预测。 低记忆负荷，感知表现由好到差：Two-Category/Non-overlap {T33-T45 = T33-T21 = T33-T241} &gt; Category-Goodness/Non-overlap {T241-T21 } &gt; UnCategorised-Categorised/Partial-overlap {T315-T45}. 高记忆负荷，感知表现由好到差: Two-Category/Non-overlap {T33-T45 = T33-T21} &gt; CategoryGoodness/Non-overlap {T241-T21} &gt; Two-Category/Partial-o verlap {T33-T241} = UnCategorised-Categorised/Partial-over lap {T315-T45} # Load the data md.dis.all = read.csv(&quot;data/ch10/md.dis.all.2021-03-21.CSV&quot;) %&gt;% # set the order of the contrast mutate(Tone_contrast = fct_relevel(Tone_contrast, &quot;T33-T45&quot;,&quot;T33-T21&quot;,&quot;T241-T21&quot;, &quot;T315-T45&quot;, &quot;T33-T241&quot; ), cgload = factor(cgload,c(&quot;sssv&quot;,&quot;ssdv&quot;,&quot;dssv&quot;,&quot;dsdv&quot;))) 10.3 数据可视化 我们计算了每个认知条件下每个声调对立的作为区分表现的指标。 library(showtext) showtext_auto() # Overall effect of taker and vowel variability plot label.md = c(&quot;T33-T45(TC-N)&quot;,&quot;T33-T21(TC-N)&quot;, &quot;T241-T21(CG-N)&quot;,&quot;T315-T45(UC-P)&quot;, &quot;T33-T241(TC-N/P)&quot;) names(label.md) &lt;- c(&quot;T33-T45&quot;,&quot;T33-T21&quot;,&quot;T241-T21&quot;, &quot;T315-T45&quot;,&quot;T33-T241&quot;) # 计算不同实验条件下每个声调对立的感知表现平均值 md.dis.df = md.dis.all%&gt;% mutate(speaker = case_when(speaker == &quot;Constant&quot; ~ &quot;单个说话人&quot;, speaker== &quot;Variable&quot; ~ &quot;两个说话人&quot;), speaker = fct_relevel(speaker, &quot;单个说话人&quot;, &quot;两个说话人&quot;))%&gt;% groupwiseMean(dprime2 ~ speaker+vowel + Tone_contrast, data = ., conf = 0.95, digits = 3) # 画柱状图 md.dis.df%&gt;% ggplot(.,aes(speaker, Mean, fill = vowel))+ geom_bar(colour = &quot;black&quot;, stat = &quot;identity&quot;, position = position_dodge(.9))+ geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = .2, size = 0.7, position = position_dodge(.9))+ facet_wrap( ~ Tone_contrast, ncol = 3, strip.position = &quot;bottom&quot;, labeller = labeller( Tone_contrast = label.md))+ scale_fill_manual(values=c(&quot;grey75&quot;, &quot;white&quot;), name=&quot;元音变异性&quot;, labels=c(&quot;单个元音&quot;, &quot;两个元音&quot;))+ labs(y = &quot;感知准确性&quot;, x = &quot;说话人变异&quot;)+ theme_classic()+ #设置xy轴标注的字体大小 theme(axis.title.y = element_text(size = 12,face = &quot;bold&quot;), axis.title.x = element_text(size = 12,face = &quot;bold&quot;), axis.ticks = element_blank(), axis.text.x = element_text(size = 12,face = &quot;bold&quot;), axis.text.y = element_text(size = 12,face = &quot;bold&quot;))+ # 设置图例的位置 theme(legend.justification=c(1,1), legend.position=c(1,0.3), legend.text = element_text(size = 12), legend.title = element_text(size = 12))+ theme(legend.key.size = unit(1, &quot;cm&quot;))+ theme(strip.text.x = element_text(size = 12, face = &quot;bold&quot;)) 10.4 统计建模 我们使用LMER模型（通过lme4，Bates, Mächler, Bolker, &amp; Walker, 2015）拟合数据，以d’为因变量，记忆负荷（低和高）、说话人和元音变异性（恒定与可变）以及音调对比（T241-T21, T33-T21, T315-T45, T33-T241, T33-T45）为固定因素，参与者为随机因素，包括受试者内固定因素的随机截距和斜率，即说话人和元音变异性条件，以使模型在数据中最大程度地具有普遍性（如Barr, Levy, Scheepers, &amp; Tily, 2013所建议的）。 对于LMER模型中固定因素显著性的估计和相关p值的计算，有几种不同的方法（Luke, 2017）。我们使用了Kenward-Roger自由度近似法（Halekoh &amp; Hojsgaard, 2014）和R中car包的Anova函数。在此，我们报告多级因素或交互作用的主要效应，而不是使用t检验与基线水平进行比较。因此，报告的主要/交互作用效应是所有其他效应水平的平均值，可以直接用于测试我们的预测。 library(car) ## 为了进一步验证假设，我们对数据使用混合模型 md.dis.mdl = lmer(dprime2 ~ ISI * speaker * vowel* Tone_contrast + (1 + speaker|Subject)+(1 + vowel|Subject), data = md.dis.all) ## boundary (singular) fit: see help(&#39;isSingular&#39;) # 模型结果 md.mdl.smry = data.frame(coef(summary(md.dis.mdl))) # 计算不同效应的P值 md.dis.mdl.tbl = data.frame(Anova(md.dis.mdl, test = &quot;F&quot;)) md.dis.mdl.tbl ## F Df Df.res ## ISI 0.08144421 1 30 ## speaker 41.07944654 1 30 ## vowel 57.40271150 1 30 ## Tone_contrast 127.22783834 4 510 ## ISI:speaker 0.04256406 1 30 ## ISI:vowel 0.02175984 1 30 ## speaker:vowel 20.06062170 1 510 ## ISI:Tone_contrast 0.31491830 4 510 ## speaker:Tone_contrast 1.54945070 4 510 ## vowel:Tone_contrast 1.02610302 4 510 ## ISI:speaker:vowel 0.40565309 1 510 ## ISI:speaker:Tone_contrast 0.04896495 4 510 ## ISI:vowel:Tone_contrast 0.90573265 4 510 ## speaker:vowel:Tone_contrast 0.33158765 4 510 ## ISI:speaker:vowel:Tone_contrast 0.11884579 4 510 ## Pr..F. ## ISI 0.777310856349093248773840514331823214888572692871093750000000000000000000000000000 ## speaker 0.000000446732319194840525892266662119634013095037516904994845390319824218750000000 ## vowel 0.000000018974107701813392937758355970799539758075979989371262490749359130859375000 ## Tone_contrast 0.000000000000000000000000000000000000000000000000000000000000000000000000002911086 ## ISI:speaker 0.837942106026152933040407333464827388525009155273437500000000000000000000000000000 ## ISI:vowel 0.883714736199696204721476533450186252593994140625000000000000000000000000000000000 ## speaker:vowel 0.000009266734389606409372165847504465574502319213934242725372314453125000000000000 ## ISI:Tone_contrast 0.868026640492182921526875816198298707604408264160156250000000000000000000000000000 ## speaker:Tone_contrast 0.186630853689322862010158132761716842651367187500000000000000000000000000000000000 ## vowel:Tone_contrast 0.393171291943390510681410887627862393856048583984375000000000000000000000000000000 ## ISI:speaker:vowel 0.524469889350749340906077122781425714492797851562500000000000000000000000000000000 ## ISI:speaker:Tone_contrast 0.995491349899420452373988155159167945384979248046875000000000000000000000000000000 ## ISI:vowel:Tone_contrast 0.460284356416936102363024474470876157283782958984375000000000000000000000000000000 ## speaker:vowel:Tone_contrast 0.856740099033116431215262309706304222345352172851562500000000000000000000000000000 ## ISI:speaker:vowel:Tone_contrast 0.975786261551118694335116288129938766360282897949218750000000000000000000000000000 # 对泰语声调对进行事后检验 md.dis.mdl.tone = data.frame(lsmeans(md.dis.mdl, pairwise ~ Tone_contrast)$contrasts) ## NOTE: Results may be misleading due to involvement in interactions md.dis.mdl.tone ## contrast estimate SE df t.ratio p.value ## 1 (T33-T45) - (T33-T21) 0.9175000 0.1154371 510 7.948049 0.0000000001850168 ## 2 (T33-T45) - (T241-T21) 1.2834375 0.1154371 510 11.118064 0.0000000001848676 ## 3 (T33-T45) - (T315-T45) 2.0775781 0.1154371 510 17.997484 0.0000000001848434 ## 4 (T33-T45) - (T33-T241) 2.2747656 0.1154371 510 19.705665 0.0000000001848434 ## 5 (T33-T21) - (T241-T21) 0.3659375 0.1154371 510 3.170015 0.0139264721185274 ## 6 (T33-T21) - (T315-T45) 1.1600781 0.1154371 510 10.049436 0.0000000001848901 ## 7 (T33-T21) - (T33-T241) 1.3572656 0.1154371 510 11.757616 0.0000000001848456 ## 8 (T241-T21) - (T315-T45) 0.7941406 0.1154371 510 6.879420 0.0000000003614909 ## 9 (T241-T21) - (T33-T241) 0.9913281 0.1154371 510 8.587601 0.0000000001848907 ## 10 (T315-T45) - (T33-T241) 0.1971875 0.1154371 510 1.708181 0.4297741957500414 预测 低记忆负荷，感知表现由好到差：{T33-T45 = T33-T21 = T33-T241} &gt; {T241-T21 } &gt; {T315-T45}. 高记忆负荷，感知表现由好到差: {T33-T45 = T33-T21} &gt; {T241-T21} &gt; {T33-T241} = {T315-T45} 结果： T33-T45 (M = 4.33, 95 % CIs [4.15, 4.52]) &gt; T33-T21 (M = 3.42, 95 % CIs [3.21, 3.63]) &gt; T241-T21 (M = 3.05, 95 % CIs [2.85, 3.26]) &gt; T33-241 (M = 2.06, 95 % CIs [1.87, 2.25]) = T315-T45 (M = 2.26, 95 % CIs [2.06, 2.45]). # 对刺激说话人变异和元音环境两个变量的交互作用进行进行事后检验 md.dis.mdl.vwl.tlk = data.frame(lsmeans(md.dis.mdl, pairwise ~ speaker:vowel)$contrasts) ## NOTE: Results may be misleading due to involvement in interactions md.dis.mdl.vwl.tlk ## contrast estimate SE df t.ratio p.value ## 1 Constant Constant - Variable Constant 0.7949375 0.1032501 113.33333 7.6991437 0.0000000000334174910 ## 2 Constant Constant - Constant Variable 0.8951875 0.1046631 108.10159 8.5530376 0.0000000000006066259 ## 3 Constant Constant - Variable Variable 1.0361250 0.1046631 59.95688 9.8996200 0.0000000000200567341 ## 4 Variable Constant - Constant Variable 0.1002500 0.1046631 59.95688 0.9578351 0.7736955322903441568 ## 5 Variable Constant - Variable Variable 0.2411875 0.1046631 108.10159 2.3044175 0.1033637878297640755 ## 6 Constant Variable - Variable Variable 0.1409375 0.1032501 113.33333 1.3650105 0.5238382126248282145 # Mandarin talker vowel interaction md.tlk.vwl = groupwiseMean(dprime2 ~ speaker + vowel, data = md.dis.all, conf = 0.95, digits = 3) md.tlk.vwl%&gt;% ggplot(aes(x=speaker, y= Mean, fill=vowel)) + geom_bar(colour = &quot;black&quot;, stat = &quot;identity&quot;, position = position_dodge(.9))+ geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = .2, size = 0.7,position = position_dodge(.9))+ labs(y = &quot;d&#39; values&quot;, x = &quot;Talker variability&quot;)+ theme_classic()+ scale_y_continuous(expand = c(0, 0), limits = c(0, 4))+ theme(axis.title.y = element_text(size = 12,face = &quot;bold&quot;), axis.title.x = element_text(size = 12,face = &quot;bold&quot;), axis.text.x = element_text(size = 12,face = &quot;bold&quot;), axis.text.y = element_text(size = 12,face = &quot;bold&quot;))+ #change lengent style scale_x_discrete(labels=c(&quot;Constant&quot;, &quot;Variable&quot;))+ scale_fill_manual(values=c(&quot;white&quot;,&quot;grey75&quot;), name=&quot;Vowel variability&quot;)+ theme(legend.title = element_text(size=12, face=&quot;bold&quot;), legend.text = element_text(size = 12, face = &quot;bold&quot;)) 除了主要效应外，我们还进行了多重比较，并使用Tukey调整来解析说话人变异性和元音变异性的交互作用。在恒定说话人的条件下，恒定元音条件（M = 3.71, 95% CI [3.51, 3.90]）的辨别效果优于可变元音条件（M = 2.81, 95% CI [2.60, 3.03]），b = 0.90, SE = 0.11, t(108) = 8.55, p &lt; 0.001。 同样地，在恒定元音的条件下，恒定说话人条件（M = 3.71）的辨别效果优于可变说话人条件（M = 2.91, 95% CI [2.70, 3.12]），b = 0.80, SE = 0.10, t(113) = 7.70, p &lt; 0.001。此外，恒定说话人 + 元音条件（M = 3.71）的得分显著高于可变说话人 + 元音条件（M = 2.67, 95% CI [2.46, 2.88]），b = 1.04, SE = 0.11, t(60) = 9.90, p &lt; 0.001。 10.5 结论 记忆负荷对感知区分任务没有影响，这与先前有关辅音的研究（Werker &amp; Logan, 1985）不同，但与之前关于声调感知的发现一致（Lee et al., 1996）。这一发现与“线索持续时间假设”相一致，即音响特性的持续时间越长，在短期记忆中衰退的可能性就越小（Fujisaki &amp; Kawashima, 1970）。词汇声调的音响特性，即基频和音调轮廓，贯穿整个响音音节的持续时间，即[ma:]和[mi:]，因此不太可能衰退，更有可能在记忆中保持稳定，而辅音的更短暂的音响特性，如共振峰转换或发声起始时间，则更容易衰退。 与同化过程没有显著影响不同的是，说话人和元音上下文的变化对非母语声调的辨别产生了影响。 在辨别任务中，听众被要求注意语音差异，因此他们默认应倾向于语音模式。然而，高刺激变异性应使听众即使在辨别任务中也倾向于更多的音位模式，这需要他们感知抽象的音高轮廓和相对高度，并减少对具体语音基频差异的注意，这些差异在辨别声调时是重要的。因此，他们的辨别准确性降低了。 "],["文本语料数据分析案例.html", "Chapter 11 文本语料数据分析案例 11.1 案例一、狄仁杰小说翻译文体风格 11.2 案例二、汉语失语症话语产出", " Chapter 11 文本语料数据分析案例 11.1 案例一、狄仁杰小说翻译文体风格 11.1.1 研究背景 详细背景请看原文献 罗伯特·范·古利克（Robert Van Gulik）是一位荷兰小说家和汉学家。他的《狄公案》侦探小说系列（总共17本）在1949年至1968年间陆续出版。该系列的第一本书，即《狄公奇案》（Celebrated Cases of Judge Dee），是从一本中文书《狄公案》翻译而来。狄公或狄仁杰是一位真实存在的历史上的侦探和政治家，生活在公元630年至700年之间，即唐代（公元618年至907年）。这部翻译作品激发了译者对中国侦探故事的兴趣，并为他自己的侦探小说（其余16本）创造了狄仁杰这一角色。 在翻译的前言中，作者声称他保留了原文中的所有中国元素，并谴责那些涉及过多改写并扭曲中国文化和历史现实的伪造“中国”故事。相反，在创作中，作者在第15册中承认，尽管他在创作后来的故事时借用了一些古代中国刑侦文学的元素，但这些故事本身是完全虚构的，即非翻译作品。于是，一个想法诞生了。 对于同一个主题故事，翻译和创造的文本语言上有什么不同呢？ 我收集了这17本书，进行了文本分析，从词汇丰富度、可读性、和句法复杂度进行了研究。 11.1.2 数据处理 library(lme4) library(lsmeans) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) library(tidyverse) # 显示小数点后很多位 options(scipen=999) # 计算置信区间 library(rcompanion) # 改变y轴度量 library(scales) # 组合图 library(cowplot) library(tidytext) require(readtext) library(rstatix) library(flextable) library(quanteda) library(quanteda.textstats) 11.1.2.1 词汇复杂度特征 dee.raw =readtext(&quot;data/ch11/*txt&quot;, docvarsfrom = &quot;filenames&quot;) dee.corpus = corpus(dee.raw) dee.corpus.summary = data.frame(summary(dee.corpus)) dee.dfm &lt;- dfm(dee.corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE #remove = stopwords(&quot;english&quot;) ) ## Warning: &#39;dfm.corpus()&#39; is deprecated. Use &#39;tokens()&#39; first. ## Warning: &#39;...&#39; should not be used for tokens() arguments; use &#39;tokens()&#39; first. dee.corpus.summary%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 1. Number of types, tokens, and sentences of each book in the Judge Dee series.&quot;) .cl-942faff0{}.cl-94251bbc{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9429ea70{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9429ea84{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-942a001e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-942a0028{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-942a0032{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-942a003c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-942a0046{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-942a0050{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.1: Table 1. Number of types, tokens, and sentences of each book in the Judge Dee series. TextTypesTokensSentencesdocvar1docvar201_Celebrated-Cases.txt60107927636861Celebrated-Cases02_The-Chinese-Bell-Murders.txt78588961146962The-Chinese-Bell-Murders lexical.diversity &lt;- textstat_lexdiv(dee.dfm, measure = &quot;all&quot;) lexical.diversity%&gt;% flextable()%&gt;% set_caption(caption = &quot;词汇复杂度指标.&quot;) .cl-9452418c{}.cl-944b708c{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-944e4802{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-944e480c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-944e59b4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-944e59be{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-944e59bf{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-944e59c8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-944e59c9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-944e59d2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.1: 词汇复杂度指标. documentTTRCRCTTRUSKIDVmMaaslgV0lgeV001_Celebrated-Cases.txt0.077996240.771085720.5131614.5049921.142930.835147497.795960.62120490.0097797380.098023960.21747915.86117113.4958502_The-Chinese-Bell-Murders.txt0.090611020.787001925.4176417.9729922.985700.8492039104.172920.78727360.0104174240.101438390.20857936.24537914.38052 # 信息熵 textstat_entropy(dee.dfm)$entropy ## [1] 9.176072 9.449439 readability &lt;- textstat_readability(dee.corpus, measure = &quot;all&quot;) readability.df = readability%&gt;% as.data.frame()%&gt;% select(document, ARI, Flesch, FOG, Coleman.Liau.short, Dale.Chall, Spache) 11.1.2.2 句法特征提取 句法复杂度分析器，L2SCA L2 Syntactic Complexity Analyzer (“L2SCA”) is a tool that allows ESL and EFL teachers and researchers to analyze the syntactic complexity of written English language samples produced by advanced English learners. L2SCA was developed by Xiaofei Lu at The Pennsylvania State University (“PSU”), University Park, PA, USA. Unless stated otherwise, these Terms of Service apply to all usage of L2SCA, including those currently offered as well as any new products or services that we may add in the future. 陆小飞老师的官网 需要注意的是，这个句法复杂度分析器是基于英语笔语开发的，基本的要求是句法分词和标注是准确的。如果是中文，标注正确率一般不如英文，如果是口语，转写时如何确定句子的单位边界也是问题。 由于文本数据处理需要一定时间，此处我们导入已经处理完成的词汇和句法数据进行分析。 lexical.diversity.df = read_csv(&quot;data/ch11/lexical.diversity.df.2022-07-21.CSV&quot;) ## Rows: 17 Columns: 6 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): document ## dbl (5): TTR, C, R, MATTR, entropy ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. readability.df = read.csv(&quot;data/ch11/readability.df.2022-07-21.CSV&quot;) metrics_SCA &lt;- read_csv(&quot;data/ch11/metrics_SCA.csv&quot;) ## Rows: 38 Columns: 15 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): text ## dbl (14): MLS, MLT, MLC, C/S, VP/T, C/T, DC/C, DC/T, T/S, CT/T, CP/T, CP/C, CN/T, CN/C ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 11.1.3 数据可视化 11.1.4 统计建模 t.test(lexical.diversity.df$TTR[2:17], mu = lexical.diversity.df$TTR[1], alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: lexical.diversity.df$TTR[2:17] ## t = 7.8923, df = 15, p-value = 0.0000005086 ## alternative hypothesis: true mean is greater than 0.08 ## 95 percent confidence interval: ## 0.09701608 Inf ## sample estimates: ## mean of x ## 0.101875 # t.test(lexical.diversity.df$C[2:17], mu = lexical.diversity.df$C[1], alternative = &quot;greater&quot;) # # t.test(lexical.diversity.df$R[2:17], mu = lexical.diversity.df$R[1], alternative = &quot;greater&quot;) # # t.test(lexical.diversity.df$MATTR[2:17], mu = lexical.diversity.df$MATTR[1], alternative = &quot;greater&quot;) # # lexical.diversity.stat = lexical.diversity.df%&gt;% # as.data.frame()%&gt;% # mutate(id = 1:n())%&gt;% # select(&quot;document&quot;, &quot;id&quot;,&quot;TTR&quot;,&quot;C&quot;,&quot;R&quot;)%&gt;% # mutate(txt.type = case_when(id &lt; 2 ~ &quot;ref&quot;, # id &gt; 1 ~ &quot;freewriting&quot;))%&gt;% # gather(&quot;measures&quot;, &quot;values&quot;, -c(&quot;document&quot;, &quot;id&quot;,&quot;txt.type&quot;))%&gt;% # mutate(measures = as.factor(measures), # measures = fct_relevel(measures, &quot;TTR&quot;,&quot;C&quot;,&quot;R&quot;))%&gt;% # filter(measures == &quot;TTR&quot;)%&gt;% # group_by(measures)%&gt;% # rstatix::t_test(values ~ 1, mu = &quot;ref&quot;) # %&gt;% # rstatix::adjust_pvalue() %&gt;% # rstatix::add_significance(&quot;p.adj&quot;) metrics_SCA.ci = metrics_SCA%&gt;% filter(text !=&quot;01.txt&quot;)%&gt;% gather(&quot;measures&quot;, &quot;values&quot;, -c(&quot;text&quot;))%&gt;% groupwiseMean(values ~ measures, data = ., conf = 0.95, digits = 3) # mean length of clause (MLC): number of words divided by number of clauses; t.test(metrics_SCA$MLC[2:17], mu = metrics_SCA$MLC[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: metrics_SCA$MLC[2:17] ## t = -9.8862, df = 15, p-value = 0.00000002904 ## alternative hypothesis: true mean is less than 9.508 ## 95 percent confidence interval: ## -Inf 8.726519 ## sample estimates: ## mean of x ## 8.558075 t.test(metrics_SCA$MLS[2:17], mu = metrics_SCA$MLS[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: metrics_SCA$MLS[2:17] ## t = -22.252, df = 15, p-value = 0.0000000000003344 ## alternative hypothesis: true mean is less than 18.8517 ## 95 percent confidence interval: ## -Inf 14.27011 ## sample estimates: ## mean of x ## 13.87829 # # t.test(metrics_SCA$MLT[2:17], mu = metrics_SCA$MLT[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;C/S&quot;[2:17], mu = metrics_SCA$&quot;C/S&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;C/T&quot;[2:17], mu = metrics_SCA$&quot;C/T&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;CT/T&quot;[2:17], mu = metrics_SCA$&quot;CT/T&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;DC/C&quot;[2:17], mu = metrics_SCA$&quot;DC/C&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;DC/T&quot;[2:17], mu = metrics_SCA$&quot;DC/T&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;CP/C&quot;[2:17], mu = metrics_SCA$&quot;CP/C&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;CP/T&quot;[2:17], mu = metrics_SCA$&quot;CP/T&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;T/S&quot;[2:17], mu = metrics_SCA$&quot;T/S&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;CN/C&quot;[2:17], mu = metrics_SCA$&quot;CN/C&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;CN/T&quot;[2:17], mu = metrics_SCA$&quot;CN/T&quot;[1], alternative = &quot;less&quot;) # # t.test(metrics_SCA$&quot;VP/T&quot;[2:17], mu = metrics_SCA$&quot;VP/T&quot;[1], alternative = &quot;less&quot;) t.test(readability.df$ARI[2:17], mu = readability.df$ARI[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: readability.df$ARI[2:17] ## t = -15.986, df = 15, p-value = 0.00000000003939 ## alternative hypothesis: true mean is less than 8.35 ## 95 percent confidence interval: ## -Inf 5.834795 ## sample estimates: ## mean of x ## 5.525 t.test(readability.df$Flesch[2:17], mu = readability.df$Flesch[1], alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: readability.df$Flesch[2:17] ## t = 11.582, df = 15, p-value = 0.000000003502 ## alternative hypothesis: true mean is greater than 71.94 ## 95 percent confidence interval: ## 77.78876 Inf ## sample estimates: ## mean of x ## 78.83187 t.test(readability.df$FOG[2:17], mu = readability.df$FOG[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: readability.df$FOG[2:17] ## t = -15.11, df = 15, p-value = 0.00000000008754 ## alternative hypothesis: true mean is less than 10.71 ## 95 percent confidence interval: ## -Inf 8.611097 ## sample estimates: ## mean of x ## 8.335625 t.test(readability.df$Coleman.Liau.short[2:17], mu = readability.df$Coleman.Liau.short[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: readability.df$Coleman.Liau.short[2:17] ## t = -7.2341, df = 15, p-value = 0.000001453 ## alternative hypothesis: true mean is less than 8.02 ## 95 percent confidence interval: ## -Inf 7.366983 ## sample estimates: ## mean of x ## 7.158125 t.test(readability.df$Dale.Chall[2:17], mu = readability.df$Dale.Chall[1], alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: readability.df$Dale.Chall[2:17] ## t = 9.4143, df = 15, p-value = 0.00000005494 ## alternative hypothesis: true mean is greater than 34.89 ## 95 percent confidence interval: ## 38.30639 Inf ## sample estimates: ## mean of x ## 39.08812 t.test(readability.df$Spache[2:17], mu = readability.df$Spache[1], alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: readability.df$Spache[2:17] ## t = -11.792, df = 15, p-value = 0.000000002742 ## alternative hypothesis: true mean is less than 5.17 ## 95 percent confidence interval: ## -Inf 4.62461 ## sample estimates: ## mean of x ## 4.529375 我们发现，翻译文本和创作文本相比，词汇丰富性较低。 翻译文本和创作文本相比，句法复杂度较高。 可读性指标显示，翻译文本的可读性比创作文本差。 11.1.5 结论 根据受限语言理论（the constrained language theory,），翻译中的复杂认知过程，如激活两种语言系统和不断进行编码切换，给翻译者增加了额外的认知负担并降低了他的工作记忆容量，促使他采用简化等策略。此外，源文本在翻译过程中也可能限制了他可用的词汇资源。相反，作家能够充分发挥他的语言库，因此产生了更丰富的词汇多样性的文本。 然而，翻译文本的句法结构比创作的更为复杂。这不能归因于翻译过程的内在限制，因为翻译者需要面对更大的认知需求，并且预计他们将使用降低认知负担的策略，导致更短或更简单的句子。 于是，我们考虑英汉语的差异性。原生的汉语往往更依赖意义而不是功能词或结构来连接从句和句子，而西方语言则依靠丰富的句法手段。因此，受源语言影响的翻译汉语相对于原生汉语展现出更复杂的句法结构。在本研究中，翻译的方向是从古代汉语到英语，因此源文本的语言结构不太可能在整体上导致更复杂的目标文本句子。 抛开这一解释，我们认为罗伯特·范·古利克作为翻译者特别意识到自己作为中介者的角色，希望承担起让古代中国侦探故事为西方世界所理解的责任。为了实现这一目标，他在翻译中使用了符合西方风格的句法手段，可能过度使用了它们。我们还可以推测，翻译文本中更复杂的句子可能与显性化效应有关。也就是说，翻译者仔细说明了原始古代中国故事中包含的信息，这些故事发生在地理和文化上与他的读者相距甚远的国家。 可读性指标显示，翻译文本可能比创作文本更难阅读。这表明翻译文本中的句子可能要么更长，要么词更难，要么两者兼而有之。考虑到翻译文本的词汇多样性较低，我们推断句法复杂性在相对较低的可读性中发挥了更大的作用。正如我们所论证的，翻译中更复杂的句子可能表明翻译者在传达源文本信息时的努力，导致可读性的下降是无意的。尽管罗伯特·范·古利克作为翻译者和小说作家将目标读者放在心中，但似乎作为翻译者，他受到了源文本的限制，并在跨越语言和文化传达古代中国侦探故事时在一定程度上牺牲了可读性。翻译者和读者在翻译/阅读来自不同文化和非常古老时期的故事时可能不得不付出代价。 11.2 案例二、汉语失语症话语产出 详细背景请看原文献 11.2.1 研究背景 口语话语产出越来越被视为评估失语症患者（PWA）语言能力的重要来源。如果有相关话语产出的转录本，一些语言学指标，如类型-标记比（TTR）或平均语句长度，可以自动化处理，以辅助PWA的话语分析。其他类型的指标，如主要概念（Dalton &amp; Richardson, 2015; Dalton &amp; Richardson, 2019; Kong, 2009; Nicholas &amp; Brookshire, 1995; Richardson &amp; Dalton, 2016; Richardson et al., 2021）、内容单元（Yorkston &amp; Beukelman, 1980）、正确信息单元（CIUs，Nicholas &amp; Brookshire, 1993）和主要事件（Capilouto et al., 2005），则需要经过训练的标注员进行主观判断。虽然这些指标在揭示PWA语言能力和缺陷的不同方面具有重要意义，但它们通常涉及到劳动密集且耗时的过程，如转录和标注，因此很少在临床实践中实时使用。因此，需要一种可以在临床环境中轻松应用的标准化和有参照的指标。 最近，核心词汇分析被开发并提出作为一种可行且易于使用的PWA评估方法（Dalton &amp; Richardson, 2015; Kim, Berube et al., 2022; Kim et al., 2019; Kim &amp; Wright, 2020）。核心词汇分析的基本假设是，通过一组典型词汇或核心词汇（MacWhinney et al., 2010），可以评估在话语产出任务中PWA的功能性沟通能力。基于健康控制组在特定任务中的产出生成的核心词汇列表可以作为指标，衡量PWA在该任务中检索词汇的能力。这种任务特定且有参照的核心词汇列表在临床环境中非常有用，因为临床医生可以通过实时检查该列表来评估PWA的口语话语产出。核心词汇列表已被证明在区分PWA与对照组以及失语症亚型方面有效（S. G. Dalton &amp; Richardson, 2015; Kim et al., 2021, 2019）。它们还与其他语言学指标（如正确信息单元、词汇多样性、句法复杂性（Kim &amp; Wright, 2020）、主要概念（Dalton &amp; Richardson, 2015）、主题单元和连贯性指标（Kim &amp; Wright, 2020））显著相关，显示出良好的同时效度。即使对于经验和训练时间非常有限的评分员来说，核心词汇列表也表现出了可接受的评分员间信度（Kim &amp; Wright, 2020）。因此，与大多数传统指标（如TTR）需要录音、转录和标注话语产出相比，核心词汇列表节省了时间，而且比其他需要长期训练且难以维持评分员一致性的指标（如主要概念）更为客观。 到目前为止，关于普通话失语症患者（PWA）话语产出的核心词汇研究仅有一项。江及其同事使用普通话失语症语料库数据（Jiang et al., 2023），为三种不同的任务类型（即图片描述、故事叙述和程序性话语）开发了核心名词和动词列表。研究发现，PWA产生的核心词汇少于对照组参与者，但核心词汇的使用与失语症的严重程度之间没有相关性。作为该领域的开创性工作，他们的研究总体上支持了核心词汇分析在评估普通话口语话语产出中的可行性和适用性。然而，核心词汇评分是否以及如何与其他语言学指标（如词汇多样性）以及话语信息性指标（如正确信息单元，Nicholas &amp; Brookshire, 1993）相关，仍然未得到解决。 从方法论上讲，江等人（2023）研究中高频词被提取为核心词汇，但如果某个特定参与者频繁使用某个单词，这可能会导致偏差。此外，在计算核心词汇评分时，特定任务类型中的任务被合并。例如，在他们的研究中，“破窗事件”、“拒绝的雨伞”、“营救小猫”和“洪水事件”被组合为图片描述任务，而“龟兔赛跑”和“狼来了”则作为故事叙述任务的一部分。然而，考虑到核心词汇列表不仅是任务特定的，而且对引发材料敏感，不同具体任务之间可能存在重要的差异。此外，江及其同事并未将功能词纳入其核心词汇列表中，而这些词已被证明与普通话PWA的语言产出相关（Wang et al., 2019）。 因此，本研究报告了我们针对每个具体任务开发的核心词汇列表，包括内容词和功能词，并使用普通话失语症语料库中的PWA数据。此外，我们假设核心词汇评分评估了词汇层面的语言产出能力和话语信息性。通过对核心词汇评分、词汇多样性指标和CIU进行相关性分析，我们检验了这些假设。 library(tidyverse) library(tidytext) library(quanteda) library(stringr) library(jiebaR) library(readtext) library(rstatix) # for cleaning up stats library(broom) # calculate confidence intervals library(rcompanion) library(cowplot) library(flextable) # remove scientific notation options(scipen=999) # add r values in regression lines library(ggpubr) # show chinese characters in ggplot library(showtext) showtext_auto() #filter function `%!in%` &lt;- Negate(`%in%`) setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) 11.2.2 Jieba 分词 text = &quot;口语话语产出越来越被视为评估失语症患者（PWA）语言能力的重要来源。&quot; # 创建一个默认的分词器 seg1 &lt;- worker() # 使用seg1进行分词 segment(text, seg1) ## [1] &quot;口语&quot; &quot;话语&quot; &quot;产出&quot; &quot;越来越&quot; &quot;被&quot; &quot;视为&quot; &quot;评估&quot; &quot;失语症&quot; &quot;患者&quot; &quot;PWA&quot; &quot;语言&quot; &quot;能力&quot; ## [13] &quot;的&quot; &quot;重要&quot; &quot;来源&quot; # # seg3 &lt;- worker(user = &quot;demo_data/dict-ch-user-demo.txt&quot;, #设置自定义词典 # # 设置停用词 # stop_word = &quot;demo_data/stopwords-ch-demo.txt&quot;) # # segment(text, seg3) 11.2.3 汉语失语症语料分词 # 导入被试信息 subjects_all &lt;- read_csv(&quot;data/ch11/subjects.all4corelexV2.csv&quot;, na = &quot;NA&quot;) ## 导入语料 df.clean &lt;- read_csv(&quot;data/ch11/df.all.clean.2022-10-05.csv&quot;)%&gt;% mutate(subject = dplyr::recode(subject,&quot;/JiangLin40a2.cha&quot;=&quot;/JiangLin40a.cha&quot;, &quot;/JiangLin40a1.cha&quot;=&quot;/JiangLin40a.cha&quot;, &quot;/JiangLin01a2.cha&quot;=&quot;/JiangLin01a.cha&quot;, &quot;/JiangLin01a1.cha&quot;=&quot;/JiangLin01a.cha&quot;, &quot;/JiangLin70a1.cha&quot;=&quot;/JiangLin70a.cha&quot;, &quot;/JiangLin70a2.cha&quot;=&quot;/JiangLin70a.cha&quot;, &quot;/JiangLin08a2.cha&quot;= &quot;/JiangLin08a.cha&quot;, &quot;/JiangLin08a1.cha&quot;= &quot;/JiangLin08a.cha&quot;))%&gt;% filter(!is.na(task))%&gt;% mutate(subject = str_remove(subject,&quot;\\\\/&quot;), subject = str_remove(subject,&quot;.cha&quot;))%&gt;% left_join(subjects_all)%&gt;% mutate(normative = as.character(normative))%&gt;% filter(normative != &quot;0&quot;)%&gt;% filter(include == &quot;Y&quot;)%&gt;% select(-Onset_Date, -Video_Date, -Informed_Consent) ## 批量分词 my_seg &lt;- worker(bylines = T, #user = &quot;demo_data/dict-ch-user-demo.txt&quot;, symbol=T) df.tokenised = df.clean%&gt;% mutate(clean = str_replace_all(clean_new, c(&quot; 一 个 &quot; = &quot; 一个 &quot;, &quot; 一 条 &quot; = &quot;一条&quot;,&quot; 一 场 &quot; = &quot; 一场 &quot;, &quot; 一 把 &quot; = &quot; 一把 &quot;, &quot; 一 群 &quot; = &quot; 一群 &quot;, &quot; 一 次 &quot; = &quot; 一次 &quot;, &quot; 一 辆 &quot; = &quot; 一辆 &quot;, &quot; 一 只 &quot; = &quot; 一只 &quot;, &quot; 一 天 &quot; = &quot; 一天 &quot;, &quot; 一 下 &quot; = &quot; 一下 &quot;, &quot; 一 张 &quot; = &quot; 一张 &quot;, &quot; 一 道 &quot; = &quot; 一道 &quot;, &quot; 一 起 &quot; = &quot; 一起 &quot;,&quot; 一 步 &quot; = &quot; 一步 &quot;,&quot; 一 看 &quot; = &quot; 一看 &quot;, &quot; 一 匹 &quot; = &quot; 一匹 &quot;,&quot; 一 觉 &quot; = &quot; 一觉 &quot;,&quot; 一 件 &quot; = &quot; 一件 &quot;, &quot;一 位&quot; = &quot;一位&quot;,&quot;一 架&quot; = &quot;一架&quot;,&quot;一 项&quot; = &quot;一项&quot;, &quot; 发 洪水 &quot; = &quot; 发洪水 &quot;,&quot; 发 大水 &quot; = &quot; 发大水 &quot;, &quot; 一 队 &quot; = &quot; 一队 &quot;)))%&gt;% # 分词 unnest_tokens(word, ## new tokens unnested clean, ## original larger units token = function(x) ## self-defined tokenization method segment(x, jiebar = my_seg) )%&gt;% #filter non-words filter(str_detect(word, &quot;\\\\w&quot;))%&gt;% #filter english words filter(!str_detect(word, &quot;[a-zA-Z]&quot;))%&gt;% filter(word !=&quot;_&quot;)%&gt;% mutate(task = dplyr::recode(task, &quot;@task2&quot;=&quot;Pic-Window&quot;,&quot;@task3&quot;=&quot;Pic-Umbrella&quot;, &quot;@task4&quot;=&quot;Pic-CatRescue&quot;,&quot;@task5&quot;=&quot;Pic-Flood&quot;, &quot;@task6&quot;=&quot;S-TortioseHare&quot;, &quot;@task7&quot;=&quot;S-CryWolf&quot;,&quot;@task8&quot;=&quot;Proc-FriedRice&quot;))%&gt;% filter(task %!in% c(&quot;@task1&quot;, &quot;@task10&quot;, &quot;@task11&quot;, &quot;@task9&quot;)) ## 计算被试年龄和教育程度 age.aq.edu.stroke = df.tokenised%&gt;% select(subject, Age, Gender, group, normative, Education_Level, PostOnsetMonth, Aphasia_Type, AQ)%&gt;% distinct()%&gt;% mutate(AQ = as.numeric(AQ))%&gt;% group_by( normative)%&gt;% summarise(n = n(), mean.age = round(mean(Age),1), max.age = round(max(Age),1), min.age = round(min(Age),1), sd.age = round(sd(Age),1), mean.edu = round(mean(Education_Level, na.rm = TRUE),1), sd.edu = round(sd(Education_Level, na.rm = TRUE),1), max.edu= round(max(Education_Level, na.rm = TRUE),1), min.edu = round(min(Education_Level, na.rm = TRUE),1), mean.AQ = round(mean(AQ, na.rm = TRUE),1), sd.AQ = round(sd(AQ, na.rm = TRUE),1), max.AQ= round(max(AQ, na.rm = TRUE),1), min.AQ = round(min(AQ, na.rm = TRUE),1), mean.stroke = round(mean(PostOnsetMonth, na.rm = TRUE),1), sd.stroke = round(sd(PostOnsetMonth, na.rm = TRUE),1)) ## 被试年龄和教育程度统计分析 age.aq.edu.stroke.t.test = df.tokenised%&gt;% select(subject, Age, Gender, group, normative, Education_Level, Aphasia_Type)%&gt;% distinct()%&gt;% gather(measures, values, c(&quot;Age&quot;, &quot;Education_Level&quot;))%&gt;% filter(normative %in%c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))%&gt;% group_by(measures)%&gt;% rstatix::t_test(values~ normative) age.aq.edu.stroke%&gt;% filter(normative != &quot;4&quot;)%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 1 Demographic information for all individuals. &quot;) .cl-99135526{}.cl-98f398f8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-990e58dc{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-990e58f0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-990e7178{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-990e7182{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-990e718c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-990e718d{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-990e7196{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-990e71a0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.2: Table 1 Demographic information for all individuals. normativenmean.agemax.agemin.agesd.agemean.edusd.edumax.edumin.edumean.AQsd.AQmax.AQmin.AQmean.strokesd.stroke14343.1702216.212.64.420699.01.0100.095.40.00.021847.9672812.712.83.120998.91.2100.095.30.00.031844.6672313.314.25.932578.412.092.954.06.35.8 age.aq.edu.stroke.t.test%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 2. Statistical tests of age and years of education among three groups.&quot;) .cl-99318d20{}.cl-9926cd68{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-992b6080{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-992b609e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-992b7af2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-992b7b06{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-992b7b1a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-992b7b1b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-992b7b24{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-992b7b2e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.2: Table 2. Statistical tests of age and years of education among three groups. measures.y.group1group2n1n2statisticdfpp.adjp.adj.signifAgevalues124318-1.241864240.419890.2210.663nsAgevalues134318-0.360084938.629650.7210.882nsAgevalues2318180.780464533.929230.4410.882nsEducation_Levelvalues124318-0.224318444.353950.8240.948nsEducation_Levelvalues134318-1.023219225.229150.3160.948nsEducation_Levelvalues231818-0.861366325.852690.3970.948ns 11.2.4 计算词汇丰富度 chinese.TTR = df.tokenised%&gt;% mutate(word2 = dplyr::recode(word, #task 2 &quot;电视机&quot;=&quot;电视&quot;,&quot;窗&quot;=&quot;窗户&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;男孩&quot;=&quot;小孩&quot;,&quot;小朋友&quot;=&quot;小孩&quot;,&quot;足球&quot;=&quot;球&quot;, #task3 &quot;雨伞&quot;=&quot;伞&quot;,&quot;孩子&quot;=&quot;小孩&quot;,&quot;小孩子&quot;=&quot;小孩&quot;, &quot;母亲&quot;=&quot;妈妈&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task4 &quot;女孩&quot;=&quot;小孩&quot;,&quot;女儿&quot;=&quot;小孩&quot;,&quot;孩子&quot;=&quot;小孩&quot;, &quot;父亲&quot;=&quot;爸爸&quot;,&quot;车子&quot;=&quot;消防车&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task5 &quot;大水&quot;=&quot;洪水&quot;,&quot;女&quot;=&quot;小孩&quot;,&quot;消防员&quot;=&quot;战士&quot;,&quot;发大水&quot;=&quot;发洪水&quot;, &quot;解放军&quot;=&quot;战士&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task 6 &quot;兔&quot;=&quot;兔子&quot;,&quot;龟&quot;=&quot;乌龟&quot;,&quot;白兔&quot;=&quot;兔子&quot;, #task 7 &quot;孩子&quot;=&quot;小孩&quot;,&quot;农民&quot;=&quot;村民&quot;,&quot;电视机&quot;=&quot;电视&quot; ))%&gt;% select(subject, normative, group, task, word2)%&gt;% mutate(token = 1, character = nchar(word2))%&gt;% group_by(subject, normative, task)%&gt;% summarise(token = sum(token), character = sum(character), type = n_distinct(word2), ttr = type/token, c = log(type)/log(token), r = type/sqrt(token))%&gt;% filter(normative %in% c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))%&gt;% group_by(normative, task)%&gt;% get_summary_stats(token, type, character, ttr,c,r, type = &quot;mean_sd&quot;)%&gt;% select(-n)%&gt;% gather(stats, value, -(normative:variable))%&gt;% mutate(value = round(value, 1))%&gt;% unite(temp, variable, stats)%&gt;% spread(temp, value)%&gt;% select( &quot;task&quot;, &quot;normative&quot;,&quot;character_mean&quot;, &quot;character_sd&quot;, &quot;type_mean&quot;, &quot;type_sd&quot;, &quot;token_mean&quot;, &quot;token_sd&quot;, &quot;ttr_mean&quot;, &quot;ttr_sd&quot;, &quot;c_mean&quot;, &quot;c_sd&quot;, &quot;r_mean&quot;, &quot;r_sd&quot;) chinese.TTR.test = df.tokenised%&gt;% mutate(word2 = dplyr::recode(word, #task 2 &quot;电视机&quot;=&quot;电视&quot;,&quot;窗&quot;=&quot;窗户&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;男孩&quot;=&quot;小孩&quot;,&quot;小朋友&quot;=&quot;小孩&quot;,&quot;足球&quot;=&quot;球&quot;, #task3 &quot;雨伞&quot;=&quot;伞&quot;,&quot;孩子&quot;=&quot;小孩&quot;,&quot;小孩子&quot;=&quot;小孩&quot;, &quot;母亲&quot;=&quot;妈妈&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task4 &quot;女孩&quot;=&quot;小孩&quot;,&quot;女儿&quot;=&quot;小孩&quot;,&quot;孩子&quot;=&quot;小孩&quot;, &quot;父亲&quot;=&quot;爸爸&quot;,&quot;车子&quot;=&quot;消防车&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task5 &quot;大水&quot;=&quot;洪水&quot;,&quot;女&quot;=&quot;小孩&quot;,&quot;消防员&quot;=&quot;战士&quot;,&quot;发大水&quot;=&quot;发洪水&quot;, &quot;解放军&quot;=&quot;战士&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task 6 &quot;兔&quot;=&quot;兔子&quot;,&quot;龟&quot;=&quot;乌龟&quot;,&quot;白兔&quot;=&quot;兔子&quot;, #task 7 &quot;孩子&quot;=&quot;小孩&quot;,&quot;农民&quot;=&quot;村民&quot;,&quot;电视机&quot;=&quot;电视&quot; ))%&gt;% select(subject, normative, group, task, word2)%&gt;% mutate(token = 1, character = nchar(word2))%&gt;% group_by(subject, normative, task)%&gt;% summarise(token = sum(token), character = sum(character), type = n_distinct(word2), ttr = type/token, c = log(type)/log(token), r = type/sqrt(token))%&gt;% #filter(normative%in%c(&quot;2&quot;))%&gt;% #group_by(normative, task)%&gt;% gather(measures, values, c(&quot;token&quot;, &quot;character&quot;,&quot;type&quot;,&quot;ttr&quot;,&quot;c&quot;,&quot;r&quot;))%&gt;% filter(normative %in% c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))%&gt;% group_by(measures, task)%&gt;% rstatix::wilcox_test(values~ normative)%&gt;% mutate(p.adj = round(p.adj,4))%&gt;% select(-p) chinese.TTR%&gt;% filter(normative != &quot;4&quot;)%&gt;% select(-c(&quot;ttr_mean&quot;,&quot;ttr_sd&quot;,&quot;c_mean&quot;,&quot;c_sd&quot;,&quot;r_mean&quot;,&quot;r_sd&quot;))%&gt;% arrange(task)%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 3. Number of characters, word types, and tokens for each task and each group. &quot;) .cl-9c984ea4{}.cl-9c901afe{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9c937898{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c9378ac{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9c938f9a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c938fae{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c938fb8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c938fc2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c938fcc{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9c938fd6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.3: Table 3. Number of characters, word types, and tokens for each task and each group. tasknormativecharacter_meancharacter_sdtype_meantype_sdtoken_meantoken_sdPic-CatRescue1156.895.759.222.6115.368.1Pic-CatRescue2148.564.259.218.1109.645.9Pic-CatRescue383.854.535.116.666.745.1Pic-Flood1108.080.844.222.973.554.3Pic-Flood299.867.841.319.067.944.4Pic-Flood357.955.026.219.545.040.6Pic-Umbrella1142.484.255.923.8104.059.6Pic-Umbrella2131.647.353.314.699.032.7Pic-Umbrella376.841.232.313.959.533.5Pic-Window1120.497.847.624.187.269.6Pic-Window2120.364.548.314.187.746.4Pic-Window353.032.124.310.540.124.5Proc-FriedRice1127.784.848.022.188.662.2Proc-FriedRice2119.760.047.618.083.541.5Proc-FriedRice352.134.423.714.139.226.8S-CryWolf1247.9168.480.437.4183.3122.6S-CryWolf2246.8126.278.729.7185.895.6S-CryWolf3100.676.838.624.076.460.1S-TortioseHare1256.4162.186.537.3176.5112.5S-TortioseHare2240.6113.284.729.9166.179.0S-TortioseHare3100.365.735.119.171.145.6 chinese.TTR.test%&gt;% filter(measures %!in% c(&quot;c&quot;,&quot;r&quot;,&quot;ttr&quot;))%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 4 Statistical comparisions of linguistic variables among normative, control and PWA goups.&quot;) .cl-9ccbb136{}.cl-9cbf7ee8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9cc484ba{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9cc484c4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9cc49c8e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9cc49c98{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9cc49c99{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9cc49d24{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9cc49d2e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9cc49d38{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.3: Table 4 Statistical comparisions of linguistic variables among normative, control and PWA goups. taskmeasures.y.group1group2n1n2statisticp.adjp.adj.signifPic-CatRescuecharactervalues124318350.50.5690nsPic-CatRescuecharactervalues134318617.00.0009***Pic-CatRescuecharactervalues231818258.50.0050**Pic-Floodcharactervalues124318382.50.9500nsPic-Floodcharactervalues134318597.50.0030**Pic-Floodcharactervalues231818247.00.0150*Pic-Umbrellacharactervalues124318340.00.4620nsPic-Umbrellacharactervalues134317603.50.0003***Pic-Umbrellacharactervalues231817255.00.0020**Pic-Windowcharactervalues124318348.00.5430nsPic-Windowcharactervalues134318658.00.0001****Pic-Windowcharactervalues231818282.00.0003***Proc-FriedRicecharactervalues124318394.50.9120nsProc-FriedRicecharactervalues134318666.00.0000****Proc-FriedRicecharactervalues231818275.00.0007***S-CryWolfcharactervalues124318351.00.5750nsS-CryWolfcharactervalues134318633.50.0003***S-CryWolfcharactervalues231818272.00.0010**S-TortioseHarecharactervalues124318381.00.9310nsS-TortioseHarecharactervalues134318684.00.0000****S-TortioseHarecharactervalues231818274.00.0004***Pic-CatRescuetokenvalues124318358.00.6520nsPic-CatRescuetokenvalues134318589.50.0040**Pic-CatRescuetokenvalues231818250.00.0110*Pic-Floodtokenvalues124318384.50.9750nsPic-Floodtokenvalues134318559.00.0200*Pic-Floodtokenvalues231818233.50.0490*Pic-Umbrellatokenvalues124318335.50.4200nsPic-Umbrellatokenvalues134317589.00.0008***Pic-Umbrellatokenvalues231817256.50.0010**Pic-Windowtokenvalues124318345.50.5170nsPic-Windowtokenvalues134318654.00.0001****Pic-Windowtokenvalues231818280.50.0004***Proc-FriedRicetokenvalues124318387.51.0000nsProc-FriedRicetokenvalues134318633.50.0003***Proc-FriedRicetokenvalues231818266.50.0020**S-CryWolftokenvalues124318349.00.5530nsS-CryWolftokenvalues134318634.00.0003***S-CryWolftokenvalues231818265.50.0020**S-TortioseHaretokenvalues124318378.50.8990nsS-TortioseHaretokenvalues134318672.00.0000****S-TortioseHaretokenvalues231818274.00.0008***Pic-CatRescuetypevalues124318334.50.4110nsPic-CatRescuetypevalues134318647.00.0001***Pic-CatRescuetypevalues231818269.00.0010**Pic-Floodtypevalues124318397.00.8810nsPic-Floodtypevalues134318613.50.0010**Pic-Floodtypevalues231818250.00.0110*Pic-Umbrellatypevalues124318368.00.7700nsPic-Umbrellatypevalues134317616.00.0001***Pic-Umbrellatypevalues231817263.00.0006***Pic-Windowtypevalues124318321.00.3000nsPic-Windowtypevalues134318693.00.0000****Pic-Windowtypevalues231818303.00.0000****Proc-FriedRicetypevalues124318381.50.9370nsProc-FriedRicetypevalues134318651.00.0001****Proc-FriedRicetypevalues231818278.00.0005***S-CryWolftypevalues124318364.50.7280nsS-CryWolftypevalues134318649.00.0001***S-CryWolftypevalues231818273.50.0009***S-TortioseHaretypevalues124318364.50.7280nsS-TortioseHaretypevalues134318727.50.0000****S-TortioseHaretypevalues231818293.50.0001**** 11.2.5 提取核心词汇 core.lex.raw = df.tokenised%&gt;% filter(normative == &quot;1&quot;)%&gt;% ## filter some functional words # anti_join(fWord)%&gt;% #merge some words mutate(word2 = dplyr::recode(word, #task 2 &quot;电视机&quot;=&quot;电视&quot;,&quot;窗&quot;=&quot;窗户&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;男孩&quot;=&quot;小孩&quot;,&quot;小朋友&quot;=&quot;小孩&quot;,&quot;足球&quot;=&quot;球&quot;, #task3 &quot;雨伞&quot;=&quot;伞&quot;,&quot;孩子&quot;=&quot;小孩&quot;,&quot;小孩子&quot;=&quot;小孩&quot;, &quot;母亲&quot;=&quot;妈妈&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task4 &quot;女孩&quot;=&quot;小孩&quot;,&quot;女儿&quot;=&quot;小孩&quot;,&quot;孩子&quot;=&quot;小孩&quot;, &quot;父亲&quot;=&quot;爸爸&quot;,&quot;车子&quot;=&quot;消防车&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task5 &quot;大水&quot;=&quot;洪水&quot;,&quot;女&quot;=&quot;小孩&quot;,&quot;消防员&quot;=&quot;战士&quot;, &quot;发大水&quot;=&quot;发洪水&quot;, &quot;解放军&quot;=&quot;战士&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task 6 &quot;兔&quot;=&quot;兔子&quot;,&quot;龟&quot;=&quot;乌龟&quot;,&quot;白兔&quot;=&quot;兔子&quot;, #task 7 &quot;孩子&quot;=&quot;小孩&quot;,&quot;农民&quot;=&quot;村民&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;饭&quot;=&quot;米饭&quot; ))%&gt;% group_by(subject,task)%&gt;% count(word2, sort = TRUE) core.lex.freq = core.lex.raw%&gt;% group_by(task, word2)%&gt;% summarise(freq = sum(n)) # number of controls n.ctr = df.tokenised%&gt;% filter(normative == &quot;1&quot;)%&gt;% distinct(subject)%&gt;% nrow() core.lex.ctrl = core.lex.raw%&gt;% mutate(present = 1)%&gt;% group_by(task, word2)%&gt;% summarise(x1distribute = sum(present)/n.ctr)%&gt;% mutate(x1distribute = round(x1distribute, 2)*100)%&gt;% arrange(desc(x1distribute)) %&gt;% mutate(top.no = 1:n())%&gt;% #filter(distribute &gt; 0.5) filter(top.no &lt; 31)%&gt;% mutate(x2chinese = case_when( word2 == &quot;了&quot; ~ &quot;FC1&quot;, word2 == &quot;的&quot; ~ &quot;FC2&quot;, word2 == &quot;一&quot; ~ &quot;one&quot;, word2 == &quot;树&quot; ~ &quot;tree&quot;, word2 == &quot;上&quot; ~ &quot;up&quot;, word2 == &quot;猫&quot; ~ &quot;cat&quot;, word2 == &quot;在&quot; ~ &quot;in&quot;, word2 == &quot;狗&quot; ~ &quot;dog&quot;, word2 == &quot;小&quot; ~ &quot;small&quot;, word2 == &quot;救&quot; ~ &quot;save&quot;, word2 == &quot;有&quot; ~ &quot;have&quot;, word2 == &quot;下来&quot; ~ &quot;come down&quot;, word2 == &quot;这个&quot; ~ &quot;this&quot;,word2 == &quot;是&quot; ~ &quot;link verb&quot;,word2 == &quot;然后&quot; ~ &quot;then&quot;, word2 == &quot;一个&quot; ~ &quot;classifier (one)&quot;,word2 == &quot;一只&quot; ~ &quot;classifier (one)&quot;, word2 == &quot;不&quot; ~ &quot;negative marker&quot;, word2 == &quot;把&quot; ~ &quot;FC3&quot;, word2 == &quot;来&quot; ~ &quot;come&quot;, word2 == &quot;梯子&quot; ~ &quot;ladder&quot;, word2 == &quot;爬&quot; ~ &quot;climb&quot;, word2 == &quot;到&quot; ~ &quot;arrive&quot;, word2 == &quot;小孩&quot; ~ &quot;kid&quot;, word2 == &quot;就&quot; ~ &quot;FC4&quot;, word2 == &quot;着&quot; ~ &quot;FC5&quot;, word2 == &quot;这&quot; ~ &quot;this&quot;, word2 == &quot;下&quot; ~ &quot;down&quot;, word2 == &quot;想&quot; ~ &quot;think&quot;, word2 == &quot;也&quot; ~ &quot;too&quot;, word2 == &quot;叫&quot; ~ &quot;shout&quot;, word2 == &quot;她&quot; ~ &quot;she&quot;, word2 == &quot;树枝&quot; ~ &quot;branch&quot;, word2 == &quot;被&quot; ~ &quot;FC6&quot;, word2 == &quot;水&quot; ~ &quot;water&quot;, word2 == &quot;吧&quot; ~ &quot;FC7&quot;, word2 == &quot;就是&quot; ~ &quot;be exactly&quot;, word2 == &quot;一&quot; ~ &quot;one&quot;, word2 == &quot;发&quot; ~ &quot;flooding&quot;, word2 == &quot;个&quot; ~ &quot;classifier&quot;, word2 == &quot;里&quot; ~ &quot;inside&quot;, word2 == &quot;伞&quot; ~ &quot;umbrella&quot;, word2 == &quot;妈妈&quot; ~ &quot;mum&quot;, word2 == &quot;他&quot; ~ &quot;he&quot;, word2 == &quot;下雨&quot; ~ &quot;raining&quot;, word2 == &quot;带&quot; ~ &quot;carry&quot;, word2 == &quot;雨&quot; ~ &quot;rain&quot;, word2 == &quot;上学&quot; ~ &quot;go to school&quot;, word2 == &quot;走&quot; ~ &quot;go&quot;, word2 == &quot;淋&quot; ~ &quot;get wet (by rain)&quot;, word2 == &quot;去&quot; ~ &quot;go&quot;, word2 == &quot;要&quot; ~ &quot;want&quot;, word2 == &quot;时候&quot; ~ &quot;time&quot;, word2 == &quot;跑&quot; ~ &quot;run&quot;, word2 == &quot;没&quot; ~ &quot;none&quot;, word2 == &quot;给&quot; ~ &quot;give&quot;, word2 == &quot;说&quot; ~ &quot;say&quot;, word2 == &quot;窗户&quot; ~ &quot;window&quot;, word2 == &quot;看&quot; ~ &quot;look&quot;, word2 == &quot;玻璃&quot; ~ &quot;glass&quot;, word2 == &quot;踢&quot; ~ &quot;kick&quot;, word2 == &quot;球&quot; ~ &quot;ball&quot;, word2 == &quot;人&quot; ~ &quot;man&quot;, word2 == &quot;电视&quot; ~ &quot;TV&quot;, word2 == &quot;那个&quot; ~ &quot;that&quot;, word2 == &quot;家&quot; ~ &quot;home&quot;, word2 == &quot;我&quot; ~ &quot;I&quot;, word2 == &quot;火腿肠&quot; ~ &quot;sausage&quot;, word2 == &quot;油&quot; ~ &quot;oil&quot;, word2 == &quot;蛋炒饭&quot; ~ &quot;egg fried rice&quot;, word2 == &quot;放&quot; ~ &quot;put&quot;, word2 == &quot;先&quot; ~ &quot;first&quot;, word2 == &quot;锅&quot; ~ &quot;wok&quot;, word2 == &quot;鸡蛋&quot; ~ &quot;egg&quot;, word2 == &quot;再&quot; ~ &quot;then&quot;, word2 == &quot;切&quot; ~ &quot;cut&quot;, word2 == &quot;倒&quot; ~ &quot;pour&quot;, word2 == &quot;米饭&quot; ~ &quot;rice&quot;, word2 == &quot;可以&quot; ~ &quot;can&quot;, word2 == &quot;米饭&quot; ~ &quot;that&quot;, word2 == &quot;家&quot; ~ &quot;home&quot;, word2 == &quot;好&quot; ~ &quot;good&quot;, word2 == &quot;做&quot; ~ &quot;do&quot;, word2 == &quot;进去&quot; ~ &quot;into&quot;, word2 == &quot;盐&quot; ~ &quot;salt&quot;, word2 == &quot;一下&quot; ~ &quot;one time&quot;, word2 == &quot;那个&quot; ~ &quot;that&quot;, word2 == &quot;狼&quot; ~ &quot;wolf&quot;, word2 == &quot;羊&quot; ~ &quot;sheep&quot;, word2 == &quot;都&quot; ~ &quot;all&quot;, word2 == &quot;喊&quot; ~ &quot;shout&quot;, word2 == &quot;吃&quot; ~ &quot;eat&quot;, word2 == &quot;真的&quot; ~ &quot;real&quot;, word2 == &quot;次&quot; ~ &quot;(this) time&quot;, word2 == &quot;又&quot; ~ &quot;again&quot;, word2 == &quot;山&quot; ~ &quot;hill&quot;, word2 == &quot;兔子&quot; ~ &quot;rabbit&quot;, word2 == &quot;乌龟&quot; ~ &quot;tortoise&quot;, word2 == &quot;它&quot; ~ &quot;it&quot;, word2 == &quot;赛跑&quot; ~ &quot;race&quot;, word2 == &quot;过&quot; ~ &quot;FC9&quot;, word2 == &quot;和&quot; ~ &quot;and&quot;, word2 == &quot;得&quot; ~ &quot;FC8&quot;, word2 == &quot;快&quot; ~ &quot;quick&quot;, word2 == &quot;睡&quot; ~ &quot;sleep&quot;, word2 == &quot;喊&quot; ~ &quot;shout&quot;, word2 == &quot;吃&quot; ~ &quot;eat&quot;, word2 == &quot;终点&quot; ~ &quot;end&quot;, word2 == &quot;很&quot; ~ &quot;very&quot;, word2 == &quot;森林&quot; ~ &quot;forest&quot;, word2 == &quot;比赛&quot; ~ &quot;games&quot;, word2 == &quot;听&quot; ~ &quot;hear&quot;, word2 == &quot;已经&quot; ~ &quot;already&quot;,word2 == &quot;开始&quot; ~ &quot;begin&quot;, word2 == &quot;洪水&quot; ~ &quot;flood&quot;, word2 == &quot;炒&quot; ~ &quot;stir fry&quot;, word2 == &quot;村民&quot; ~ &quot;villager&quot;, word2 == &quot;里面&quot; ~ &quot;inside&quot;, TRUE ~ &quot;####&quot;)) core.lex.ctrl.wide = core.lex.ctrl%&gt;% ungroup()%&gt;% gather(measures, value, c(&quot;word2&quot;, &quot;x1distribute&quot;, &quot;x2chinese&quot;))%&gt;% mutate(measures = paste(task, measures, sep = &quot;-&quot;))%&gt;% select(-task)%&gt;% spread(measures, value) core.lex.ctrl.wide%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 5 Core lexicon for different tasks&quot;) .cl-9d6d598c{}.cl-9d61c3b0{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9d65e3c8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9d65e3dc{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9d65fb56{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d65fb6a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d65fb74{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d65fb75{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d65fb7e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9d65fb88{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.4: Table 5 Core lexicon for different tasks top.noPic-CatRescue-word2Pic-CatRescue-x1distributePic-CatRescue-x2chinesePic-Flood-word2Pic-Flood-x1distributePic-Flood-x2chinesePic-Umbrella-word2Pic-Umbrella-x1distributePic-Umbrella-x2chinesePic-Window-word2Pic-Window-x1distributePic-Window-x2chineseProc-FriedRice-word2Proc-FriedRice-x1distributeProc-FriedRice-x2chineseS-CryWolf-word2S-CryWolf-x1distributeS-CryWolf-x2chineseS-TortioseHare-word2S-TortioseHare-x1distributeS-TortioseHare-x2chinese1了100FC1了91FC1了100FC1了100FC1火腿肠100sausage了100FC1乌龟100tortoise2树95tree的86FC2伞100umbrella踢100kick炒98stir fry他100he了100FC13上93up在84in妈妈95mum球95ball把95FC3来98come兔子100rabbit4在88in一个81classifier (one)他91he在91in米饭95rice狼98wolf就98FC45下来84come down这个77this下雨88raining的86FC2放88put的98FC2的98FC26猫84cat小孩74kid的86FC2到79arrive了84FC1就95FC4在95in7的84FC2是74link verb不81negative marker把77FC3油81oil在93in它93it8狗81dog救72save带81carry窗户72window然后81then羊93sheep跑93run9不79negative marker洪水67flood上学79go to school这个72this蛋炒饭81egg fried rice没91none快79quick10把77FC3有60have着79FC5是70link verb先79first放86put赛跑79race11小74small然后58then就74FC4然后70then就74FC4有86have过79FC912来74come这58this小孩70kid一个65classifier (one)锅70wok一个84classifier (one)一77one13梯子74ladder她51she雨70rain他63he的67FC2小孩79kid到74arrive14然后74then着51FC5然后67then看60look再63then人77man得74FC815爬74climb把47FC3要65want不53negative marker鸡蛋63egg喊77shout我74I16救72save树枝47branch走65go就53FC4切60cut是77link verb时候74time17是72link verb一44one去60go玻璃53glass可以56can都77all睡74sleep18这个72this水44water时候60time一51one好53good真的72real不72negative marker19有70have上42up淋60get wet (by rain)上51up是53link verb不70negative marker有72have20到67arrive吧37FC7在58in小孩51kid做51do时候70time是67link verb21着67FC5就37FC4有53have吧49FC7在51in说70say然后67then22一个65classifier (one)不35negative marker说53say电视49TV倒49pour又65again动物65####23就65FC4个35classifier这个53this这49this盐44salt然后65then听65hear24下60down去35go上51up有44have里面44inside吃63eat和65and25也60too来35come是51link verb着44FC5我42I这个60this很65very26这60this被35FC6跑51run人42man进去42into听58hear比赛63games27去58go小33small把49FC3个40classifier一下40one time呢58####爬63climb28她58she里33inside没49none那个40that之后37####把58FC3终点63end29小孩58kid中28####到47arrive里面40inside以后37####次58(this) time开始60begin30想58think到28arrive回47####家37home就是37be exactly上56up没60none 11.2.6 比较核心词汇分数 core.score.pwa.ctrl = df.tokenised%&gt;% #filter(Age &gt; 20 &amp; Age &lt; 65 )%&gt;% filter(normative %in% c(&quot;2&quot;,&quot;3&quot;))%&gt;% select(subject,group,task,word, AQ, Aphasia_Type, include)%&gt;% filter(include == &quot;Y&quot;)%&gt;% ## filter some functional words #anti_join(fWord)%&gt;% #merge some words mutate(word2 = dplyr::recode(word, #task 2 &quot;电视机&quot;=&quot;电视&quot;,&quot;窗&quot;=&quot;窗户&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;男孩&quot;=&quot;小孩&quot;,&quot;小朋友&quot;=&quot;小孩&quot;,&quot;足球&quot;=&quot;球&quot;, #task3 &quot;雨伞&quot;=&quot;伞&quot;,&quot;孩子&quot;=&quot;小孩&quot;,&quot;小孩子&quot;=&quot;小孩&quot;, &quot;母亲&quot;=&quot;妈妈&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task4 &quot;女孩&quot;=&quot;小孩&quot;,&quot;女儿&quot;=&quot;小孩&quot;,&quot;孩子&quot;=&quot;小孩&quot;, &quot;父亲&quot;=&quot;爸爸&quot;,&quot;车子&quot;=&quot;消防车&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task5 &quot;大水&quot;=&quot;洪水&quot;,&quot;女&quot;=&quot;小孩&quot;,&quot;消防员&quot;=&quot;战士&quot;,&quot;发大水&quot;=&quot;发洪水&quot;, &quot;解放军&quot;=&quot;战士&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task 6 &quot;兔&quot;=&quot;兔子&quot;,&quot;龟&quot;=&quot;乌龟&quot;,&quot;白兔&quot;=&quot;兔子&quot;, #task 7 &quot;孩子&quot;=&quot;小孩&quot;,&quot;农民&quot;=&quot;村民&quot;,&quot;电视机&quot;=&quot;电视&quot; ))%&gt;% select(-word)%&gt;% distinct()%&gt;% inner_join(core.lex.ctrl, by = c(&quot;task&quot;,&quot;word2&quot;))%&gt;% mutate(present = 1)%&gt;% group_by(subject, group, task)%&gt;% filter(task!=&quot;@task1&quot; &amp; task != &quot;@task9&quot;)%&gt;% summarise(score = sum(present))%&gt;% mutate(id = paste(subject, group, sep = &quot;_&quot;)) cn.matrix = data.frame( expand.grid(task2 = unique(core.score.pwa.ctrl$task), id2 = unique(core.score.pwa.ctrl$id)))%&gt;% mutate(unique_id = paste(id2, task2, sep = &quot;_&quot;)) fig.core.score = core.score.pwa.ctrl %&gt;% mutate(unique_id = paste(id, task, sep = &quot;_&quot;))%&gt;% right_join(cn.matrix)%&gt;% mutate(subject= str_extract(id2, &quot;^\\\\w*_&quot;), subject = str_remove(subject,&quot;_&quot;), task = task2, group = str_extract(id2, &quot;control|patient&quot;))%&gt;% # replace na with 0 mutate(score = coalesce(score, 0))%&gt;% groupwiseMean(score ~ group + task, data = ., conf = 0.95, digits = 3) %&gt;% ggplot(.,aes(task, Mean, fill = group))+ geom_bar(colour = &quot;black&quot;, stat = &quot;identity&quot;, position = position_dodge(.9))+ geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper), width = .2, size = 0.7, position = position_dodge(.9))+ scale_fill_manual(values=c(&quot;grey75&quot;, &quot;white&quot;), name=&quot;Participants&quot;, labels=c(&quot;Control&quot;, &quot;PWA&quot;))+ labs(y = &quot;Core Lexicon scores&quot;, x = &quot;Task&quot;)+ theme_bw() fig.core.score ## 统计分析 core.score.pwa.ctrl.df = core.score.pwa.ctrl %&gt;% mutate(unique_id = paste(id, task, sep = &quot;_&quot;))%&gt;% right_join(cn.matrix)%&gt;% mutate(subject= str_extract(id2, &quot;^\\\\w*_&quot;), subject = str_remove(subject,&quot;_&quot;), task = task2, group = str_extract(id2, &quot;control|patient&quot;))%&gt;% # replace na with 0 mutate(score = coalesce(score, 0)) stat.test.normality = core.score.pwa.ctrl.df %&gt;% group_by(task)%&gt;% rstatix::shapiro_test(score)%&gt;% rstatix::add_significance(&quot;p&quot;) core.lex.wilcox = core.score.pwa.ctrl.df %&gt;% group_by(task)%&gt;% rstatix::wilcox_test(score ~ group)%&gt;% rstatix::add_significance(&quot;p&quot;) core.lex.wilcox.effectsize = core.score.pwa.ctrl.df %&gt;% group_by(task)%&gt;% wilcox_effsize(score ~ group) core.lex.wilcox.result = core.lex.wilcox%&gt;% select(task, n1, n2, statistic, p, p.signif)%&gt;% left_join(core.lex.wilcox.effectsize)%&gt;% select(task, n1, n2, statistic, p, p.signif, effsize, magnitude)%&gt;% mutate(effsize = round(effsize,2), statistic = round(statistic, 0)) core.lex.wilcox.result%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 7 Statistical comparisons of core lexicon scores between healthy controls and PWA in each discourse production task. &quot;) .cl-9e8af75c{}.cl-9e83c31a{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9e86a35a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9e86a364{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9e86b548{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9e86b552{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9e86b566{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9e86b570{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9e86b57a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9e86b57b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.5: Table 7 Statistical comparisons of core lexicon scores between healthy controls and PWA in each discourse production task. taskn1n2statisticpp.signifeffsizemagnitudePic-CatRescue18182910.00004570****0.68largePic-Flood18182680.00082000***0.56largePic-Umbrella18182900.00005000****0.68largePic-Window18183110.00000240****0.79largeProc-FriedRice18182910.00004450****0.68largeS-CryWolf18183080.00000373****0.77largeS-TortioseHare18183000.00001320****0.73large 11.2.7 核心词汇分数和词汇丰富度相关性分析 ### correlation with aphasia severity score.aq.correlation = core.score.pwa.ctrl.df %&gt;% left_join(subjects_all)%&gt;% filter(group == &quot;patient&quot;)%&gt;% select(subject, group, task, AQ, score)%&gt;% mutate(AQ = as.numeric(AQ), task = dplyr::recode(task, &quot;@task2&quot;=&quot;Pic-Window&quot;,&quot;@task3&quot;=&quot;Pic-Umbrella&quot;, &quot;@task4&quot;=&quot;Pic-CatRescue&quot;,&quot;@task5&quot;=&quot;Pic-Flood&quot;, &quot;@task6&quot;=&quot;S-TortioseHare&quot;, &quot;@task7&quot;=&quot;S-CryWolf&quot;,&quot;@task8&quot;=&quot;Proc-FriedRice&quot;))%&gt;% group_by(task)%&gt;% cor_test(AQ, score, method = &quot;spearman&quot;) ### linguistic variables linguistic = df.tokenised%&gt;% mutate(word2 = dplyr::recode(word, #task 2 &quot;电视机&quot;=&quot;电视&quot;,&quot;窗&quot;=&quot;窗户&quot;,&quot;电视机&quot;=&quot;电视&quot;, &quot;男孩&quot;=&quot;小孩&quot;,&quot;小朋友&quot;=&quot;小孩&quot;,&quot;足球&quot;=&quot;球&quot;, #task3 &quot;雨伞&quot;=&quot;伞&quot;,&quot;孩子&quot;=&quot;小孩&quot;,&quot;小孩子&quot;=&quot;小孩&quot;, &quot;母亲&quot;=&quot;妈妈&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task4 &quot;女孩&quot;=&quot;小孩&quot;,&quot;女儿&quot;=&quot;小孩&quot;,&quot;孩子&quot;=&quot;小孩&quot;, &quot;父亲&quot;=&quot;爸爸&quot;,&quot;车子&quot;=&quot;消防车&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task5 &quot;大水&quot;=&quot;洪水&quot;,&quot;女&quot;=&quot;小孩&quot;,&quot;消防员&quot;=&quot;战士&quot;,&quot;发大水&quot;=&quot;发洪水&quot;, &quot;解放军&quot;=&quot;战士&quot;,&quot;电视机&quot;=&quot;电视&quot;,&quot;电视机&quot;=&quot;电视&quot;, #task 6 &quot;兔&quot;=&quot;兔子&quot;,&quot;龟&quot;=&quot;乌龟&quot;,&quot;白兔&quot;=&quot;兔子&quot;, #task 7 &quot;孩子&quot;=&quot;小孩&quot;,&quot;农民&quot;=&quot;村民&quot;,&quot;电视机&quot;=&quot;电视&quot; ))%&gt;% select(subject, normative, group, task, word2)%&gt;% mutate(token = 1, character = nchar(word2))%&gt;% group_by(subject, normative, task)%&gt;% summarise(token = sum(token), character = sum(character), type = n_distinct(word2), ttr = type/token, c = log(type)/log(token), r = type/sqrt(token))%&gt;% filter(normative %in% c(&quot;2&quot;,&quot;3&quot;))%&gt;% mutate(group = dplyr::recode(normative, &quot;2&quot;=&quot;control&quot;,&quot;3&quot;=&quot;patient&quot;)) score.linguistic.correlation.full = core.score.pwa.ctrl.df%&gt;% #filter(group == &quot;patient&quot;)%&gt;% left_join(linguistic)%&gt;% mutate_all(~ifelse(is.na(.), 0, .))%&gt;% left_join(subjects_all, by = c(&quot;subject&quot;, &quot;group&quot;))%&gt;% mutate(AQ = as.numeric(AQ))%&gt;% group_by(group, task)%&gt;% cor_test(token, character, type, ttr, score, c,r,AQ, method = &quot;spearman&quot;) score.linguistic.cor.final = score.linguistic.correlation.full%&gt;% mutate(var = paste(var1, var2, sep = &quot;-&quot;), statistic = round(statistic, 0), p = round(p,4))%&gt;% filter(group == &quot;patient&quot;)%&gt;% filter(var %in% c(&quot;score-ttr&quot;,&quot;score-c&quot;,&quot;score-r&quot;,&quot;score-AQ&quot;, &quot;ttr-AQ&quot;,&quot;c-AQ&quot;,&quot;r-AQ&quot;)) score.aq.correlation %&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 8 Correlations of core lexicon scores with aphasia serverity. &quot;) .cl-a954feda{}.cl-a94e0ce2{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a950e958{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a950e962{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a950fac4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a950face{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a950facf{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a950fad8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a950fad9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a950fae2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.6: Table 8 Correlations of core lexicon scores with aphasia serverity. taskvar1var2corstatisticpmethodPic-CatRescueAQscore0.460524.46970.0555SpearmanPic-FloodAQscore0.390593.64180.1120SpearmanPic-UmbrellaAQscore0.230748.08620.3630SpearmanPic-WindowAQscore0.069901.68670.7840SpearmanProc-FriedRiceAQscore0.410573.86960.0930SpearmanS-CryWolfAQscore0.470517.13080.0511SpearmanS-TortioseHareAQscore0.510474.97930.0307Spearman chinese.TTR %&gt;% select(&quot;task&quot;,&quot;normative&quot;,&quot;ttr_mean&quot;,&quot;ttr_sd&quot;,&quot;c_mean&quot;,&quot;c_sd&quot;,&quot;r_mean&quot;,&quot;r_sd&quot;)%&gt;% arrange(task)%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 9. Lexical diversity measures of each group per task. &quot;) .cl-a9831e3c{}.cl-a97b2cd6{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a97e56f4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a97e56fe{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a97e6c70{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a97e6c7a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a97e6c84{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a97e6c85{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a97e6c8e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a97e6c98{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.6: Table 9. Lexical diversity measures of each group per task. tasknormativettr_meanttr_sdc_meanc_sdr_meanr_sdPic-CatRescue10.60.10.90.05.60.7Pic-CatRescue20.60.10.90.05.70.7Pic-CatRescue30.60.10.90.04.30.8Pic-Flood10.70.10.90.05.20.9Pic-Flood20.60.10.90.05.01.0Pic-Flood30.70.20.90.13.91.1Pic-Umbrella10.60.10.90.05.50.9Pic-Umbrella20.60.10.90.05.30.7Pic-Umbrella30.60.10.90.04.20.8Pic-Window10.60.10.90.05.20.8Pic-Window20.60.10.90.05.30.4Pic-Window30.70.20.90.13.90.8Proc-FriedRice10.60.10.90.05.10.8Proc-FriedRice20.60.10.90.05.20.7Proc-FriedRice30.70.20.90.13.71.0S-CryWolf10.50.10.90.06.00.9S-CryWolf20.50.10.80.05.80.8S-CryWolf30.60.10.90.04.41.2S-TortioseHare10.50.10.90.06.60.9S-TortioseHare20.50.10.90.06.60.9S-TortioseHare30.50.10.80.04.11.1 chinese.TTR.test %&gt;% filter(measures %in% c(&quot;c&quot;,&quot;r&quot;,&quot;ttr&quot;))%&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 10 Statistical comparisions of lexical diversity measures among normative, control and PWA goups per task. &quot;) .cl-a9b50a82{}.cl-a9aa0ee8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a9ade59a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a9ade5ae{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a9adfcb0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9adfcc4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9adfcce{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9adfcd8{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9adfce2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9adfce3{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.6: Table 10 Statistical comparisions of lexical diversity measures among normative, control and PWA goups per task. taskmeasures.y.group1group2n1n2statisticp.adjp.adj.signifPic-CatRescuecvalues124318352.00.7680nsPic-CatRescuecvalues134318443.00.7680nsPic-CatRescuecvalues231818199.00.7560nsPic-Floodcvalues124318455.00.8670nsPic-Floodcvalues134318414.51.0000nsPic-Floodcvalues231818159.01.0000nsPic-Umbrellacvalues124318463.50.4580nsPic-Umbrellacvalues134317465.00.3150nsPic-Umbrellacvalues231817190.00.4580nsPic-Windowcvalues124318387.01.0000nsPic-Windowcvalues134318407.51.0000nsPic-Windowcvalues231818167.01.0000nsProc-FriedRicecvalues124318308.00.6510nsProc-FriedRicecvalues134318370.01.0000nsProc-FriedRicecvalues231818168.01.0000nsS-CryWolfcvalues124318471.00.5610nsS-CryWolfcvalues134318338.00.5610nsS-CryWolfcvalues231818126.00.5610nsS-TortioseHarecvalues124318357.00.6440nsS-TortioseHarecvalues134318543.00.0390*S-TortioseHarecvalues231818234.00.0450*Pic-CatRescuervalues124318336.00.4280nsPic-CatRescuervalues134318685.00.0000****Pic-CatRescuervalues231818294.00.0000****Pic-Floodrvalues124318398.00.8690nsPic-Floodrvalues134318665.50.0000****Pic-Floodrvalues231818259.00.0030**Pic-Umbrellarvalues124318431.50.4870nsPic-Umbrellarvalues134317635.00.0000****Pic-Umbrellarvalues231817261.00.0004***Pic-Windowrvalues124318332.00.3890nsPic-Windowrvalues134318688.50.0000****Pic-Windowrvalues231818315.00.0000****Proc-FriedRicervalues124318363.00.7130nsProc-FriedRicervalues134318657.00.0001****Proc-FriedRicervalues231818281.00.0004***S-CryWolfrvalues124318432.00.4820nsS-CryWolfrvalues134318657.00.0001****S-CryWolfrvalues231818270.00.0008***S-TortioseHarervalues124318351.00.5780nsS-TortioseHarervalues134318742.00.0000****S-TortioseHarervalues231818308.00.0000****Pic-CatRescuettrvalues124318375.01.0000nsPic-CatRescuettrvalues134318344.01.0000nsPic-CatRescuettrvalues231818150.51.0000nsPic-Floodttrvalues124318441.51.0000nsPic-Floodttrvalues134318358.01.0000nsPic-Floodttrvalues231818132.01.0000nsPic-Umbrellattrvalues124318440.01.0000nsPic-Umbrellattrvalues134317379.01.0000nsPic-Umbrellattrvalues231817129.01.0000nsPic-Windowttrvalues124318404.50.8670nsPic-Windowttrvalues134318319.50.8670nsPic-Windowttrvalues231818130.50.8670nsProc-FriedRicettrvalues124318332.50.4700nsProc-FriedRicettrvalues134318268.50.1860nsProc-FriedRicettrvalues231818124.00.4700nsS-CryWolfttrvalues124318445.00.3630nsS-CryWolfttrvalues134318230.00.0270*S-CryWolfttrvalues23181879.00.0270*S-TortioseHarettrvalues124318364.01.0000nsS-TortioseHarettrvalues134318367.01.0000nsS-TortioseHarettrvalues231818165.51.0000ns score.linguistic.cor.final %&gt;% flextable()%&gt;% set_caption(caption = &quot;Table 11. Correlations of core lexicon scores with lexical diversity R.&quot;) .cl-a9f71b52{}.cl-a9ed3cd6{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a9f0fb1e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a9f0fb32{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a9f11388{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9f1139c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9f113a6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9f113a7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9f113b0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a9f113ba{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 11.6: Table 11. Correlations of core lexicon scores with lexical diversity R. grouptaskvar1var2corstatisticpmethodvarpatientPic-CatRescuettrAQ0.2107660.4030Spearmanttr-AQpatientPic-FloodttrAQ-0.13010920.6160Spearmanttr-AQpatientPic-UmbrellattrAQ0.0159540.9540Spearmanttr-AQpatientPic-WindowttrAQ0.3706120.1330Spearmanttr-AQpatientProc-FriedRicettrAQ0.0938790.7140Spearmanttr-AQpatientS-CryWolfttrAQ-0.20011630.4250Spearmanttr-AQpatientS-TortioseHarettrAQ0.3706140.1350Spearmanttr-AQpatientPic-CatRescuescorettr-0.41013690.0884Spearmanscore-ttrpatientPic-Floodscorettr-0.68016320.0018Spearmanscore-ttrpatientPic-Umbrellascorettr0.1208540.6380Spearmanscore-ttrpatientPic-Windowscorettr-0.50014530.0347Spearmanscore-ttrpatientProc-FriedRicescorettr-0.41013630.0937Spearmanscore-ttrpatientS-CryWolfscorettr-0.70016490.0012Spearmanscore-ttrpatientS-TortioseHarescorettr-0.40013570.0996Spearmanscore-ttrpatientPic-CatRescuescorec-0.11010780.6580Spearmanscore-cpatientPic-Floodscorec-0.55015030.0177Spearmanscore-cpatientPic-Umbrellascorec0.4405410.0667Spearmanscore-cpatientPic-Windowscorec-0.35013080.1550Spearmanscore-cpatientProc-FriedRicescorec-0.20011650.4220Spearmanscore-cpatientS-CryWolfscorec-0.32012800.1940Spearmanscore-cpatientS-TortioseHarescorec-0.12010890.6240Spearmanscore-cpatientPic-CatRescuescorer0.8001980.0001Spearmanscore-rpatientPic-Floodscorer0.8301690.0000Spearmanscore-rpatientPic-Umbrellascorer0.8101820.0000Spearmanscore-rpatientPic-Windowscorer0.6103800.0075Spearmanscore-rpatientProc-FriedRicescorer0.8701290.0000Spearmanscore-rpatientS-CryWolfscorer0.8701240.0000Spearmanscore-rpatientS-TortioseHarescorer0.8801170.0000Spearmanscore-rpatientPic-CatRescuescoreAQ0.4605240.0555Spearmanscore-AQpatientPic-FloodscoreAQ0.3905940.1120Spearmanscore-AQpatientPic-UmbrellascoreAQ0.2307480.3630Spearmanscore-AQpatientPic-WindowscoreAQ0.0699020.7840Spearmanscore-AQpatientProc-FriedRicescoreAQ0.4105740.0930Spearmanscore-AQpatientS-CryWolfscoreAQ0.4705170.0511Spearmanscore-AQpatientS-TortioseHarescoreAQ0.5104750.0307Spearmanscore-AQpatientPic-CatRescuecAQ0.4205660.0874Spearmanc-AQpatientPic-FloodcAQ-0.08810540.7290Spearmanc-AQpatientPic-UmbrellacAQ0.2007760.4270Spearmanc-AQpatientPic-WindowcAQ0.3905900.1090Spearmanc-AQpatientProc-FriedRicecAQ0.1408350.5840Spearmanc-AQpatientS-CryWolfcAQ0.1308400.5980Spearmanc-AQpatientS-TortioseHarecAQ0.5004840.0363Spearmanc-AQpatientPic-CatRescuerAQ0.6003920.0105Spearmanr-AQpatientPic-FloodrAQ0.4605240.0569Spearmanr-AQpatientPic-UmbrellarAQ0.2007760.4270Spearmanr-AQpatientPic-WindowrAQ0.1208560.6440Spearmanr-AQpatientProc-FriedRicerAQ0.1508190.5390Spearmanr-AQpatientS-CryWolfrAQ0.6303620.0065Spearmanr-AQpatientS-TortioseHarerAQ0.6803120.0026Spearmanr-AQ "],["案例三语音数据处理案例.html", "Chapter 12 案例三、语音数据处理案例 12.1 研究背景 12.2 数据导入及整理 12.3 数据建模 12.4 展示与呈现", " Chapter 12 案例三、语音数据处理案例 12.1 研究背景 详细背景请看原文献 越南南部方言中SV214调（hỏi）和SV415调（ngã）的音调合并，使得南部越南语的音系系统与北部越南语不同。鉴于这一差异的重要性及其对泰语音调感知同化模式的潜在影响，我们在声学研究中验证了南部越南语中这两个音调的合并情况。为了对基频（f0）轮廓进行全面而动态的比较，我们采用了广义加性混合模型（GAMM），这是一种非线性回归方法，不需要对数据进行聚合或预先选择轮廓中的固定时间点。该方法能够检测动态变化数据中的一般模式，同时考虑到受试者和项目相关的变异性（Nixon, van Rij, Mok, Baayen, &amp; Chen, 2016; Wieling, 2018）。通过这种方式，它可以揭示在数据聚合或任意选择单一时间点时被掩盖的模式。 setwd(&quot;~/Nutstore Files/310_Tutorial/LanguageDS-e&quot;) #install.packages(&quot;plotfunctions&quot;) library(tidyverse) ## 构建模型 library(mgcv) ## 作图 library(itsadug) 12.2 数据导入及整理 contour_data_all = read.csv(&quot;data/ch12/nv.sv.data.2024-08-24.csv&quot;) # modelling NV difference nv.data = contour_data_all%&gt;% ungroup()%&gt;% filter(tone %in% c(&quot;NV415&quot;, &quot;NV214&quot;) )%&gt;% #filter(subject != 214)%&gt;% rename(Time = Timepoint)%&gt;% select(-language)%&gt;% mutate(id = as.factor(id), tone = as.factor(tone), subject = as.factor(subject)) #sort data per individual trajectory (for autocorrelation) nv.data = data.frame(nv.data) nv.data$id = as.factor(nv.data$id) nv.data = start_event(nv.data, event = c(&quot;subject&quot;, &quot;id&quot;)) 12.3 数据建模 12.3.1 越南语北部方言声调建模 # preliminary model without autocorrelation correction model.nv = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), data= nv.data) ## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated 1-d smooths of same variable. summary(model.nv) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.11948 0.01606 -7.441 0.000000000000187 *** ## toneNV415 0.21877 0.02108 10.378 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time):toneNV214 2.90 3.290 7.629 0.0000313 *** ## s(Time):toneNV415 4.84 5.425 10.217 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV214 17.74 35.000 2.491 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV415 21.47 35.000 5.316 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.676 Deviance explained = 68.8% ## fREML = -878.66 Scale est. = 0.013402 n = 1280 nvacf = acf_resid(model.nv) #获取参数 nvacf[2] ## 1 ## 0.7104244 ## with autocorrelation correction model.nv.b = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = nvacf[2], AR.start = nv.data$start.event, discrete = T, nthreads = 2, data= nv.data) ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : openMP not available: single ## threaded computation only ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : model has repeated 1-d smooths of ## same variable. summary(model.nv.b) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.12704 0.01503 -8.455 &lt;0.0000000000000002 *** ## toneNV415 0.23482 0.01943 12.083 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time):toneNV214 3.796 4.208 6.028 0.0000918 *** ## s(Time):toneNV415 5.725 6.114 9.966 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV214 25.133 35.000 3.414 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV415 25.402 35.000 6.310 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.672 Deviance explained = 68.8% ## fREML = -1436.5 Scale est. = 0.01023 n = 1280 # plot plot_smooth(model.nv.b, ylab = &quot;Normalised f0&quot;, xlab = &quot;Normalised time&quot;, view=&quot;Time&quot;, cond=list(tone=c(&quot;NV415&quot;)), ylim=c(-0.5,1), rm.ranef=T, main=&quot;Northen Vietnamese&quot;, rug=FALSE, col=&quot;blue&quot;, hide.label = &quot;fitted values&quot;) ## Summary: ## * tone : factor; set to the value(s): NV415. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 211. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneNV214,s(Time,subject):toneNV415 ## plot_smooth(model.nv.b, view=&quot;Time&quot;, cond=list(tone=c(&quot;NV214&quot;)), v0 = 6, rug=FALSE,rm.ranef=T, col=&quot;green&quot;,add=T) ## Summary: ## * tone : factor; set to the value(s): NV214. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 211. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneNV214,s(Time,subject):toneNV415 ## 验证越南语北部方言是否存在这个声调对立 模型比较法 模型1区分两个对立 model.nv = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), data= nv.data) ## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated 1-d smooths of same variable. #模型2不区分两个对立 model.nv0 = bam(normF0 ~ tone + s(Time)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), data= nv.data) #比较两个模型 compareML(model.nv, model.nv0) ## model.nv: normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## model.nv0: normF0 ~ tone + s(Time) + s(Time, subject, by = tone, bs = &quot;fs&quot;, ## m = 1) ## ## Chi-square test of fREML scores ## ----- ## Model Score Edf Difference Df p.value Sig. ## 1 model.nv0 -868.0538 8 ## 2 model.nv -878.6559 10 10.602 2.000 2.486e-05 *** ## ## AIC difference: -4.63, model model.nv has lower AIC. #此方法缺点是计算量比较大。但是对于不是很复杂的模型没有影响。 差值比较法 通过修改模型的设定，将表示两个原始平滑函数之间差异的函数包含在模型中。接下来，发现这个差异平滑函数是显著的，这就表明有显著差异。为了拟合这个新模型，我们首先需要创建一个新的二元变量，该变量在一个水平上等于0，而在另一个水平上等于1。我们现在创建一个名为IS214的变量，该变量在单词‘NV214’上为1，而在单词‘NV415’上为0。 nv.data$IS214 = (nv.data$tone == &quot;NV214&quot;)*1 model.nv.binary = bam(normF0 ~ s(Time) + s(Time, by=IS214)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = nvacf[2], AR.start = nv.data$start.event, discrete = T, nthreads = 2, data= nv.data) ## Warning in bam(normF0 ~ s(Time) + s(Time, by = IS214) + s(Time, subject, : openMP not available: single threaded ## computation only summary(model.nv.binary) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ s(Time) + s(Time, by = IS214) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09447 0.01406 6.718 0.0000000000283 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time) 5.770 6.150 8.406 &lt;0.0000000000000002 *** ## s(Time):IS214 4.732 5.074 31.089 &lt;0.0000000000000002 *** ## s(Time,subject):toneNV214 23.543 35.000 3.181 &lt;0.0000000000000002 *** ## s(Time,subject):toneNV415 27.465 35.000 6.699 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.672 Deviance explained = 68.8% ## fREML = -1434.9 Scale est. = 0.01023 n = 1280 soluation 3 nv.data$toneO = as.ordered(nv.data$tone) contrasts(nv.data$toneO) = &quot;contr.treatment&quot; model.nv.ord = bam(normF0 ~ toneO+ s(Time) + s(Time, by=toneO)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = nvacf[2], AR.start = nv.data$start.event, discrete = T, nthreads = 2, data= nv.data) ## Warning in bam(normF0 ~ toneO + s(Time) + s(Time, by = toneO) + s(Time, : openMP not available: single threaded ## computation only summary(model.nv.ord ) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ toneO + s(Time) + s(Time, by = toneO) + s(Time, subject, ## by = tone, bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.11903 0.01444 -8.241 0.000000000000000437 *** ## toneONV415 0.21311 0.02010 10.605 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time) 4.385 4.798 5.805 0.0000695 *** ## s(Time):toneONV415 4.855 5.224 9.888 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV214 24.989 35.000 3.374 &lt; 0.0000000000000002 *** ## s(Time,subject):toneNV415 25.920 35.000 6.324 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.673 Deviance explained = 68.8% ## fREML = -1436.8 Scale est. = 0.010231 n = 1280 12.3.2 越南语南部方言声调建模 sv.data = contour_data_all%&gt;% ungroup()%&gt;% filter(tone %in% c(&quot;SV415&quot;, &quot;SV214&quot;) )%&gt;% rename(Time = Timepoint)%&gt;% select(-language)%&gt;% mutate(id = as.factor(id), tone = as.factor(tone), subject = as.factor(subject)) #sort data per individual trajectory (for autocorrelation) sv.data = start_event(sv.data, event = c(&quot;subject&quot;, &quot;id&quot;)) # preliminary model without autocorrelation correction model.sv = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), data= sv.data) ## Warning in gam.side(sm, X, tol = .Machine$double.eps^0.5): model has repeated 1-d smooths of same variable. summary(model.sv) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.041462 0.003706 -11.189 &lt;0.0000000000000002 *** ## toneSV415 0.022919 0.010785 2.125 0.0338 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time):toneSV214 5.503 6.116 25.066 &lt;0.0000000000000002 *** ## s(Time):toneSV415 5.433 6.061 25.096 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV214 19.606 35.000 5.389 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV415 21.771 35.000 5.725 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.81 Deviance explained = 81.8% ## fREML = -1135.6 Scale est. = 0.0087883 n = 1280 svacf = acf_resid(model.sv) svacf[2] ## 1 ## 0.5071284 ## with autocorrelation correction model.sv.b = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = svacf[2], AR.start = sv.data$start.event, discrete = T, nthreads = 2, data= sv.data) ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : openMP not available: single ## threaded computation only ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : model has repeated 1-d smooths of ## same variable. summary(model.sv.b) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.042435 0.005763 -7.363 0.000000000000329 *** ## toneSV415 0.024513 0.010851 2.259 0.0241 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time):toneSV214 5.965 6.442 24.104 &lt;0.0000000000000002 *** ## s(Time):toneSV415 5.887 6.383 23.590 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV214 22.274 35.000 4.895 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV415 23.859 35.000 4.822 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.809 Deviance explained = 81.8% ## fREML = -1371.9 Scale est. = 0.0078498 n = 1280 # plot plot_smooth(model.sv.b, ylab = &quot;Normalised f0&quot;, xlab = &quot;Normalised time&quot;, view=&quot;Time&quot;, cond=list(tone=c(&quot;SV415&quot;)), ylim=c(-0.5,1), rm.ranef=T, main=&quot;Southen Vietnamese&quot;, rug=FALSE, col=&quot;blue&quot;, hide.label = &quot;fitted values&quot;) ## Summary: ## * tone : factor; set to the value(s): SV415. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 411. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneSV214,s(Time,subject):toneSV415 ## plot_smooth(model.sv.b, view=&quot;Time&quot;, cond=list(tone=c(&quot;SV214&quot;)), v0 = 6, rug=FALSE,rm.ranef=T, col=&quot;green&quot;,add=T) ## Summary: ## * tone : factor; set to the value(s): SV214. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 411. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneSV214,s(Time,subject):toneSV415 ## # testing if the tone contrast exists #solution 1 model.sv.b = bam(normF0 ~ tone + s(Time, by=tone)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = svacf[2], AR.start = sv.data$start.event, discrete = T, nthreads = 2, data= sv.data) ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : openMP not available: single ## threaded computation only ## Warning in bam(normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, : model has repeated 1-d smooths of ## same variable. model.sv.b0 = bam(normF0 ~ tone + s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = svacf[2], AR.start = sv.data$start.event, discrete = T, nthreads = 2, data= sv.data) ## Warning in bam(normF0 ~ tone + s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), : openMP not available: single ## threaded computation only compareML(model.sv, model.sv.b0) ## model.sv: normF0 ~ tone + s(Time, by = tone) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## model.sv.b0: normF0 ~ tone + s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1) ## ## Model model.sv.b0 preferred: lower fREML score (197.409), and lower df (4.000). ## ----- ## Model Score Edf Difference Df ## 1 model.sv -1135.594 10 ## 2 model.sv.b0 -1333.003 6 -197.409 4.000 ## ## AIC difference: 478.04, model model.sv.b0 has lower AIC. # solution 2 sv.data$IS214 = (sv.data$tone == &quot;SV214&quot;)*1 model.sv.binary = bam(normF0 ~ s(Time) + s(Time, by=IS214)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = svacf[2], AR.start = sv.data$start.event, discrete = T, nthreads = 2, data= sv.data) ## Warning in bam(normF0 ~ s(Time) + s(Time, by = IS214) + s(Time, subject, : openMP not available: single threaded ## computation only summary(model.sv.binary) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ s(Time) + s(Time, by = IS214) + s(Time, subject, by = tone, ## bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02383 0.00942 -2.53 0.0115 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time) 6.956 7.387 42.964 &lt;0.0000000000000002 *** ## s(Time):IS214 2.000 2.000 2.206 0.111 ## s(Time,subject):toneSV214 23.485 35.000 4.868 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV415 24.879 35.000 4.749 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.809 Deviance explained = 81.8% ## fREML = -1378 Scale est. = 0.0078415 n = 1280 # soluation 3 sv.data$toneO = as.ordered(sv.data$tone) contrasts(sv.data$toneO) = &quot;contr.treatment&quot; model.sv.ord = bam(normF0 ~ toneO+ s(Time) + s(Time, by=toneO)+ s(Time, subject, by = tone, bs = &quot;fs&quot;, m = 1), rho = svacf[2], AR.start = sv.data$start.event, discrete = T, nthreads = 2, data= sv.data) ## Warning in bam(normF0 ~ toneO + s(Time) + s(Time, by = toneO) + s(Time, : openMP not available: single threaded ## computation only summary(model.sv.ord ) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## normF0 ~ toneO + s(Time) + s(Time, by = toneO) + s(Time, subject, ## by = tone, bs = &quot;fs&quot;, m = 1) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.046695 0.005703 -8.188 0.000000000000000665 *** ## toneOSV415 0.022863 0.011004 2.078 0.0379 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time) 6.956 7.387 42.114 &lt;0.0000000000000002 *** ## s(Time):toneOSV415 1.000 1.000 0.095 0.758 ## s(Time,subject):toneSV214 23.485 35.000 4.868 &lt;0.0000000000000002 *** ## s(Time,subject):toneSV415 24.879 35.000 4.749 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.809 Deviance explained = 81.8% ## fREML = -1378 Scale est. = 0.0078415 n = 1280 12.4 展示与呈现 # plot the comparison library(cowplot) # png(paste(dir.fig, paste(&quot;gam.mdl&quot;,Sys.Date(),&quot;png&quot;,sep = &quot;.&quot;),sep = &quot;&quot;), units=&quot;in&quot;, width=14, height=10, res=600) par(mfrow=c(1,2)) # Northen Vietnamese plot_smooth(model.nv.b, ylab = &quot;Normalised f0&quot;, xlab = &quot;Normalised time&quot;, view=&quot;Time&quot;, cond=list(tone=c(&quot;NV415&quot;)), ylim=c(-0.5,0.6), rm.ranef=T, main=&quot;Northern Vietnamese&quot;, rug=FALSE, col=&quot;black&quot;, hide.label = &quot;fitted values&quot;) ## Summary: ## * tone : factor; set to the value(s): NV415. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 211. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneNV214,s(Time,subject):toneNV415 ## plot_smooth(model.nv.b, view=&quot;Time&quot;, cond=list(tone=c(&quot;NV214&quot;)), v0 = 6, rug=FALSE,rm.ranef=T, col=&quot;gray&quot;,add=T) ## Summary: ## * tone : factor; set to the value(s): NV214. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 211. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneNV214,s(Time,subject):toneNV415 ## # Southern vietnamese plot_smooth(model.sv.b, ylab = &quot;Normalised f0&quot;, xlab = &quot;Normalised time&quot;, view=&quot;Time&quot;, cond=list(tone=c(&quot;SV415&quot;)), ylim=c(-0.5,0.6), rm.ranef=T, main=&quot;Southern Vietnamese&quot;, rug=FALSE, col=&quot;black&quot;, hide.label = &quot;fitted values&quot;) ## Summary: ## * tone : factor; set to the value(s): SV415. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 411. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneSV214,s(Time,subject):toneSV415 ## plot_smooth(model.sv.b, view=&quot;Time&quot;, cond=list(tone=c(&quot;SV214&quot;)), v0 = 6, rug=FALSE,rm.ranef=T, col=&quot;gray&quot;,add=T) ## Summary: ## * tone : factor; set to the value(s): SV214. ## * Time : numeric predictor; with 30 values ranging from 1.000000 to 10.000000. ## * subject : factor; set to the value(s): 411. (Might be canceled as random effect, check below.) ## * NOTE : The following random effects columns are canceled: s(Time,subject):toneSV214,s(Time,subject):toneSV415 ## # dev.off() # write.csv(nv.data, # file = paste(&quot;data/ch12/&quot;, # paste(&quot;nv.sv.data&quot;, # Sys.Date(), # &quot;csv&quot;, # sep = &quot;.&quot;), # sep = &quot;&quot;) ,row.names = FALSE) "],["案例四机器学习.html", "Chapter 13 案例四、机器学习 13.1 训练普通话声调模型 13.2 训练泰语母语者声调模型 13.3 模拟普通话听者感知泰语声调 13.4 模拟泰语母语听者感知普通话声调", " Chapter 13 案例四、机器学习 详细背景请看原文献 13.1 训练普通话声调模型 # import data md_discrete = read.csv(&quot;data/ch13/md_discrete.csv&quot;) # Selecting features dataset_md = md_discrete[ , c(3, 4:9)] # scale the data dataset_md[,-1] = scale(dataset_md[-1]) dataset_md$tone = as.factor(dataset_md$tone) # Fitting classifier # model based on discrete features svm_md = svm(formula = tone ~ ., data = dataset_md, type = &#39;C-classification&#39;,#regression or classification kernel = &#39;radial&#39;, cost = &quot;2&quot;, cross = 10) # total accuracy # summary(svm_md) svm_md$tot.accuracy ## [1] 96.67319 # # model based on contour features # svm_contour = svm(formula = tone ~ ., # data = dataset_thai_8point, # type = &#39;C-classification&#39;,#regression or classification # kernel = &#39;radial&#39;, # cost = &quot;2&quot;, # cross = 10) # svm_contour$tot.accuracy # producing confusion matrix prediction_md &lt;- predict(svm_md, dataset_md) md_confusion = data.frame(pred = prediction_md, tone = md_discrete$tone) md_confusion_table = md_confusion%&gt;% group_by(tone,pred)%&gt;% summarize(n = n())%&gt;% group_by(tone)%&gt;% mutate(sum = sum(n), percent = round((n/sum)*100,2))%&gt;% select(tone,pred,percent)%&gt;% spread(tone, percent) ## `summarise()` has grouped output by &#39;tone&#39;. You can override using the `.groups` argument. md_confusion_table ## # A tibble: 4 × 5 ## pred M214 M35 M51 M55 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M214 96.1 NA NA NA ## 2 M35 3.91 100 NA 0.78 ## 3 M51 NA NA 98.4 NA ## 4 M55 NA NA 1.57 99.2 # write.csv(md_confusion_table, # file =&quot;data/processed/md_confusion_table.csv&quot;, # row.names = FALSE) md_plot = md_confusion%&gt;% group_by(pred, tone)%&gt;% summarise(n = n())%&gt;% group_by(tone)%&gt;% mutate(percent = n/sum(n), sum = sum(n))%&gt;% ggplot(aes(pred, tone))+ geom_tile(aes(fill = percent*100 ))+ geom_text(aes(label = round(percent*100, 1)), size = 4) + scale_x_discrete(name = &quot;Predictions&quot;, limits=c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;))+ scale_y_discrete(name = &quot;Mandarin tones&quot;, limits=c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;blue&quot;) + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank()) ## `summarise()` has grouped output by &#39;pred&#39;. You can override using the `.groups` argument. md_plot # Print this our for publication # png(&quot;figure4.png&quot;, units=&quot;in&quot;, width=5, height=4, res=600) # md_plot # dev.off() 13.2 训练泰语母语者声调模型 # import data thai_discrete = read.csv(&quot;data/ch13/thai_discrete.csv&quot;) # Selecting features dataset_thai = thai_discrete[ , c(2, 6:11)] # scale the data dataset_thai[,-1] = scale(dataset_thai[-1]) # Fitting classifier # model based on discrete features svm_thai = svm(formula = tone ~ ., data = dataset_thai, type = &#39;C-classification&#39;,#regression or classification kernel = &#39;radial&#39;, cost = &quot;2&quot;, cross = 10) # total accuracy # summary(svm_thai) svm_thai$tot.accuracy ## [1] 91.62791 检测泰语母语者声调模型的正确率。 # confusion matrix prediction &lt;- predict(svm_thai, dataset_thai) thai_confusion = data.frame(pred = prediction, tone = thai_discrete$tone) thai_confusion_table = thai_confusion%&gt;% group_by(tone,pred)%&gt;% summarize(n = n())%&gt;% group_by(tone)%&gt;% mutate(sum = sum(n), percent = round((n/sum)*100,2))%&gt;% select(tone,pred,percent)%&gt;% spread(tone, percent) ## `summarise()` has grouped output by &#39;tone&#39;. You can override using the `.groups` argument. thai_confusion$tone = as.factor(thai_confusion$tone) # write.csv(thai_confusion_table, # file = &quot;data/processed/thai_confusion_table &quot;, # row.names = FALSE) thai_plot = thai_confusion%&gt;% group_by(pred, tone)%&gt;% summarise(n = n())%&gt;% group_by(tone)%&gt;% mutate(percent = n/sum(n), sum = sum(n))%&gt;% ggplot(aes(pred, tone))+ geom_tile(aes(fill = percent*100 ))+ geom_text(aes(label = round(percent*100, 1)), size = 4)+ scale_x_discrete(name = &quot;Predictions&quot;, limits=c(&quot;21&quot;,&quot;33&quot;,&quot;45&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T21&quot;, &quot;T33&quot;, &quot;T45&quot;,&quot;T315&quot;,&quot;T241&quot;))+ scale_y_discrete(name = &quot;Thai tones&quot;, limits=c(&quot;21&quot;,&quot;33&quot;,&quot;45&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T21&quot;, &quot;T33&quot;, &quot;T45&quot;,&quot;T315&quot;,&quot;T241&quot;))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;red&quot;) + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank()) ## `summarise()` has grouped output by &#39;pred&#39;. You can override using the `.groups` argument. thai_plot # Print this our for publication # png(&quot;thai_tones.png&quot;, units=&quot;in&quot;, width=5, height=4, res=600) # thai_plot # dev.off() 13.3 模拟普通话听者感知泰语声调 # mandarin listeners percieve Thai tones prediction_md_thai &lt;- predict(svm_md, dataset_thai) table( prediction_md_thai, thai_discrete$tone) ## ## prediction_md_thai 21 33 45 241 315 ## M214 65 26 0 0 21 ## M35 0 3 55 1 61 ## M51 20 22 7 79 4 ## M55 1 35 24 6 0 # SVM assimilation table md.svm.assim = data.frame(pred = prediction_md_thai, tone = thai_discrete$tone)%&gt;% group_by(pred, tone)%&gt;% summarise(n = n())%&gt;% group_by(tone)%&gt;% mutate(percent = round((n/sum(n)),3)*100, sum = sum(n)) ## `summarise()` has grouped output by &#39;pred&#39;. You can override using the `.groups` argument. md.svm.assim$pred = factor(md.svm.assim$pred, c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;)) md.svm.assim$tone = factor(md.svm.assim$tone,c(&quot;45&quot;,&quot;33&quot;,&quot;21&quot;,&quot;315&quot;,&quot;241&quot;)) # Stacked barplot md.svm.assim.stacked = ggplot(md.svm.assim, aes(fill=pred, y=percent, x=tone, label = percent)) + geom_bar( stat=&quot;identity&quot;)+ scale_fill_manual( values=c(&quot;M55&quot; = &quot;black&quot;, &quot;M35&quot;=&quot;gray25&quot;, &quot;M214&quot;=&quot;gray75&quot;, &quot;M51&quot;=&quot;white&quot;), name=&quot;Mandarin&quot;, breaks=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;))+ geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;)+ xlab(&quot;Thai tones&quot;)+ ylab(&quot;Mandarin responses(%)&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, 101))+ theme_classic()+ theme(legend.title = element_text(size=12, face=&quot;bold&quot;))+ theme(legend.text = element_text(size = 12, face = &quot;bold&quot;))+ theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10))+ scale_x_discrete(breaks = c(&quot;45&quot;, &quot;33&quot;, &quot;21&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T45&quot;, &quot;T33&quot;, &quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;))+ geom_text(size = 4, position = position_stack(vjust = 0.5),color=&quot;blue&quot;) md.svm.assim.stacked # png(&quot;figure/md.svm.assim.stacked .png&quot;, # units=&quot;in&quot;, width=5, height=4, res=600) # md.svm.assim.stacked # dev.off() # heat map md.svm.assim.heat = md.svm.assim%&gt;% filter(percent&gt;10)%&gt;% ggplot(aes(tone, pred))+ geom_tile(aes(fill = percent ))+ geom_text(aes(label = percent)) + scale_x_discrete(name = &quot;Thai stimuli&quot;, limits=c(&quot;45&quot;,&quot;33&quot;,&quot;21&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T45&quot;,&quot;T33&quot;,&quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;))+ scale_y_discrete(name = &quot;Predicted responses&quot;, limits=c(&quot;M51&quot;,&quot;M214&quot;,&quot;M35&quot;,&quot;M55&quot;), labels=c(&quot;M51&quot;,&quot;M214&quot;,&quot;M35&quot;,&quot;M55&quot;))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;gray&quot;) + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank())+ theme(legend.position=&quot;none&quot;) md.svm.assim.heat final.cm = read.csv(file = &quot;data/ch13/final.cm.csv&quot;) final.cm$stimuli = as.factor(final.cm$stimuli) md.human.assim.heat = final.cm%&gt;% group_by(stimuli, response)%&gt;% filter(subject !=&quot;106&quot;)%&gt;% summarise(cat.mean = round ((sum(percentage)/12)*100, 1))%&gt;% filter(cat.mean &gt;10)%&gt;% ggplot(aes(stimuli, response))+ geom_tile(aes(fill = cat.mean ))+ geom_text(aes(label = cat.mean)) + scale_x_discrete(name = &quot;Thai stimuli&quot;, limits=c(&quot;45&quot;,&quot;33&quot;,&quot;21&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T45&quot;,&quot;T33&quot;,&quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;))+ scale_y_discrete(name = &quot;Mandarin listeners&#39; responses&quot;, limits=c(&quot;M51&quot;,&quot;M214&quot;,&quot;M35&quot;,&quot;M55&quot;), labels=c(&quot;M51&quot;,&quot;M214&quot;,&quot;M35&quot;,&quot;M55&quot;))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;gray&quot;) + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank()) ## `summarise()` has grouped output by &#39;stimuli&#39;. You can override using the `.groups` argument. md.human.assim.heat md_compare = plot_grid(md.svm.assim.heat, md.human.assim.heat, nrow=1, labels=c(&#39;A&#39;, &#39;B&#39;), rel_widths = c(1, 1.4) ) #Or labels=&quot;AUTO&quot; md_compare # png(&quot;md_compare.png&quot;, units=&quot;in&quot;, width=8, height=3, res=600) # md_compare # dev.off() 13.4 模拟泰语母语听者感知普通话声调 # thai listeners percieve mandarin tones prediction_thai_md &lt;- predict(svm_thai, dataset_md) table(prediction_thai_md, md_discrete$tone) ## ## prediction_thai_md M214 M35 M51 M55 ## 21 82 0 15 0 ## 33 6 2 7 41 ## 45 0 49 8 55 ## 241 5 4 91 32 ## 315 35 73 6 0 # set the order of thai tones thai.svm.assim = data.frame(pred = prediction_thai_md, tone = md_discrete$tone)%&gt;% group_by(pred, tone)%&gt;% summarise(n = n())%&gt;% group_by(tone)%&gt;% mutate(percent = round((n/sum(n)),3)*100, sum = sum(n)) ## `summarise()` has grouped output by &#39;pred&#39;. You can override using the `.groups` argument. thai.svm.assim$tone = factor(thai.svm.assim$tone, c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;)) thai.svm.assim$pred = factor(thai.svm.assim$pred, c(&quot;45&quot;,&quot;33&quot;,&quot;21&quot;,&quot;315&quot;,&quot;241&quot;)) thai.svm.assim.stacked = ggplot(thai.svm.assim, aes(fill=pred, y=percent, x=tone, label = percent)) + geom_bar( stat=&quot;identity&quot;)+ scale_fill_manual( values=c(&quot;45&quot; = &quot;black&quot;, &quot;33&quot;=&quot;gray25&quot;, &quot;21&quot;=&quot;gray50&quot;, &quot;315&quot;=&quot;gray75&quot;, &quot;241&quot;=&quot;white&quot;), name=&quot;Thai&quot;, breaks=c(&quot;45&quot;, &quot;33&quot;, &quot;21&quot;,&quot;315&quot;,&quot;241&quot;), labels=c(&quot;T45&quot;, &quot;T33&quot;, &quot;T21&quot;,&quot;T315&quot;,&quot;T241&quot;))+ geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;)+ xlab(&quot;Mandarin tones&quot;)+ ylab(&quot;Thai responses(%)&quot;) + scale_y_continuous(expand = c(0, 0), limits = c(0, 100))+ theme_classic()+ theme(legend.title = element_text(size=12, face=&quot;bold&quot;))+ theme(legend.text = element_text(size = 12, face = &quot;bold&quot;))+ theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10))+ scale_x_discrete(breaks = c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;))+ geom_text(size = 4, position = position_stack(vjust = 0.5),color=&quot;blue&quot;) # Print this our for publication # png(&quot;figure3.png&quot;, units=&quot;in&quot;, width=5, height=4, res=600) # figure3 # dev.off() # svm predictions thai.svm.assim.heat = thai.svm.assim%&gt;% filter(percent&gt;10)%&gt;% ggplot(aes(tone, pred))+ geom_tile(aes(fill = percent ))+ geom_text(aes(label = percent))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;gray&quot;)+ theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank())+ scale_x_discrete(name = &quot;Mandarin stimuli&quot;, breaks = c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;))+ scale_y_discrete(name = &quot;Predicted responses&quot;, limits=c(&quot;241&quot;,&quot;315&quot;,&quot;21&quot;,&quot;33&quot;,&quot;45&quot;), labels=c(&quot;T241&quot;, &quot;T315&quot;, &quot;T21&quot;,&quot;T33&quot;,&quot;T45&quot;))+ theme(legend.position=&quot;none&quot;) thai.svm.assim.heat # human listener predictions thai_assm &lt;- read_csv(&quot;data/ch13/thai_assm.csv&quot;) ## Rows: 20 Columns: 3 ## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): tone ## dbl (2): pred, percent ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. thai_assm$pred = as.character(thai_assm$pred) thai_assm$tone = factor(thai_assm$tone, c(&quot;M55&quot;,&quot;M35&quot;,&quot;M214&quot;,&quot;M51&quot;)) thai_assm$pred = factor(thai_assm$pred,c(&quot;45&quot;,&quot;33&quot;,&quot;21&quot;,&quot;315&quot;,&quot;241&quot;)) thai.human.assim.heat = thai_assm%&gt;% filter(percent&gt;10)%&gt;% ggplot(aes(tone, pred))+ geom_tile(aes(fill = percent ))+ geom_text(aes(label = percent))+ scale_fill_gradient(name=&quot;Percentage (%)&quot;, low = &quot;white&quot;, high = &quot;gray&quot;)+ theme(axis.text.x = element_text(face=&quot;bold&quot;, size=10), axis.text.y = element_text(face=&quot;bold&quot;, size=10), panel.background = element_blank())+ scale_x_discrete(name = &quot;Mandarin stimuli&quot;, breaks = c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;), labels=c(&quot;M55&quot;, &quot;M35&quot;, &quot;M214&quot;,&quot;M51&quot;))+ scale_y_discrete(name = &quot;Thai listners&#39; responses&quot;, limits=c(&quot;241&quot;,&quot;315&quot;,&quot;21&quot;,&quot;33&quot;,&quot;45&quot;), labels=c(&quot;T241&quot;, &quot;T315&quot;, &quot;T21&quot;,&quot;T33&quot;,&quot;T45&quot;)) thai_compare = plot_grid(thai.svm.assim.heat , thai.human.assim.heat, nrow=1, labels=c(&#39;A&#39;, &#39;B&#39;), rel_widths = c(1, 1.3)) thai_compare png(&quot;thai_compare.png&quot;, units=&quot;in&quot;, width=8, height=3, res=600) thai_compare dev.off() ## quartz_off_screen ## 2 "],["文献计量研究案例-语音感知.html", "Chapter 14 文献计量研究案例-语音感知 14.1 数据导入及整理 14.2 数据可视化与展示", " Chapter 14 文献计量研究案例-语音感知 详细背景请看原文献 首先，我们使用Web of Science核心合集进行了主题为“speech perception”（带引号）的搜索，搜索对象为社会科学引文索引（SSCI）、科学引文索引扩展版（SCIE）、新兴资源引文索引（ESCI）中发表的研究文章（文档类型=文章），搜索日期为2021年2月24日。时间跨度设定为2000年至2020年，非英语论文被排除。最初找到了9436篇研究文章。鉴于本次文献计量分析的重点是从语音学/语言学、心理学、神经科学和言语病理学的角度研究言语感知，因此排除了属于Web of Science分类中的耳鼻喉科的纯医学研究（n = 2731）。 这些文章的完整记录和被引用参考文献被下载并通过R语言中的bibliometrix包（Aria 和 Cuccurullo, 2017）进行处理。我们检查了数据集，并移除了数据缺失的文章，这进一步将研究文章的数量减少到6407篇（见图1）。bibliometrix包是用R语言开发的，提供了多种用于综合文献计量分析的功能，并且可以无缝集成其他R包，以实现更高级的数据建模和可视化。我们选择bibliometrix包而非其他软件，是因为它提供了一个更加开放、灵活、可定制且可重复的工作流程。 14.1 数据导入及整理 # read files filenames = list.files(path = &quot;data/ch14/data-24-02-2021&quot;, pattern = &quot;txt&quot;, full.names = TRUE) # converting to bibliometric data frame speech.data = convert2df(filenames, dbsource = &quot;wos&quot;, format = &quot;plaintext&quot;) # use only research articles speech.data = speech.data%&gt;% filter(PY != 2021 &amp; !is.na(PY))%&gt;% filter(DT == &quot;ARTICLE&quot;) speech.df = select(speech.data, &quot;AU&quot; , &quot;AF&quot; , &quot;CR&quot; , &quot;AB&quot; )%&gt;% mutate(AB = tolower(AB)) 14.2 数据可视化与展示 14.2.1 语音感知研究概况(2000-2020). 14.2.2 语音感知研究发表最多的杂志 14.2.3 研究能产性分析 14.2.4 语音感知研究产出量 (2000-2020). 14.2.5 语音感知研究数量最高的10个国家（地区） (2000-2020). SCP = 单一国家发表; MCP = 多个国家合作发表 14.2.6 发表量最大的10个国家发表趋势 14.2.7 语音感知领域发表量最高20位学者(2000-2020). 14.2.8 语音感知领域发表量最高20位学者每年发表和引用趋势(2000-2020). . TC = 所有发表. 圆圈大小代表发表数量；颜色深度表示引用数数量。 14.2.9 合作情况 语音感知研究中最活跃的20所大学/研究机构及其合作网络。网络中的每个节点代表一个不同的大学/研究机构，节点的直径与该机构与其他机构的合作强度成正比。节点之间的连线表示这些大学或研究机构之间的合作路径。 语音感知研究中前20个国家的合作网络。 网络中的每个节点代表一个不同的国家，节点的直径与该国家与其他国家合作的强度成正比。节点之间的连线表示国家之间的合作路径。 14.2.10 研究影响力 最高被引研究 语音感知高被引作者 语音感知研究的共引网络。节点代表研究文章。每个节点的标签颜色与其所在的集群相同，节点的大小与其共引度成正比。 语音感知研究的文献耦合网络，其中节点代表研究文章。每个节点的标签颜色与其所在的集群相同，节点的大小与其文献耦合度成正比。 前20个作者定义关键词、机器生成关键词、基于标题和摘要的两词短语（Freq = 频率）注意：在论文表格中，一些术语进行了合并处理，例如在标题两词短语列中，“Cochlear implant”（人工耳蜗）和“cochlear implantation”（人工耳蜗植入）被合并。 作者自定义词网络 "],["数据可视化-3.html", "Chapter 15 数据可视化 15.1 条形图（Bar plots） 15.2 瓦片图（tile plot） 15.3 线图 15.4 散点图 15.5 脑区图 15.6 Venn图 15.7 diagram 15.8 Gant plot 15.9 图标Lengends 15.10 坐标轴 15.11 配色 15.12 注释标记 15.13 theme", " Chapter 15 数据可视化 #用于数据 library(janeaustenr) #用于分词 library(tidytext) library(dplyr) library(stringr) austen.word &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% unnest_tokens(word, text)%&gt;% count(word, sort = TRUE) %&gt;% mutate(book = as.factor(book))%&gt;% #筛选Emma这个小说 #filter(book == &quot;Emma&quot;)%&gt;% #显示前20个高频词 head(60) # install.packages(&quot;ggpubr&quot;) # # library(ggpubr) library(tidyverse) library(languageR) data(&quot;lexdec&quot;) austen.word %&gt;% #给高频词排序 mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) # # sorted first by book, then by count # nameorder &lt;- austen.word$word[order(austen.word$book, # austen.word$n)] # # # Turn name into a factor, with levels in the order of nameorder # austen.word$word &lt;- factor(austen.word$word, levels = nameorder) 15.1 条形图（Bar plots） 15.1.1 基于频数 ggplot(austen.word, aes(x = reorder(word, n), y = n, fill = book)) + geom_col(colour = &quot;black&quot;) + #scale_fill_manual(values = c(&quot;#669933&quot;, &quot;#FFCC66&quot;)) + xlab(&quot;Words&quot;) ggplot(austen.word, aes(x = reorder(word, n), y = n, fill = book)) + geom_col(colour = &quot;black&quot;) + #scale_fill_manual(values = c(&quot;#669933&quot;, &quot;#FFCC66&quot;)) + xlab(&quot;Words&quot;)+ facet_grid(book ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) ggplot(austen.word, aes(y = reorder(word, n), x = n, fill = book)) + geom_col(colour = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Pastel1&quot;)+ xlab(&quot;Words&quot;)+ facet_grid(book ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) # Cleveland Dot Plot ggplot(austen.word, aes(x = n, y = reorder(word, n))) + geom_segment(aes(yend = word), xend = 0, colour = &quot;grey50&quot;) + geom_point(size = 3, aes(colour = book)) + scale_colour_brewer(palette = &quot;Set1&quot;, limits = unique(c(austen.word$book))) + theme_bw() + theme( panel.grid.major.y = element_blank(), # No horizontal grid lines legend.position = c(1, 0.55), # Put legend inside plot area legend.justification = c(1, 0.5) ) ggplot(austen.word, aes(x = n, y = reorder(word, n))) + geom_segment(aes(yend = word), xend = 0, colour = &quot;grey50&quot;) + geom_point(size = 3, aes(colour = book)) + scale_colour_brewer(palette = &quot;Set1&quot;, limits = unique(c(austen.word$book)), guide = FALSE) + theme_bw() + theme(panel.grid.major.y = element_blank()) + facet_grid(book ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) 15.1.2 基于平均值 #bar plot lexdec.plt = lexdec%&gt;% group_by(Class, Correct)%&gt;% summarise(mean = mean(RT))%&gt;% ggplot(aes(Class, mean, fill = Correct))+ geom_col(position = &quot;dodge&quot;, colour = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Pastel1&quot;) ## `summarise()` has grouped output by &#39;Class&#39;. You can override using the `.groups` argument. lexdec.plt 15.1.3 调整柱的宽度和之间的距离 lexdec%&gt;% group_by(Class, Correct)%&gt;% summarise(mean = mean(RT))%&gt;% ggplot(aes(Class, mean, fill = Correct))+ # 去掉下面这行 #geom_col(position = &quot;dodge&quot;, colour = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Pastel1&quot;)+ # 调整柱的宽度和之间的距离 geom_col(width = 0.3, position = position_dodge(0.7)) ## `summarise()` has grouped output by &#39;Class&#39;. You can override using the `.groups` argument. 15.1.4 添加柱子的数值 lexdec.plt+ geom_text( aes(label = round(mean,1)), colour = &quot;black&quot;, size = 3, vjust = 1.5, position = position_dodge(.9) ) lexdec.plt+ geom_text( aes(label = paste(round(mean,1), &quot;ms&quot;)), colour = &quot;black&quot;, size = 3, vjust = 1.5, position = position_dodge(.9) ) 15.2 瓦片图（tile plot） 对于两个分类变量，我们可以使用交叉表（contingency table）来描述两个分类变量之间的协变性，显示各自的频率。 table(lexdec$NativeLanguage,lexdec$Sex) ## ## F M ## English 553 395 ## Other 553 158 lexdec%&gt;% count(Class, Correct)%&gt;% ggplot(aes(Class, Correct))+ geom_tile(aes(fill=n)) ggplot(lexdec)+ geom_count(aes(Class, Correct)) # You may need to install first, with install.packages(&quot;vcd&quot;) library(vcd) df.lexdec = lexdec%&gt;% count(Class, Correct, Complex) # Split by Admit, then Gender, then Dept mosaic( ~ Class+ Correct + Complex, data = df.lexdec) mosaic( ~ Class+ Correct + Complex, data = df.lexdec, highlighting = &quot;Class&quot;, highlighting_fill = c(&quot;lightblue&quot;, &quot;pink&quot;), direction = c(&quot;v&quot;,&quot;h&quot;,&quot;v&quot;)) 15.3 线图 lexdec.line.plt = lexdec%&gt;% group_by(Class, Correct)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% ggplot(aes(Class, mean, # 一定要有group把数据连起来 group = Correct, color = Correct, linetype = Correct, fill = Correct))+ #geom_point() + geom_line() + scale_colour_brewer(palette = &quot;Set1&quot;) ## `summarise()` has grouped output by &#39;Class&#39;. You can override using the `.groups` argument. lexdec.line.plt lexdec.line.plt + geom_point(size = 4) # Make the points a little larger lexdec.line.plt + geom_point(size = 4, shape = 21) # Also use a point with a color fill lexdec%&gt;% group_by(Class, Correct)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% ggplot(aes(Class, mean, # 一定要有group把数据连起来 group = Correct, color = Correct, linetype = Correct))+ scale_colour_brewer(palette = &quot;Set1&quot;)+ geom_line(position = position_dodge(0.2)) + # Dodge lines by 0.2 geom_point(position = position_dodge(0.2), size = 4)# Dodge points by 0.2 ## `summarise()` has grouped output by &#39;Class&#39;. You can override using the `.groups` argument. 图例编号 lexdec.line.plt + geom_point(shape = 22, size = 3, fill = &quot;white&quot;) lexdec.line.plt + geom_point(shape = 22, size = 3)+ scale_fill_manual(values = c(&quot;black&quot;,&quot;white&quot;)) # Create a simulated dataset set.seed(123) # For reproducibility years &lt;- 1994:2023 population_estimate &lt;- 100 + (years - 1994) * 2 + rnorm(length(years), 0, 10) lower_ci &lt;- population_estimate - 1.96 * 10 upper_ci &lt;- population_estimate + 1.96 * 10 old.cn &lt;- data.frame( Year = years, PopulationEstimate = population_estimate, LowerCI = lower_ci, UpperCI = upper_ci ) # Plotting with ggplot2 ggplot(old.cn , aes(x = Year, y = PopulationEstimate)) + geom_line(color = &quot;blue&quot;) + geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI), alpha = 0.2) + labs( title = &quot;China&#39;s Elderly Population with 95% Confidence Interval&quot;, x = &quot;Year&quot;, y = &quot;Population Estimate (in millions)&quot; ) + theme_minimal() # With a dotted line for upper and lower bounds ggplot(old.cn , aes(x = Year, y = PopulationEstimate)) + geom_line(aes(y = LowerCI), colour = &quot;grey50&quot;, linetype = &quot;dotted&quot;) + geom_line(aes(y = UpperCI), colour = &quot;grey50&quot;, linetype = &quot;dotted&quot;) + geom_line() 15.4 散点图 lex.scatter.plt = lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency)) + geom_point() lex.scatter.plt lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency)) + geom_point(shape = 21) lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency, shape = NativeLanguage, colour = NativeLanguage)) + geom_point() lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency, shape = NativeLanguage, colour = NativeLanguage)) + geom_point() + scale_shape_manual(values = c(1,2)) + scale_colour_brewer(palette = &quot;Set1&quot;) lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency, shape = NativeLanguage, colour = FamilySize)) + geom_point() lex.scatter.plt+ geom_point(colour = &quot;grey60&quot;) + stat_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; lex.scatter.plt+ geom_point(colour = &quot;grey60&quot;) + stat_smooth(method = loess) ## `geom_smooth()` using formula = &#39;y ~ x&#39; lexdec%&gt;% ggplot(., aes(x = RT, y = Frequency, colour = NativeLanguage)) + geom_point() + scale_colour_brewer(palette = &quot;Set1&quot;)+ geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 15.5 脑区图 # Enable this universe options(repos = c( ggseg = &#39;https://ggseg.r-universe.dev&#39;, CRAN = &#39;https://cloud.r-project.org&#39;)) setwd(&quot;~/Nutstore Files/115_JournalPaper/7_aphasiaBankLexical&quot;) fig = &quot;/Users/chenjuqiang/Nutstore Files/115_JournalPaper/7_aphasiaBankLexical/data/figure/&quot; # Install some packages #install.packages(&#39;ggsegBrodmann&#39;) library(ggseg) #&gt; Warning: package &#39;ggseg&#39; was built under R version 4.1.1 #&gt; Loading required package: ggplot2 library(ggseg3d) library(ggsegBrodmann) dk ## # dk cortical brain atlas ## regions: 35 ## hemispheres: left, right ## side views: lateral, medial ## palette: yes ## use: ggplot() + geom_brain() ## ---- ## hemi side region label roi ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 left lateral bankssts lh_bankssts 0002 ## 2 left lateral caudal middle frontal lh_caudalmiddlefrontal 0004 ## 3 left lateral fusiform lh_fusiform 0008 ## 4 left lateral inferior parietal lh_inferiorparietal 0009 ## 5 left lateral inferior temporal lh_inferiortemporal 0010 ## 6 left lateral lateral occipital lh_lateraloccipital 0012 ## 7 left lateral lateral orbitofrontal lh_lateralorbitofrontal 0013 ## 8 left lateral middle temporal lh_middletemporal 0016 ## 9 left lateral pars opercularis lh_parsopercularis 0019 ## 10 left lateral pars orbitalis lh_parsorbitalis 0020 ## # ℹ 76 more rows dk.df= as.data.frame(dk) # Figure 1 ggseg() # Figure 3 ggseg(mapping = aes(fill = region), colour=&quot;black&quot;) + scale_fill_brain(&quot;dk&quot;) + theme(legend.justification = c(1,0), legend.position = &quot;bottom&quot;, legend.text = element_text(size = 5)) + guides(fill = guide_legend(ncol = 3)) ggseg(view = &quot;medial&quot;,mapping = aes(fill = region), colour=&quot;black&quot;) + scale_fill_brain(&quot;dk&quot;) + theme(legend.justification = c(1,0), legend.position = &quot;bottom&quot;, legend.text = element_text(size = 5)) + guides(fill = guide_legend(ncol = 3)) ggseg(hemisphere = &quot;left&quot;,mapping = aes(fill = region), colour=&quot;black&quot;) + scale_fill_brain(&quot;dk&quot;) + theme(legend.justification = c(1,0), legend.position = &quot;bottom&quot;, legend.text = element_text(size = 5)) + guides(fill = guide_legend(ncol = 3)) ggseg (atlas = &quot;aseg&quot;, mapping = aes(fill = region)) + theme(legend.justification = c(1, 0), legend.position = &quot;bottom&quot; , legend.text = element_text(size = 5)) + guides(fill = guide_legend(ncol = 3)) plot(brodmann) + theme(legend.position = &quot;bottom&quot;, legend.text = element_text(size = 9)) + guides(fill = guide_legend(ncol = 6)) # png(paste(fig, paste(&quot;bordmann&quot;,Sys.Date(),&quot;png&quot;,sep = &quot;.&quot;), sep = &quot;&quot;), # units=&quot;in&quot;, width=14, height=7, res=600) # # plot(brodmann) + # theme(legend.position = &quot;bottom&quot;, # legend.text = element_text(size = 9)) + # guides(fill = guide_legend(ncol = 6)) # # dev.off() 15.6 Venn图 library(eulerr) library(showtext) # font_install(source_han_serif()) # font_families() showtext_auto() ds_venn &lt;- euler(c(&quot;A&quot; = 10, &quot;B&quot; = 10, &quot;C&quot; = 10, &quot;A&amp;B&quot; = 4, &quot;A&amp;C&quot; = 4, &quot;B&amp;C&quot; = 4, &quot;A&amp;B&amp;C&quot; = 2)) plot(ds_venn, labels = c(&quot;Computer programming&quot;, &quot;Statistics&quot;, &quot;Domain knowledge&quot;, &quot;Machine learning&quot;, &quot;Traditional research&quot;,&quot;Danger zone&quot;, &quot;Data Science&quot;)) # font_files() # font_add(&quot;Songti SC&quot;, &quot;Songti.ttc&quot;) ds_venn_chinese &lt;- euler(c(&quot;A&quot; = 10, &quot;B&quot; = 10, &quot;C&quot; = 10, &quot;A&amp;B&quot; = 4, &quot;A&amp;C&quot; = 4, &quot;B&amp;C&quot; = 4, &quot;A&amp;B&amp;C&quot; = 2)) plot(ds_venn_chinese, labels = c(&quot;计算机编程&quot;, &quot;统计&quot;, &quot;专业知识&quot;, &quot;Machine learning&quot;, &quot;Traditional research&quot;,&quot;Danger zone&quot;, &quot;Data Science&quot;)) # png(&quot;figure1_1.png&quot;, units=&quot;in&quot;, width=5, height=4, res=600) # figure1_1 # dev.off() 15.7 diagram library(DiagrammeR) grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10, rankdir = LR] # several &#39;node&#39; statements node [shape = box, style = unfilled, fontname = Helvetica] 导入; 整理; 展示 node [shape = circle, style = filled, fontname = Helvetica] 转化; 可视化; 建模; # several &#39;edge&#39; statements 导入 -&gt; 整理-&gt;转化 转化 -&gt; 可视化 建模 -&gt; 转化 可视化-&gt;建模 建模-&gt;展示 #Communicate -&gt; Model #Communicate -&gt; Visualize #Model -&gt; Communicate[dir = both] #Visualize -&gt; Communicate [dir = both] } &quot;) # figure 1.2b grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10, rankdir = LR， nodesep = 0.2] # several &#39;node&#39; statements node [shape = box, style = unfilled, fontname = Helvetica] 问题设计;理解数据;提取特征;建模分析;结果呈现;代码部署 # several &#39;edge&#39; statements 问题设计 -&gt; 理解数据 -&gt; 提取特征 -&gt; 建模分析 建模分析 -&gt; 结果呈现 建模分析 -&gt; 代码部署 建模分析 -&gt; 问题设计 } &quot;) grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10, rankdir = LR， nodesep = 0.2] # several &#39;node&#39; statements node [shape = box, style = unfilled, fontname = Helvetica] # several &#39;edge&#39; statements 研究目的设定 -&gt; 提取数据 -&gt; 数据准备 -&gt; 数据探索-&gt;数据建模-&gt;呈现及自动化 提取数据 -&gt; 内部数据 提取数据 -&gt; 外部数据 } &quot;) grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [layout = neato, overlap = false, fontsize = 10, rankdir = LR] # several &#39;node&#39; statements node [shape = oval, style = unfilled, fontname = Helvetica, #fixedsize = true, width = 1.5] 提出期望 # several &#39;edge&#39; statements 提出期望 -&gt; 收集数据 -&gt; 对比期望-&gt;提出期望 设定问题-&gt; 探索分析-&gt;数据建模-&gt;模型解读 -&gt; 成果交流 -&gt; 设定问题 收集数据 -&gt; 设定问题[dir = both] } &quot;) grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [layout = neato, overlap = false, fontsize = 10, rankdir = LR] # several &#39;node&#39; statements node [shape = oval, style = unfilled, fontname = Helvetica, #fixedsize = true, width = 1.5] # several &#39;edge&#39; statements 语言研究-&gt; 语料库语言学 语言研究-&gt; 量化语言学 语言研究-&gt; 计算语言学 语言研究-&gt; 计算文体学 语言研究-&gt; 实验语音学 语言研究-&gt; 心理语言学 } &quot;) grViz(&quot; digraph data_wrangling_workflow { # a &#39;graph&#39; statement graph [layout = neato, overlap = false, fontsize = 10, rankdir = LR] # several &#39;node&#39; statements node [shape = oval, style = unfilled, fontname = Helvetica, #fixedsize = true, width = 1.5] # several &#39;edge&#39; statements tidyverse-&gt; dplyr数据转化 tidyverse-&gt; tidyr数据清理 tidyverse-&gt; readr数据导入 tidyverse-&gt; ggplot2数据可视化 tidyverse-&gt; purrr功能性编程 tidyverse-&gt; stringr文本处理 tidyverse-&gt; tibble数据框 } &quot;) # png(&quot;figure1_2.png&quot;, units=&quot;in&quot;, width=5, height=4, res=600) # plot(figure1_2) # dev.off() 15.8 Gant plot library(&quot;plan&quot;) g &lt;- new(&quot;gantt&quot;) g &lt;- ganttAddTask(g, &quot;Courses&quot;) # no times, so a heading g &lt;- ganttAddTask(g, &quot;Linguistics&quot;, &quot;2020-09-03&quot;, &quot;2020-12-05&quot;, done=100) g &lt;- ganttAddTask(g, &quot;Experimental design (B)&quot;, &quot;2020-09-03&quot;, &quot;2020-12-05&quot;, done=100) g &lt;- ganttAddTask(g, &quot;Phonetics&quot;, &quot;2020-09-03&quot;, &quot;2020-12-05&quot;, done=100) g &lt;- ganttAddTask(g, &quot;Phonology&quot;, &quot;2021-01-03&quot;, &quot;2021-04-05&quot;) g &lt;- ganttAddTask(g, &quot;Cognitive Science&quot;, &quot;2021-01-03&quot;, &quot;2021-04-05&quot;) g &lt;- ganttAddTask(g, &quot;Time-series Analysis&quot;, &quot;2021-01-03&quot;, &quot;2021-04-05&quot;) g &lt;- ganttAddTask(g, &quot;Research&quot;) # no times, so a heading g &lt;- ganttAddTask(g, &quot;Literature review&quot;, &quot;2020-09-03&quot;, &quot;2021-02-01&quot;, done=20) g &lt;- ganttAddTask(g, &quot;Develop analysis skills&quot;, &quot;2020-09-03&quot;, &quot;2021-08-01&quot;, done=30) g &lt;- ganttAddTask(g, &quot;Thesis work&quot;, &quot;2020-10-01&quot;, &quot;2021-07-15&quot;, done=30) g &lt;- ganttAddTask(g, &quot;Thesis proposal&quot;, &quot;2020-09-01&quot;, &quot;2020-10-01&quot;, done = 100) g &lt;- ganttAddTask(g, &quot;Writing (papers &amp; thesis)&quot;, &quot;2021-03-01&quot;, &quot;2021-07-15&quot;, done = 80) g &lt;- ganttAddTask(g, &quot;Thesis examination&quot;, &quot;2021-09-01&quot;, &quot;2021-09-05&quot;) #png(&quot;timeline.png&quot;, width=15, height=10, res=300, units=&quot;in&quot;) plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), event.time=&quot;2021-02-13&quot;, event.label=&quot;Report Date&quot;) par(lend=&quot;square&quot;) # default is round legend(&quot;topright&quot;, pch=22, cex = 1.5, pt.cex=2, pt.bg=gray(c(0.3, 0.9)),bty = &quot;n&quot;, border=&quot;black&quot;, legend=c(&quot;Completed &quot;, &quot;InProgress &quot;) , bg=&quot;white&quot;) #dev.off() #各种不同的图例 # legend(&quot;topright&quot;, # #shape # pch=22, # # size of the text # cex = 0.9, # #text.width = 12, # # inset = c(0, 0.6), # # no box border # #bty = &quot;n&quot;, # # size of the icon # pt.cex=2, # pt.bg=gray(c(0.3, 0.9)), # #border=&quot;white&quot;, # legend=c(&quot;Completed&quot;,&quot;In Progress&quot;), # #title=&quot;MSc plan&quot;, # bg=&quot;white&quot;) # # ## get corrdinates # coord &lt;- par(&quot;usr&quot;) # legend(x = coord[2] * 0.85, y = coord[4]*0.5, # xjust = 0, # yjust = 0, # #shape # pch=22, # # size of the text # cex = 0.7, # #text.width = 12, # inset = c(1, 0.5), # # no box border # #bty = &quot;n&quot;, # # size of the icon # pt.cex=1, # pt.bg=gray(c(0.3, 0.9)), # #border=&quot;white&quot;, # legend=c(&quot;Completed&quot;,&quot;In Progress&quot;), # #title=&quot;MSc plan&quot;, # bg=&quot;white&quot;) # # legend( x= &quot;bottomright&quot;, y=100, # legend=c(&quot;quantile&quot;,&quot;90st&quot;), # col=c(&quot;blue&quot;, &quot;yellow&quot;, ), # pch=c(&quot;.&quot;,&quot;.&quot;)) # # legend(&quot;topright&quot;, y = 1, # legend(pch=22, # pt.cex= 1, # pt.bg=gray(c(0.3, 0.9)), # border=&quot;black&quot;, # legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), # bg=&quot;white&quot;)) # # legend(x = data.frame(x = &quot;Courses&quot;, y = 2017-09-1), # pch=22, # pt.cex=2, # pt.bg=gray(c(0.3, 0.9)), # horiz=TRUE, # border=&quot;black&quot;, # legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), # title=&quot;Juqiang&#39;s plan&quot;, bg=&quot;white&quot;) # author&#39;s reply # g &lt;- new(&quot;gantt&quot;) # g &lt;- ganttAddTask(g, &quot;Courses&quot;) # no times, so a heading # g &lt;- ganttAddTask(g, &quot;Physical Oceanography (A)&quot;, &quot;2016-09-03&quot;, &quot;2016-12-05&quot;, done=100) # g &lt;- ganttAddTask(g, &quot;Chemistry Oceanography (B)&quot;, &quot;2016-09-03&quot;, &quot;2016-12-05&quot;, done=100) # g &lt;- ganttAddTask(g, &quot;Fluid Dynamics (A+)&quot;, &quot;2016-09-03&quot;, &quot;2016-12-05&quot;, done=100) # g &lt;- ganttAddTask(g, &quot;Biological Oceanography&quot;, &quot;2017-01-03&quot;, &quot;2017-04-05&quot;) # g &lt;- ganttAddTask(g, &quot;Geological Oceanography&quot;, &quot;2017-01-03&quot;, &quot;2017-04-05&quot;) # g &lt;- ganttAddTask(g, &quot;Time-series Analysis&quot;, &quot;2017-01-03&quot;, &quot;2017-04-05&quot;) # g &lt;- ganttAddTask(g, &quot;Research&quot;) # no times, so a heading # g &lt;- ganttAddTask(g, &quot;Literature review&quot;, &quot;2016-09-03&quot;, &quot;2017-02-01&quot;, done=20) # g &lt;- ganttAddTask(g, &quot;Develop analysis skills&quot;, &quot;2016-09-03&quot;, &quot;2017-08-01&quot;, done=30) # g &lt;- ganttAddTask(g, &quot;Thesis work&quot;, &quot;2016-10-01&quot;, &quot;2018-07-15&quot;, done=30) # g &lt;- ganttAddTask(g, &quot;Thesis proposal&quot;, &quot;2017-05-01&quot;, &quot;2017-06-01&quot;) # g &lt;- ganttAddTask(g, &quot;Writing (papers &amp; thesis)&quot;, &quot;2017-03-01&quot;, &quot;2018-07-15&quot;) # g &lt;- ganttAddTask(g, &quot;Defend thesis&quot;, &quot;2018-09-01&quot;, &quot;2018-09-05&quot;) # # plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), # event.time=&quot;2018-08-13&quot;, event.label=&quot;Report Date&quot;) # # par(lend=&quot;square&quot;) # default is round # # par(xpd=NA) # legend(x=0.8, y=1.05, # pch=22, # pt.cex= 1, # pt.bg=gray(c(0.3, 0.9)), # border=&quot;black&quot;, # legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), # bg=&quot;white&quot;) ## ggplot version library(lubridate) library(scales) library(Cairo) # Alternatively you can put all this in a CSV file with the same columns and # then load it with read_csv() # tasks &lt;- read_csv(&quot;path/to/the/file&quot;) tasks &lt;- tribble( ~Start, ~End, ~Project, ~Task, &quot;2015-11-15&quot;, &quot;2015-11-20&quot;, &quot;Data collection&quot;, &quot;Use IssueCrawler to expand lists&quot;, &quot;2015-11-21&quot;, &quot;2015-11-25&quot;, &quot;Data collection&quot;, &quot;Complete INGO databases&quot;, &quot;2015-11-16&quot;, &quot;2015-12-15&quot;, &quot;Data collection&quot;, &quot;Find all INGO legislation&quot;, &quot;2015-12-15&quot;, &quot;2015-12-25&quot;, &quot;Data collection&quot;, &quot;Code INGO restrictions&quot;, &quot;2015-11-15&quot;, &quot;2015-11-25&quot;, &quot;Data collection&quot;, &quot;Develop general INGO survey&quot;, &quot;2015-11-25&quot;, &quot;2015-12-31&quot;, &quot;Data collection&quot;, &quot;Administer survey&quot;, &quot;2015-12-25&quot;, &quot;2015-12-31&quot;, &quot;Data analysis&quot;, &quot;Model stability and restrictions&quot;, &quot;2016-01-01&quot;, &quot;2016-02-02&quot;, &quot;Writing&quot;, &quot;Chapter on formal restrictions (H1)&quot;, &quot;2016-01-15&quot;, &quot;2016-02-15&quot;, &quot;Writing&quot;, &quot;Paper or chapter on survey results (H3 and H4)&quot;, &quot;2016-03-16&quot;, &quot;2016-03-19&quot;, &quot;Writing&quot;, &quot;ISA conference in Atlanta&quot;, &quot;2016-02-01&quot;, &quot;2016-03-01&quot;, &quot;Data collection&quot;, &quot;Historical INGO restrictions in China&quot;, &quot;2016-02-01&quot;, &quot;2016-03-01&quot;, &quot;Data collection&quot;, &quot;Historical INGO restrictions in Egypt&quot;, &quot;2016-05-01&quot;, &quot;2016-06-01&quot;, &quot;Data collection&quot;, &quot;Fieldwork in London and Beijing&quot;, &quot;2016-03-01&quot;, &quot;2016-04-01&quot;, &quot;Data analysis&quot;, &quot;Analyze restrictions in China and Egypt&quot;, &quot;2016-04-01&quot;, &quot;2016-06-01&quot;, &quot;Data analysis&quot;, &quot;Analyze INGO activities&quot;, &quot;2016-04-01&quot;, &quot;2016-06-15&quot;, &quot;Writing&quot;, &quot;Chapter on application of restrictions (H2)&quot;, &quot;2016-05-01&quot;, &quot;2016-07-01&quot;, &quot;Writing&quot;, &quot;Chapter on INGO ideal points (H3)&quot;, &quot;2016-05-01&quot;, &quot;2016-07-01&quot;, &quot;Writing&quot;, &quot;Chapter on INGO flexibility (H4)&quot;, &quot;2016-07-01&quot;, &quot;2016-08-01&quot;, &quot;Writing&quot;, &quot;Theory&quot;, &quot;2016-08-01&quot;, &quot;2016-09-15&quot;, &quot;Writing&quot;, &quot;Conclusion&quot;, &quot;2016-08-01&quot;, &quot;2016-09-15&quot;, &quot;Writing&quot;, &quot;Introduction&quot; ) # Convert data to long for ggplot tasks.long &lt;- tasks %&gt;% mutate(Start = ymd(Start), End = ymd(End)) %&gt;% gather(date.type, task.date, -c(Project, Task)) %&gt;% arrange(date.type, task.date) %&gt;% mutate(Task = factor(Task, levels=rev(unique(Task)), ordered=TRUE)) # Custom theme for making a clean Gantt chart theme_gantt &lt;- function(base_size=11 ) { ret &lt;- theme_bw(base_size) %+replace% theme(panel.background = element_rect(fill=&quot;#ffffff&quot;, colour=NA), axis.title.x=element_text(vjust=-0.2), axis.title.y=element_text(vjust=1.5), title=element_text(vjust=1.2), panel.border = element_blank(), axis.line=element_blank(), panel.grid.minor=element_blank(), panel.grid.major.y = element_blank(), panel.grid.major.x = element_line(size=0.5, colour=&quot;grey80&quot;), axis.ticks=element_blank(), legend.position=&quot;bottom&quot;, axis.title=element_text(size=rel(0.8) ), strip.text=element_text(size=rel(1)), strip.background=element_rect(fill=&quot;#ffffff&quot;, colour=NA), panel.spacing.y=unit(1.5, &quot;lines&quot;), legend.key = element_blank()) ret } # Calculate where to put the dotted lines that show up every three entries x.breaks &lt;- seq(length(tasks$Task) + 0.5 - 3, 0, by=-3) # Build plot timeline &lt;- ggplot(tasks.long, aes(x=Task, y=task.date, colour=Project)) + geom_line(size=6) + geom_vline(xintercept=x.breaks, colour=&quot;grey80&quot;, linetype=&quot;dotted&quot;) + guides(colour=guide_legend(title=NULL)) + labs(x=NULL, y=NULL) + coord_flip() + scale_y_date(date_breaks=&quot;2 months&quot;, labels=date_format(&quot;%b ‘%y&quot;)) + theme_gantt() + theme(axis.text.x=element_text(angle=45, hjust=1)) timeline 15.9 图标Lengends 15.9.1 图标整体 ## 去除图标 lexdec.plt + theme(legend.position = &quot;none&quot;) ## 置于上方 lexdec.plt + theme(legend.position = &quot;top&quot;) #置于其他地方 lexdec.plt + theme(legend.position = c(.8, .3)) # 左上角 lexdec.plt + theme(legend.position = c(1, 0), legend.justification = c(1, 0)) # 右下角 lexdec.plt + theme(legend.position = c(1, 1), legend.justification = c(1, 1)) lexdec.plt + theme(legend.position = c(.85, .2)) + theme(legend.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) lexdec.plt + theme(legend.position = c(.85, .2)) + theme(legend.background = element_blank()) + # Remove overall border theme(legend.key = element_blank()) # Remove border around each item 15.9.2 图标标题 # show chinese characters in ggplot library(showtext) showtext_auto() # 改变图例标题内容 lexdec.plt +labs(fill = &quot;是否\\n正确&quot;) # 改变图例标题风格 lexdec.plt + theme(legend.title = element_text( face = &quot;italic&quot;, #family = &quot;Times&quot;, colour = &quot;red&quot;, size = 14) ) # 去掉图例标题 lexdec.plt + guides(fill = guide_legend(title = NULL)) 15.9.3 图例标识 lexdec.plt + scale_fill_discrete( ## 改变顺序 limits = c(&quot;incorrect&quot;, &quot;correct&quot;), # 改变内容 labels = c(&quot;错误&quot;, &quot;正确&quot;) ) ## Scale for fill is already present. ## Adding another scale for fill, which will replace the existing scale. lexdec.plt + guides(fill = guide_legend(reverse = TRUE)) lexdec.plt + theme(legend.text = element_text( face = &quot;italic&quot;, family = &quot;sans&quot;, colour = &quot;red&quot;, size = 14) ) 15.10 坐标轴 # 翻转 lexdec.plt + coord_flip() # 纵坐标范围设定 lexdec.plt + coord_cartesian(ylim = c(6, 7)) # lexdec%&gt;% # group_by(Class, Correct)%&gt;% # summarise(mean = mean(RT))%&gt;% # ggplot(aes(Class, mean, fill = Correct))+ # geom_bar(aes(Class, mean, fill = Correct))+ # #geom_col(position = &quot;dodge&quot;, colour = &quot;black&quot;) + # scale_fill_brewer(palette = &quot;Pastel1&quot;)+ # ylim(4, 7) # # 连续坐标顺序调整 lexdec.plt + scale_y_reverse() lexdec.plt + scale_y_reverse(limits = c(8, 0)) # Create a data frame with the vowel formant data vowel_data &lt;- data.frame( Vowel = c(&quot;/i/&quot;, &quot;/e/&quot;, &quot;/a/&quot;, &quot;/o/&quot;, &quot;/u/&quot;), F1 = c(300, 500, 700, 500, 350), F2 = c(2400, 2200, 1200, 900, 800) ) # Create a scatter plot using ggplot2 vowel.plt = ggplot(vowel_data, aes(x = F2, y = F1, label = Vowel)) + geom_point(size = 3) + geom_text(vjust = -1, hjust = 0.5) + xlab(&quot;F2 (Hz)&quot;) + ylab(&quot;F1 (Hz)&quot;) + scale_y_reverse(limits = c(800, 100))+ scale_x_reverse(limits = c(2500, 600))+ ggtitle(&quot;Vowel Formant Scatter Plot&quot;) vowel.plt ## 离散坐标顺序调整 lexdec.plt + scale_x_discrete(limits = c(&quot;plant&quot;, &quot;animal&quot;)) lexdec.plt + scale_x_discrete(limits = c(&quot;plant&quot;)) ## Warning: Removed 2 rows containing missing values (`geom_col()`). ## 去除坐标轴标号 # 可以用于几幅图合并的时候节省空间 lexdec.plt + theme(axis.text.y = element_blank()) lexdec.plt + #去掉横纵坐标 theme(axis.ticks = element_blank(), axis.text.y = element_blank()) lexdec.plt + #去掉纵坐标及横线 scale_y_continuous(breaks = NULL) ## 改变横纵坐标形式 lexdec.plt + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) lexdec.plt + theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) lexdec.plt + theme( axis.text.x = element_text(family = &quot;sans&quot;, face = &quot;italic&quot;, colour = &quot;darkred&quot;, size = rel(0.9))) # 横纵坐标轴名称 lexdec.plt + xlab(&quot;动物还是植物&quot;) + ylab(&quot;反应时&quot;) lexdec.plt + xlab(&quot;&quot;) + ylab(&quot;&quot;) lexdec.plt + theme(axis.title.y = element_text(angle = 0, face = &quot;italic&quot;, size = 14)) lexdec.plt + theme(axis.title.x = element_text( angle = 90, face = &quot;italic&quot;, colour = &quot;darkred&quot;, size = 14) ) lexdec.plt + theme( panel.border = element_blank(), axis.line = element_line(colour = &quot;blue&quot;, size = 2, lineend = &quot;square&quot;)) # Create a data frame with the sample BNC word frequency data bnc_data &lt;- data.frame( Word = c(&quot;the&quot;, &quot;of&quot;, &quot;and&quot;, &quot;to&quot;, &quot;a&quot;, &quot;in&quot;, &quot;that&quot;, &quot;is&quot;, &quot;was&quot;, &quot;it&quot;, &quot;for&quot;, &quot;on&quot;, &quot;with&quot;, &quot;as&quot;, &quot;by&quot;, &quot;at&quot;, &quot;from&quot;, &quot;this&quot;, &quot;be&quot;, &quot;are&quot;), Frequency = c(6187266, 2947667, 2687866, 2595604, 2167570, 1724982, 1037961, 1017505, 1009930, 989823, 900003, 892343, 750221, 692788, 678463, 670544, 567532, 559451, 548621, 541298)) # View the data frame print(bnc_data) ## Word Frequency ## 1 the 6187266 ## 2 of 2947667 ## 3 and 2687866 ## 4 to 2595604 ## 5 a 2167570 ## 6 in 1724982 ## 7 that 1037961 ## 8 is 1017505 ## 9 was 1009930 ## 10 it 989823 ## 11 for 900003 ## 12 on 892343 ## 13 with 750221 ## 14 as 692788 ## 15 by 678463 ## 16 at 670544 ## 17 from 567532 ## 18 this 559451 ## 19 be 548621 ## 20 are 541298 # Create a line plot using ggplot2 bnc.plt = ggplot(bnc_data, aes(x = reorder(Word, -Frequency), y = Frequency, group = 1)) + geom_line(color = &quot;steelblue&quot;, size = 1.2) + geom_point(color = &quot;darkred&quot;, size = 3) + xlab(&quot;Word&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Word Frequency Distribution in BNC&quot;) + #theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) bnc.plt bnc.plt + scale_y_log10() ggplot(bnc_data, aes(x = reorder(Word, -Frequency), y = log10(Frequency), group = 1)) + geom_line(color = &quot;steelblue&quot;, size = 1.2) + geom_point(color = &quot;darkred&quot;, size = 3) + xlab(&quot;Word&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Word Frequency Distribution in BNC&quot;) + #theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## 极点坐标轴 # Create the dataset cognitive_data &lt;- data.frame( Group = c(&quot;AD&quot;, &quot;MCI&quot;, &quot;Normal&quot;), Memory = c(20, 35, 45), Language = c(22, 30, 43), Attention = c(18, 28, 40), Executive = c(15, 25, 42), Visuospatial = c(17, 23, 40), Orientation = c(10, 20, 45), Attention = c(18, 28, 40) ) # Reshape the data to a long format cognitive_data_long &lt;- cognitive_data %&gt;% pivot_longer(cols = -Group, names_to = &quot;Cognitive_Measure&quot;, values_to = &quot;Score&quot;) # Ensure the polygons close by repeating the first measure at the end cognitive_data_long &lt;- cognitive_data_long %&gt;% group_by(Group) %&gt;% arrange(Group, Cognitive_Measure) %&gt;% bind_rows(mutate(., Cognitive_Measure = first(Cognitive_Measure), Score = first(Score))) # Create the radar chart using ggplot2 ggplot(cognitive_data_long, aes(x = Cognitive_Measure, y = Score, group = Group, color = Group)) + geom_polygon(aes(fill = Group), alpha = 0.3) + geom_line(size = 1) + geom_point(size = 2) + coord_polar() + theme_minimal() + labs(title = &quot;Cognitive Measures Across AD, MCI, and Normal Aging&quot;, x = &quot;Cognitive Measure&quot;, y = &quot;Score&quot;) + theme(axis.text.x = element_text(size = 12)) 15.11 配色 15.12 注释标记 15.13 theme # Create the base plot # hw_plot &lt;- ggplot(heightweight, aes(x = ageYear, y = heightIn)) + # geom_point() # # Grey theme (the default) vowel.plt + theme_grey() # Black-and-white theme vowel.plt + theme_bw() # Minimal theme without background annotations vowel.plt + theme_minimal() # Classic theme, with axis lines but no gridlines vowel.plt + theme_classic() "],["表格制作.html", "Chapter 16 表格制作 16.1 gt 16.2 DT: easy filtering &amp; sorting 16.3 modelsummary", " Chapter 16 表格制作 library(tidyverse) library(languageR) data(&quot;lexdec&quot;) # show chinese characters in ggplot library(showtext) showtext_auto() 16.1 gt The gt package in R is a powerful tool for creating elegant and customizable tables for data visualization and reporting. It offers options for formatting, styling, and theming tables, as well as support for handling complex data structures and creating publication-ready tables with ease. The GT package, stands for “Grammar of Tables”. It was created by the RStudio team and first released in 2018. It offers an intuitive, tidyverse-inspired syntax, making table creation accessible, including for beginners. GT’s user-friendly design for handling complex formatting has quickly gained popularity in the R community. Its ease of use and readability make it a go-to choice for many R users seeking to create clear and aesthetically pleasing tables. library(gt) lexdec%&gt;% group_by(Class, Correct, NativeLanguage)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% gt() ## `summarise()` has grouped output by &#39;Class&#39;, &#39;Correct&#39;. You can override using the `.groups` argument. #oiozptfkhv table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #oiozptfkhv thead, #oiozptfkhv tbody, #oiozptfkhv tfoot, #oiozptfkhv tr, #oiozptfkhv td, #oiozptfkhv th { border-style: none; } #oiozptfkhv p { margin: 0; padding: 0; } #oiozptfkhv .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #oiozptfkhv .gt_caption { padding-top: 4px; padding-bottom: 4px; } #oiozptfkhv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #oiozptfkhv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #oiozptfkhv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oiozptfkhv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oiozptfkhv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oiozptfkhv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #oiozptfkhv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #oiozptfkhv .gt_column_spanner_outer:first-child { padding-left: 0; } #oiozptfkhv .gt_column_spanner_outer:last-child { padding-right: 0; } #oiozptfkhv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #oiozptfkhv .gt_spanner_row { border-bottom-style: hidden; } #oiozptfkhv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #oiozptfkhv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #oiozptfkhv .gt_from_md > :first-child { margin-top: 0; } #oiozptfkhv .gt_from_md > :last-child { margin-bottom: 0; } #oiozptfkhv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #oiozptfkhv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #oiozptfkhv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #oiozptfkhv .gt_row_group_first td { border-top-width: 2px; } #oiozptfkhv .gt_row_group_first th { border-top-width: 2px; } #oiozptfkhv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oiozptfkhv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #oiozptfkhv .gt_first_summary_row.thick { border-top-width: 2px; } #oiozptfkhv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oiozptfkhv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oiozptfkhv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #oiozptfkhv .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #oiozptfkhv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #oiozptfkhv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oiozptfkhv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oiozptfkhv .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oiozptfkhv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oiozptfkhv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #oiozptfkhv .gt_left { text-align: left; } #oiozptfkhv .gt_center { text-align: center; } #oiozptfkhv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #oiozptfkhv .gt_font_normal { font-weight: normal; } #oiozptfkhv .gt_font_bold { font-weight: bold; } #oiozptfkhv .gt_font_italic { font-style: italic; } #oiozptfkhv .gt_super { font-size: 65%; } #oiozptfkhv .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #oiozptfkhv .gt_asterisk { font-size: 100%; vertical-align: 0; } #oiozptfkhv .gt_indent_1 { text-indent: 5px; } #oiozptfkhv .gt_indent_2 { text-indent: 10px; } #oiozptfkhv .gt_indent_3 { text-indent: 15px; } #oiozptfkhv .gt_indent_4 { text-indent: 20px; } #oiozptfkhv .gt_indent_5 { text-indent: 25px; } Class Correct NativeLanguage mean animal correct English 6.316985 animal correct Other 6.484137 animal incorrect English 6.210368 animal incorrect Other 6.542047 plant correct English 6.328352 plant correct Other 6.448381 plant incorrect English 6.162450 plant incorrect Other 6.631645 lexdec%&gt;% group_by(Class, Correct, NativeLanguage)%&gt;% summarise(mean = mean(RT))%&gt;% #ungroup()%&gt;% gt() ## `summarise()` has grouped output by &#39;Class&#39;, &#39;Correct&#39;. You can override using the `.groups` argument. #vgpqvrqbdm table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #vgpqvrqbdm thead, #vgpqvrqbdm tbody, #vgpqvrqbdm tfoot, #vgpqvrqbdm tr, #vgpqvrqbdm td, #vgpqvrqbdm th { border-style: none; } #vgpqvrqbdm p { margin: 0; padding: 0; } #vgpqvrqbdm .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #vgpqvrqbdm .gt_caption { padding-top: 4px; padding-bottom: 4px; } #vgpqvrqbdm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #vgpqvrqbdm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #vgpqvrqbdm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vgpqvrqbdm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vgpqvrqbdm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vgpqvrqbdm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #vgpqvrqbdm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #vgpqvrqbdm .gt_column_spanner_outer:first-child { padding-left: 0; } #vgpqvrqbdm .gt_column_spanner_outer:last-child { padding-right: 0; } #vgpqvrqbdm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #vgpqvrqbdm .gt_spanner_row { border-bottom-style: hidden; } #vgpqvrqbdm .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #vgpqvrqbdm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #vgpqvrqbdm .gt_from_md > :first-child { margin-top: 0; } #vgpqvrqbdm .gt_from_md > :last-child { margin-bottom: 0; } #vgpqvrqbdm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #vgpqvrqbdm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #vgpqvrqbdm .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #vgpqvrqbdm .gt_row_group_first td { border-top-width: 2px; } #vgpqvrqbdm .gt_row_group_first th { border-top-width: 2px; } #vgpqvrqbdm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vgpqvrqbdm .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #vgpqvrqbdm .gt_first_summary_row.thick { border-top-width: 2px; } #vgpqvrqbdm .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vgpqvrqbdm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vgpqvrqbdm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #vgpqvrqbdm .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #vgpqvrqbdm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #vgpqvrqbdm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vgpqvrqbdm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vgpqvrqbdm .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #vgpqvrqbdm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vgpqvrqbdm .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #vgpqvrqbdm .gt_left { text-align: left; } #vgpqvrqbdm .gt_center { text-align: center; } #vgpqvrqbdm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #vgpqvrqbdm .gt_font_normal { font-weight: normal; } #vgpqvrqbdm .gt_font_bold { font-weight: bold; } #vgpqvrqbdm .gt_font_italic { font-style: italic; } #vgpqvrqbdm .gt_super { font-size: 65%; } #vgpqvrqbdm .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #vgpqvrqbdm .gt_asterisk { font-size: 100%; vertical-align: 0; } #vgpqvrqbdm .gt_indent_1 { text-indent: 5px; } #vgpqvrqbdm .gt_indent_2 { text-indent: 10px; } #vgpqvrqbdm .gt_indent_3 { text-indent: 15px; } #vgpqvrqbdm .gt_indent_4 { text-indent: 20px; } #vgpqvrqbdm .gt_indent_5 { text-indent: 25px; } NativeLanguage mean animal - correct English 6.316985 Other 6.484137 animal - incorrect English 6.210368 Other 6.542047 plant - correct English 6.328352 Other 6.448381 plant - incorrect English 6.162450 Other 6.631645 # 添加表格标题和副标题 lexdec%&gt;% group_by(Class, Correct, NativeLanguage)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% gt() %&gt;% tab_header(title = md(&quot;词汇**反应时**&quot;), subtitle = md(&quot;不同词汇 *不同语言背景*&quot;)) ## `summarise()` has grouped output by &#39;Class&#39;, &#39;Correct&#39;. You can override using the `.groups` argument. #evsytvynyw table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #evsytvynyw thead, #evsytvynyw tbody, #evsytvynyw tfoot, #evsytvynyw tr, #evsytvynyw td, #evsytvynyw th { border-style: none; } #evsytvynyw p { margin: 0; padding: 0; } #evsytvynyw .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #evsytvynyw .gt_caption { padding-top: 4px; padding-bottom: 4px; } #evsytvynyw .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #evsytvynyw .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #evsytvynyw .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #evsytvynyw .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #evsytvynyw .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #evsytvynyw .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #evsytvynyw .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #evsytvynyw .gt_column_spanner_outer:first-child { padding-left: 0; } #evsytvynyw .gt_column_spanner_outer:last-child { padding-right: 0; } #evsytvynyw .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #evsytvynyw .gt_spanner_row { border-bottom-style: hidden; } #evsytvynyw .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #evsytvynyw .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #evsytvynyw .gt_from_md > :first-child { margin-top: 0; } #evsytvynyw .gt_from_md > :last-child { margin-bottom: 0; } #evsytvynyw .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #evsytvynyw .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #evsytvynyw .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #evsytvynyw .gt_row_group_first td { border-top-width: 2px; } #evsytvynyw .gt_row_group_first th { border-top-width: 2px; } #evsytvynyw .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #evsytvynyw .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #evsytvynyw .gt_first_summary_row.thick { border-top-width: 2px; } #evsytvynyw .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #evsytvynyw .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #evsytvynyw .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #evsytvynyw .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #evsytvynyw .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #evsytvynyw .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #evsytvynyw .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #evsytvynyw .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #evsytvynyw .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #evsytvynyw .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #evsytvynyw .gt_left { text-align: left; } #evsytvynyw .gt_center { text-align: center; } #evsytvynyw .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #evsytvynyw .gt_font_normal { font-weight: normal; } #evsytvynyw .gt_font_bold { font-weight: bold; } #evsytvynyw .gt_font_italic { font-style: italic; } #evsytvynyw .gt_super { font-size: 65%; } #evsytvynyw .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #evsytvynyw .gt_asterisk { font-size: 100%; vertical-align: 0; } #evsytvynyw .gt_indent_1 { text-indent: 5px; } #evsytvynyw .gt_indent_2 { text-indent: 10px; } #evsytvynyw .gt_indent_3 { text-indent: 15px; } #evsytvynyw .gt_indent_4 { text-indent: 20px; } #evsytvynyw .gt_indent_5 { text-indent: 25px; } 词汇反应时 不同词汇 不同语言背景 Class Correct NativeLanguage mean animal correct English 6.316985 animal correct Other 6.484137 animal incorrect English 6.210368 animal incorrect Other 6.542047 plant correct English 6.328352 plant correct Other 6.448381 plant incorrect English 6.162450 plant incorrect Other 6.631645 # 添加脚注 lexdec%&gt;% group_by(Class, Correct, NativeLanguage)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% gt() %&gt;% tab_footnote( footnote = &quot;Source: lexdec package&quot;, locations = cells_body(columns = NativeLanguage, rows = 1) ) ## `summarise()` has grouped output by &#39;Class&#39;, &#39;Correct&#39;. You can override using the `.groups` argument. #yncngcwvzo table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #yncngcwvzo thead, #yncngcwvzo tbody, #yncngcwvzo tfoot, #yncngcwvzo tr, #yncngcwvzo td, #yncngcwvzo th { border-style: none; } #yncngcwvzo p { margin: 0; padding: 0; } #yncngcwvzo .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #yncngcwvzo .gt_caption { padding-top: 4px; padding-bottom: 4px; } #yncngcwvzo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #yncngcwvzo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #yncngcwvzo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yncngcwvzo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yncngcwvzo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #yncngcwvzo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #yncngcwvzo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #yncngcwvzo .gt_column_spanner_outer:first-child { padding-left: 0; } #yncngcwvzo .gt_column_spanner_outer:last-child { padding-right: 0; } #yncngcwvzo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #yncngcwvzo .gt_spanner_row { border-bottom-style: hidden; } #yncngcwvzo .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #yncngcwvzo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #yncngcwvzo .gt_from_md > :first-child { margin-top: 0; } #yncngcwvzo .gt_from_md > :last-child { margin-bottom: 0; } #yncngcwvzo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #yncngcwvzo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #yncngcwvzo .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #yncngcwvzo .gt_row_group_first td { border-top-width: 2px; } #yncngcwvzo .gt_row_group_first th { border-top-width: 2px; } #yncngcwvzo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yncngcwvzo .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #yncngcwvzo .gt_first_summary_row.thick { border-top-width: 2px; } #yncngcwvzo .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yncngcwvzo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #yncngcwvzo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #yncngcwvzo .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #yncngcwvzo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #yncngcwvzo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #yncngcwvzo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yncngcwvzo .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #yncngcwvzo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #yncngcwvzo .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #yncngcwvzo .gt_left { text-align: left; } #yncngcwvzo .gt_center { text-align: center; } #yncngcwvzo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #yncngcwvzo .gt_font_normal { font-weight: normal; } #yncngcwvzo .gt_font_bold { font-weight: bold; } #yncngcwvzo .gt_font_italic { font-style: italic; } #yncngcwvzo .gt_super { font-size: 65%; } #yncngcwvzo .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #yncngcwvzo .gt_asterisk { font-size: 100%; vertical-align: 0; } #yncngcwvzo .gt_indent_1 { text-indent: 5px; } #yncngcwvzo .gt_indent_2 { text-indent: 10px; } #yncngcwvzo .gt_indent_3 { text-indent: 15px; } #yncngcwvzo .gt_indent_4 { text-indent: 20px; } #yncngcwvzo .gt_indent_5 { text-indent: 25px; } Class Correct NativeLanguage mean animal correct English1 6.316985 animal correct Other 6.484137 animal incorrect English 6.210368 animal incorrect Other 6.542047 plant correct English 6.328352 plant correct Other 6.448381 plant incorrect English 6.162450 plant incorrect Other 6.631645 1 Source: lexdec package lexdec%&gt;% group_by(Class, Correct, NativeLanguage)%&gt;% summarise(mean = mean(RT))%&gt;% ungroup()%&gt;% gt() %&gt;% tab_spanner( label = &quot;词汇特征&quot;, columns = c(Class, Correct)) ## `summarise()` has grouped output by &#39;Class&#39;, &#39;Correct&#39;. You can override using the `.groups` argument. #uehsjrjlbj table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #uehsjrjlbj thead, #uehsjrjlbj tbody, #uehsjrjlbj tfoot, #uehsjrjlbj tr, #uehsjrjlbj td, #uehsjrjlbj th { border-style: none; } #uehsjrjlbj p { margin: 0; padding: 0; } #uehsjrjlbj .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uehsjrjlbj .gt_caption { padding-top: 4px; padding-bottom: 4px; } #uehsjrjlbj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uehsjrjlbj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #uehsjrjlbj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uehsjrjlbj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uehsjrjlbj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uehsjrjlbj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uehsjrjlbj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uehsjrjlbj .gt_column_spanner_outer:first-child { padding-left: 0; } #uehsjrjlbj .gt_column_spanner_outer:last-child { padding-right: 0; } #uehsjrjlbj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #uehsjrjlbj .gt_spanner_row { border-bottom-style: hidden; } #uehsjrjlbj .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #uehsjrjlbj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uehsjrjlbj .gt_from_md > :first-child { margin-top: 0; } #uehsjrjlbj .gt_from_md > :last-child { margin-bottom: 0; } #uehsjrjlbj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uehsjrjlbj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #uehsjrjlbj .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #uehsjrjlbj .gt_row_group_first td { border-top-width: 2px; } #uehsjrjlbj .gt_row_group_first th { border-top-width: 2px; } #uehsjrjlbj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uehsjrjlbj .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #uehsjrjlbj .gt_first_summary_row.thick { border-top-width: 2px; } #uehsjrjlbj .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uehsjrjlbj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uehsjrjlbj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uehsjrjlbj .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #uehsjrjlbj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uehsjrjlbj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uehsjrjlbj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uehsjrjlbj .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uehsjrjlbj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uehsjrjlbj .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uehsjrjlbj .gt_left { text-align: left; } #uehsjrjlbj .gt_center { text-align: center; } #uehsjrjlbj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uehsjrjlbj .gt_font_normal { font-weight: normal; } #uehsjrjlbj .gt_font_bold { font-weight: bold; } #uehsjrjlbj .gt_font_italic { font-style: italic; } #uehsjrjlbj .gt_super { font-size: 65%; } #uehsjrjlbj .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #uehsjrjlbj .gt_asterisk { font-size: 100%; vertical-align: 0; } #uehsjrjlbj .gt_indent_1 { text-indent: 5px; } #uehsjrjlbj .gt_indent_2 { text-indent: 10px; } #uehsjrjlbj .gt_indent_3 { text-indent: 15px; } #uehsjrjlbj .gt_indent_4 { text-indent: 20px; } #uehsjrjlbj .gt_indent_5 { text-indent: 25px; } 词汇特征 NativeLanguage mean Class Correct animal correct English 6.316985 animal correct Other 6.484137 animal incorrect English 6.210368 animal incorrect Other 6.542047 plant correct English 6.328352 plant correct Other 6.448381 plant incorrect English 6.162450 plant incorrect Other 6.631645 16.1.1 gtsummary The gtsummary package lets you automatically summarize information about your dataset. In the following case, we use the tbl_summary() function to obtain the main information on the iris dataset. The package detects the variable type and generates the appropriate summary type. library(gtsummary) lexdec%&gt;% tbl_summary() #vltnqnvchn table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #vltnqnvchn thead, #vltnqnvchn tbody, #vltnqnvchn tfoot, #vltnqnvchn tr, #vltnqnvchn td, #vltnqnvchn th { border-style: none; } #vltnqnvchn p { margin: 0; padding: 0; } #vltnqnvchn .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #vltnqnvchn .gt_caption { padding-top: 4px; padding-bottom: 4px; } #vltnqnvchn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #vltnqnvchn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #vltnqnvchn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vltnqnvchn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vltnqnvchn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #vltnqnvchn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #vltnqnvchn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #vltnqnvchn .gt_column_spanner_outer:first-child { padding-left: 0; } #vltnqnvchn .gt_column_spanner_outer:last-child { padding-right: 0; } #vltnqnvchn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #vltnqnvchn .gt_spanner_row { border-bottom-style: hidden; } #vltnqnvchn .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #vltnqnvchn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #vltnqnvchn .gt_from_md > :first-child { margin-top: 0; } #vltnqnvchn .gt_from_md > :last-child { margin-bottom: 0; } #vltnqnvchn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #vltnqnvchn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #vltnqnvchn .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #vltnqnvchn .gt_row_group_first td { border-top-width: 2px; } #vltnqnvchn .gt_row_group_first th { border-top-width: 2px; } #vltnqnvchn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vltnqnvchn .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #vltnqnvchn .gt_first_summary_row.thick { border-top-width: 2px; } #vltnqnvchn .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vltnqnvchn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #vltnqnvchn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #vltnqnvchn .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #vltnqnvchn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #vltnqnvchn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #vltnqnvchn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vltnqnvchn .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #vltnqnvchn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #vltnqnvchn .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #vltnqnvchn .gt_left { text-align: left; } #vltnqnvchn .gt_center { text-align: center; } #vltnqnvchn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #vltnqnvchn .gt_font_normal { font-weight: normal; } #vltnqnvchn .gt_font_bold { font-weight: bold; } #vltnqnvchn .gt_font_italic { font-style: italic; } #vltnqnvchn .gt_super { font-size: 65%; } #vltnqnvchn .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #vltnqnvchn .gt_asterisk { font-size: 100%; vertical-align: 0; } #vltnqnvchn .gt_indent_1 { text-indent: 5px; } #vltnqnvchn .gt_indent_2 { text-indent: 10px; } #vltnqnvchn .gt_indent_3 { text-indent: 15px; } #vltnqnvchn .gt_indent_4 { text-indent: 20px; } #vltnqnvchn .gt_indent_5 { text-indent: 25px; } Characteristic N = 1,6591 Subject     A1 79 (4.8%)     A2 79 (4.8%)     A3 79 (4.8%)     C 79 (4.8%)     D 79 (4.8%)     I 79 (4.8%)     J 79 (4.8%)     K 79 (4.8%)     M1 79 (4.8%)     M2 79 (4.8%)     P 79 (4.8%)     R1 79 (4.8%)     R2 79 (4.8%)     R3 79 (4.8%)     S 79 (4.8%)     T1 79 (4.8%)     T2 79 (4.8%)     V 79 (4.8%)     W1 79 (4.8%)     W2 79 (4.8%)     Z 79 (4.8%) RT 6.35 (6.21, 6.50) Trial 106 (64, 146) Sex     F 1,106 (67%)     M 553 (33%) NativeLanguage     English 948 (57%)     Other 711 (43%) Correct     correct 1,594 (96%)     incorrect 65 (3.9%) PrevType     nonword 855 (52%)     word 804 (48%) PrevCorrect     correct 1,542 (93%)     incorrect 117 (7.1%) Word     almond 21 (1.3%)     ant 21 (1.3%)     apple 21 (1.3%)     apricot 21 (1.3%)     asparagus 21 (1.3%)     avocado 21 (1.3%)     banana 21 (1.3%)     bat 21 (1.3%)     beaver 21 (1.3%)     bee 21 (1.3%)     beetroot 21 (1.3%)     blackberry 21 (1.3%)     blueberry 21 (1.3%)     broccoli 21 (1.3%)     bunny 21 (1.3%)     butterfly 21 (1.3%)     camel 21 (1.3%)     carrot 21 (1.3%)     cat 21 (1.3%)     cherry 21 (1.3%)     chicken 21 (1.3%)     clove 21 (1.3%)     crocodile 21 (1.3%)     cucumber 21 (1.3%)     dog 21 (1.3%)     dolphin 21 (1.3%)     donkey 21 (1.3%)     eagle 21 (1.3%)     eggplant 21 (1.3%)     elephant 21 (1.3%)     fox 21 (1.3%)     frog 21 (1.3%)     gherkin 21 (1.3%)     goat 21 (1.3%)     goose 21 (1.3%)     grape 21 (1.3%)     gull 21 (1.3%)     hedgehog 21 (1.3%)     horse 21 (1.3%)     kiwi 21 (1.3%)     leek 21 (1.3%)     lemon 21 (1.3%)     lettuce 21 (1.3%)     lion 21 (1.3%)     magpie 21 (1.3%)     melon 21 (1.3%)     mole 21 (1.3%)     monkey 21 (1.3%)     moose 21 (1.3%)     mouse 21 (1.3%)     mushroom 21 (1.3%)     mustard 21 (1.3%)     olive 21 (1.3%)     orange 21 (1.3%)     owl 21 (1.3%)     paprika 21 (1.3%)     peanut 21 (1.3%)     pear 21 (1.3%)     pig 21 (1.3%)     pineapple 21 (1.3%)     potato 21 (1.3%)     radish 21 (1.3%)     reindeer 21 (1.3%)     shark 21 (1.3%)     sheep 21 (1.3%)     snake 21 (1.3%)     spider 21 (1.3%)     squid 21 (1.3%)     squirrel 21 (1.3%)     stork 21 (1.3%)     strawberry 21 (1.3%)     swan 21 (1.3%)     tomato 21 (1.3%)     tortoise 21 (1.3%)     vulture 21 (1.3%)     walnut 21 (1.3%)     wasp 21 (1.3%)     whale 21 (1.3%)     woodpecker 21 (1.3%) Frequency 4.75 (3.95, 5.65) FamilySize 0.00 (0.00, 1.10) SynsetCount     0.6931472 189 (11%)     1.0986123 735 (44%)     1.3862944 273 (16%)     1.6094379 147 (8.9%)     1.7917595 84 (5.1%)     1.9459101 105 (6.3%)     2.0794415 63 (3.8%)     2.1972246 21 (1.3%)     2.3025851 42 (2.5%) Length     3 168 (10%)     4 210 (13%)     5 399 (24%)     6 315 (19%)     7 189 (11%)     8 210 (13%)     9 105 (6.3%)     10 63 (3.8%) Class     animal 924 (56%)     plant 735 (44%) FreqSingular 69 (23, 146) FreqPlural 49 (19, 132) DerivEntropy 0.04 (0.00, 0.68) Complex     complex 210 (13%)     simplex 1,449 (87%) rInfl 0.19 (-0.30, 0.64) meanRT 6.36 (6.32, 6.42) SubjFreq 3.88 (3.16, 4.68) meanSize 3.10 (1.89, 3.71) meanWeight 2.76 (1.46, 3.42) BNCw 3 (2, 7) BNCc 1 (0, 3) BNCd 4 (1, 10) BNCcRatio 0.27 (0.10, 0.56) BNCdRatio 0.93 (0.56, 2.13) 1 n (%); Median (IQR) lexdec %&gt;% select(Class, Correct, NativeLanguage, RT)%&gt;% tbl_summary( by=NativeLanguage, # group by the `vs` variable (dichotomous: 0 or 1) statistic = list( all_continuous() ~ &quot;{mean} ({sd})&quot;, # will display: mean (standard deviation) all_categorical() ~ &quot;{n} / {N} ({p}%)&quot; # will display: n / N (percentage) ) ) %&gt;% add_overall() %&gt;% # statistics for all observations add_p() %&gt;% # add p-values bold_labels() %&gt;% # make label in bold italicize_levels() # make categories in label in italic #kdlnsygvzi table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #kdlnsygvzi thead, #kdlnsygvzi tbody, #kdlnsygvzi tfoot, #kdlnsygvzi tr, #kdlnsygvzi td, #kdlnsygvzi th { border-style: none; } #kdlnsygvzi p { margin: 0; padding: 0; } #kdlnsygvzi .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kdlnsygvzi .gt_caption { padding-top: 4px; padding-bottom: 4px; } #kdlnsygvzi .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kdlnsygvzi .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #kdlnsygvzi .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kdlnsygvzi .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kdlnsygvzi .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kdlnsygvzi .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kdlnsygvzi .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kdlnsygvzi .gt_column_spanner_outer:first-child { padding-left: 0; } #kdlnsygvzi .gt_column_spanner_outer:last-child { padding-right: 0; } #kdlnsygvzi .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kdlnsygvzi .gt_spanner_row { border-bottom-style: hidden; } #kdlnsygvzi .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #kdlnsygvzi .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kdlnsygvzi .gt_from_md > :first-child { margin-top: 0; } #kdlnsygvzi .gt_from_md > :last-child { margin-bottom: 0; } #kdlnsygvzi .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kdlnsygvzi .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #kdlnsygvzi .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #kdlnsygvzi .gt_row_group_first td { border-top-width: 2px; } #kdlnsygvzi .gt_row_group_first th { border-top-width: 2px; } #kdlnsygvzi .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kdlnsygvzi .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #kdlnsygvzi .gt_first_summary_row.thick { border-top-width: 2px; } #kdlnsygvzi .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kdlnsygvzi .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kdlnsygvzi .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kdlnsygvzi .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #kdlnsygvzi .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kdlnsygvzi .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kdlnsygvzi .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kdlnsygvzi .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #kdlnsygvzi .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kdlnsygvzi .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #kdlnsygvzi .gt_left { text-align: left; } #kdlnsygvzi .gt_center { text-align: center; } #kdlnsygvzi .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kdlnsygvzi .gt_font_normal { font-weight: normal; } #kdlnsygvzi .gt_font_bold { font-weight: bold; } #kdlnsygvzi .gt_font_italic { font-style: italic; } #kdlnsygvzi .gt_super { font-size: 65%; } #kdlnsygvzi .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #kdlnsygvzi .gt_asterisk { font-size: 100%; vertical-align: 0; } #kdlnsygvzi .gt_indent_1 { text-indent: 5px; } #kdlnsygvzi .gt_indent_2 { text-indent: 10px; } #kdlnsygvzi .gt_indent_3 { text-indent: 15px; } #kdlnsygvzi .gt_indent_4 { text-indent: 20px; } #kdlnsygvzi .gt_indent_5 { text-indent: 25px; } Characteristic Overall, N = 1,6591 English, N = 9481 Other, N = 7111 p-value2 Class >0.9     animal 924 / 1,659 (56%) 528 / 948 (56%) 396 / 711 (56%)     plant 735 / 1,659 (44%) 420 / 948 (44%) 315 / 711 (44%) Correct 0.019     correct 1,594 / 1,659 (96%) 920 / 948 (97%) 674 / 711 (95%)     incorrect 65 / 1,659 (3.9%) 28 / 948 (3.0%) 37 / 711 (5.2%) RT 6.39 (0.24) 6.32 (0.20) 6.47 (0.26) 1 n / N (%); Mean (SD) 2 Pearson’s Chi-squared test; Wilcoxon rank sum test 16.1.2 gtExtras gtExtras augments and expands the functionalities of the gt package. It allows to create even more sophisticated and visually appealing tables. It comes with a set of themes to make your table good-looking with just one more line of code. It provides functions to easily add plots in table cells. It also has helper functions to help with colors and icons. #install.packages(&quot;gtExtras&quot;) library(gtExtras) # create aggregated dataset agg_iris = iris %&gt;% group_by(Species) %&gt;% summarize( Sepal.L = list(Sepal.Length), Sepal.W = list(Sepal.Width), Petal.L = list(Petal.Length), Petal.W = list(Petal.Width) ) agg_lexdec = lexdec %&gt;% select( NativeLanguage, RT, Frequency, FamilySize)%&gt;% group_by(NativeLanguage)%&gt;% summarize( RT= list(RT), Frequency = list(Frequency), FamilySize = list(FamilySize) ) # display the table with default output with gt package agg_lexdec %&gt;% gt() #hjyhruiggz table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #hjyhruiggz thead, #hjyhruiggz tbody, #hjyhruiggz tfoot, #hjyhruiggz tr, #hjyhruiggz td, #hjyhruiggz th { border-style: none; } #hjyhruiggz p { margin: 0; padding: 0; } #hjyhruiggz .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hjyhruiggz .gt_caption { padding-top: 4px; padding-bottom: 4px; } #hjyhruiggz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hjyhruiggz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #hjyhruiggz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hjyhruiggz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hjyhruiggz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hjyhruiggz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hjyhruiggz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hjyhruiggz .gt_column_spanner_outer:first-child { padding-left: 0; } #hjyhruiggz .gt_column_spanner_outer:last-child { padding-right: 0; } #hjyhruiggz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #hjyhruiggz .gt_spanner_row { border-bottom-style: hidden; } #hjyhruiggz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #hjyhruiggz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hjyhruiggz .gt_from_md > :first-child { margin-top: 0; } #hjyhruiggz .gt_from_md > :last-child { margin-bottom: 0; } #hjyhruiggz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hjyhruiggz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #hjyhruiggz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #hjyhruiggz .gt_row_group_first td { border-top-width: 2px; } #hjyhruiggz .gt_row_group_first th { border-top-width: 2px; } #hjyhruiggz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hjyhruiggz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #hjyhruiggz .gt_first_summary_row.thick { border-top-width: 2px; } #hjyhruiggz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hjyhruiggz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hjyhruiggz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hjyhruiggz .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #hjyhruiggz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hjyhruiggz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hjyhruiggz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hjyhruiggz .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hjyhruiggz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hjyhruiggz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hjyhruiggz .gt_left { text-align: left; } #hjyhruiggz .gt_center { text-align: center; } #hjyhruiggz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hjyhruiggz .gt_font_normal { font-weight: normal; } #hjyhruiggz .gt_font_bold { font-weight: bold; } #hjyhruiggz .gt_font_italic { font-style: italic; } #hjyhruiggz .gt_super { font-size: 65%; } #hjyhruiggz .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #hjyhruiggz .gt_asterisk { font-size: 100%; vertical-align: 0; } #hjyhruiggz .gt_indent_1 { text-indent: 5px; } #hjyhruiggz .gt_indent_2 { text-indent: 10px; } #hjyhruiggz .gt_indent_3 { text-indent: 15px; } #hjyhruiggz .gt_indent_4 { text-indent: 20px; } #hjyhruiggz .gt_indent_5 { text-indent: 25px; } NativeLanguage RT Frequency FamilySize English 6.340359, 6.308098, 6.349139, 6.186209, 6.025866, 6.180017, 6.035481, 6.263398, 6.459904, 6.184149, 6.137727, 6.242223, 6.003887, 6.035481, 6.322565, 5.988961, 6.285998, 6.381816, 5.958425, 6.124683, 6.741701, 6.821107, 6.406880, 6.421622, 6.320768, 6.208590, 6.214608, 7.114769, 6.091310, 6.208590, 6.366470, 6.582025, 6.514713, 6.352629, 6.278521, 5.988961, 6.079933, 6.126869, 6.284134, 6.089045, 6.383507, 6.220590, 6.381816, 6.202536, 6.356108, 6.107023, 6.517671, 6.317165, 6.086775, 6.163315, 6.673298, 6.434547, 6.333280, 6.202536, 6.111467, 6.326149, 6.903747, 6.306275, 6.118097, 6.472346, 6.697034, 6.148468, 6.599870, 6.775366, 6.366470, 6.274762, 6.023448, 6.139885, 6.293419, 6.021023, 6.188264, 6.194405, 5.899897, 6.364751, 5.973810, 6.037871, 5.973810, 6.282267, 6.040255, 6.329721, 6.208590, 6.270988, 6.450470, 6.366470, 6.293419, 6.198479, 6.154858, 6.208590, 6.156979, 6.146329, 5.942799, 5.993961, 6.167516, 6.013715, 6.161207, 6.146329, 6.210600, 6.163315, 6.831954, 6.154858, 6.210600, 6.220590, 6.253829, 6.082219, 6.289716, 6.068426, 6.152733, 6.192362, 6.721426, 6.222576, 6.154858, 6.538140, 6.025866, 5.942799, 6.086775, 6.208590, 6.152733, 6.220590, 6.280396, 6.159095, 5.993961, 6.641182, 6.248043, 6.230481, 6.163315, 5.966147, 6.434547, 6.077642, 5.902633, 6.249975, 6.095825, 6.169611, 6.214608, 6.345636, 6.363028, 6.169611, 6.278521, 6.115892, 6.285998, 6.251904, 6.175867, 6.113682, 6.284134, 6.013715, 6.075346, 6.104793, 5.828946, 6.522093, 6.236370, 6.489205, 6.700731, 6.466145, 6.257668, 6.315358, 6.308098, 6.236370, 6.198479, 6.129050, 6.533789, 6.538140, 6.246107, 6.416732, 6.284134, 6.473891, 6.269096, 6.398595, 6.196444, 6.333280, 6.173786, 6.173786, 6.180017, 6.386879, 6.499787, 6.173786, 6.070738, 6.304449, 6.208590, 6.395262, 6.483107, 6.293419, 6.293419, 6.272877, 6.496775, 6.255750, 6.234411, 6.380123, 6.306275, 6.259581, 6.249975, 6.280396, 6.487684, 6.586172, 6.411818, 6.366470, 6.214608, 6.276643, 6.326149, 6.270988, 6.190315, 6.115892, 6.285998, 6.295266, 6.287859, 6.284134, 6.236370, 6.186209, 6.436150, 6.369901, 6.152733, 6.146329, 6.440947, 6.173786, 6.331502, 6.486161, 6.171701, 6.324359, 6.679599, 6.311735, 6.423247, 6.591674, 6.405228, 6.338594, 6.418365, 6.376727, 6.163315, 6.469250, 6.265301, 6.385194, 6.493754, 6.426488, 6.171701, 6.180017, 6.293419, 6.304449, 6.364751, 6.246107, 6.234411, 6.011267, 6.075346, 6.131226, 6.109248, 6.311735, 5.998937, 6.144186, 6.100319, 6.104793, 6.070738, 6.216606, 6.001415, 6.075346, 6.204558, 6.070738, 6.230481, 6.352629, 6.297109, 6.434547, 6.318968, 6.142037, 6.129050, 6.309918, 5.950643, 5.852202, 6.133398, 6.192362, 6.052089, 6.152733, 6.111467, 6.156979, 5.894403, 6.152733, 6.120297, 6.169611, 6.037871, 6.326149, 6.208590, 6.224558, 6.426488, 6.352629, 6.182085, 6.188264, 6.711740, 6.196444, 6.194405, 6.284134, 6.137727, 6.188264, 6.580639, 6.300786, 6.077642, 6.329721, 6.079933, 6.165418, 6.456770, 6.190315, 6.354370, 6.357842, 6.529419, 6.324359, 6.313548, 6.240276, 5.988961, 6.033086, 5.953243, 6.037871, 6.152733, 6.095825, 6.077642, 6.077642, 6.016157, 6.177944, 6.493754, 6.289716, 6.486161, 6.122493, 6.091310, 6.398595, 6.061457, 6.107023, 6.322565, 6.357842, 6.120297, 6.100319, 6.063785, 6.267201, 6.137727, 6.200509, 6.214608, 6.391917, 6.095825, 6.163315, 6.287859, 6.150603, 6.126869, 6.102559, 6.148468, 6.066108, 6.150603, 6.040255, 6.075346, 6.113682, 6.144186, 6.291569, 6.196444, 6.356108, 6.156979, 6.011267, 6.390241, 6.173786, 6.137727, 6.035481, 6.066108, 6.347389, 6.126869, 6.244167, 6.148468, 6.263398, 6.122493, 6.037871, 6.182085, 6.309918, 6.244167, 6.118097, 6.175867, 6.184149, 6.135565, 6.066108, 6.054439, 7.054450, 6.244167, 6.077642, 6.113682, 6.139885, 6.139885, 6.280396, 6.089045, 6.077642, 6.033086, 5.940171, 6.326149, 6.056784, 6.309918, 6.084499, 6.173786, 6.156979, 6.175867, 6.070738, 6.056784, 6.293419, 6.122493, 5.998937, 6.131226, 6.154858, 6.161207, 6.302619, 6.329721, 6.194405, 6.180017, 6.406880, 6.276643, 6.163315, 6.336826, 6.257668, 6.198479, 6.133398, 6.180017, 6.161207, 5.981414, 6.177944, 6.234411, 6.390241, 6.274762, 6.198479, 6.202536, 6.234411, 6.276643, 6.115892, 6.313548, 6.282267, 6.650279, 6.333280, 6.391917, 6.144186, 6.244167, 6.212606, 6.514713, 6.502790, 6.289716, 6.408529, 6.293419, 6.280396, 6.278521, 6.131226, 7.425358, 6.445720, 6.633318, 6.935370, 6.396930, 6.699500, 6.298949, 6.315358, 6.272877, 6.200509, 6.322565, 6.263398, 6.232448, 6.368187, 6.171701, 6.562444, 6.335054, 6.228511, 6.144186, 6.226537, 6.265301, 6.361302, 6.514713, 6.297109, 6.280396, 6.525030, 6.173786, 6.291569, 6.315358, 6.287859, 6.165418, 6.300786, 6.091310, 6.135565, 6.274762, 6.302619, 6.490724, 6.212606, 6.200509, 6.171701, 6.025866, 6.263398, 6.410175, 6.423247, 6.510258, 6.349139, 6.513230, 6.398595, 6.410175, 6.359574, 6.302619, 6.669498, 6.447306, 6.618739, 6.352629, 6.541030, 6.287859, 6.390241, 6.405228, 6.356108, 6.311735, 6.315358, 6.393591, 6.551080, 6.582025, 6.642487, 6.927558, 6.405228, 6.666957, 6.646391, 6.591674, 6.501290, 6.536692, 6.434547, 6.501290, 6.347389, 6.461468, 6.565265, 6.416732, 6.352629, 6.532334, 6.584791, 6.398595, 6.272877, 6.584791, 6.385194, 6.408529, 6.720220, 6.690842, 6.505784, 6.424869, 6.369901, 6.492240, 6.822197, 6.467699, 6.238325, 6.378426, 6.461468, 6.552508, 6.655440, 6.300786, 6.903747, 6.605298, 6.447306, 6.577861, 6.388561, 6.396930, 6.401917, 6.380123, 6.584791, 6.419995, 6.475433, 6.322565, 6.632002, 6.284134, 6.561031, 6.478510, 6.350886, 6.142037, 6.476972, 6.331502, 6.799056, 6.345636, 6.265301, 6.047372, 6.302619, 6.693324, 6.693324, 6.835185, 6.329721, 6.293419, 6.345636, 6.329721, 6.126869, 6.473891, 6.635947, 6.188264, 6.234411, 6.313548, 6.212606, 6.591674, 6.687109, 6.871091, 6.614726, 6.582025, 6.533789, 6.492240, 6.400257, 6.393591, 6.472346, 6.150603, 6.626718, 6.218600, 6.472346, 6.347389, 6.480045, 6.347389, 6.884487, 6.481577, 6.335054, 6.202536, 6.375025, 6.381816, 6.175867, 6.543912, 6.180017, 6.569481, 6.331502, 6.059123, 6.150603, 6.356108, 6.584791, 6.573680, 6.274762, 6.375025, 6.583409, 6.498282, 6.461468, 6.376727, 6.469250, 6.263398, 6.163315, 6.184149, 6.406880, 6.175867, 6.226537, 6.276643, 6.464588, 6.139885, 6.196444, 6.660575, 6.447306, 6.393591, 6.388561, 6.165418, 6.481577, 6.253829, 6.267201, 6.343880, 6.167516, 6.118097, 6.161207, 6.255750, 6.230481, 6.234411, 6.285998, 6.236370, 6.142037, 6.224558, 6.329721, 6.663133, 6.357842, 6.605298, 6.418365, 6.921658, 6.973543, 6.190315, 6.265301, 6.452049, 6.282267, 6.869014, 6.291569, 6.926577, 6.508769, 6.543912, 6.383507, 7.142037, 6.638568, 6.324359, 6.436150, 6.385194, 6.472346, 6.347389, 6.621406, 6.238325, 6.261492, 6.309918, 6.315358, 6.398595, 6.665684, 6.423247, 6.458338, 6.535241, 6.315358, 6.308098, 6.190315, 6.388561, 6.347389, 6.240276, 6.228511, 6.169611, 6.318968, 6.142037, 6.661855, 6.375025, 6.109248, 6.359574, 6.388561, 6.192362, 6.198479, 6.378426, 6.280396, 6.148468, 6.416732, 6.356108, 6.291569, 6.356108, 6.089045, 6.188264, 6.135565, 6.025866, 6.582025, 6.120297, 6.400257, 6.218600, 6.429719, 6.318968, 6.612041, 6.391917, 6.204558, 6.240276, 6.375025, 6.715383, 6.357842, 6.300786, 6.326149, 6.413459, 6.342121, 6.177944, 6.295266, 6.748760, 6.680855, 6.276643, 6.450470, 6.593045, 6.188264, 6.396930, 6.507278, 6.340359, 6.381816, 6.452049, 6.249975, 6.306275, 6.745236, 6.452049, 6.278521, 6.113682, 6.326149, 7.125283, 6.261492, 6.424869, 6.487684, 6.472346, 6.393591, 6.701960, 6.406880, 6.253829, 6.562444, 6.003887, 6.196444, 6.381816, 6.338594, 6.291569, 6.329721, 6.361302, 6.212606, 6.369901, 6.424869, 6.597146, 6.192362, 6.450470, 6.324359, 6.424869, 6.317165, 6.084499, 6.326149, 6.584791, 6.335054, 6.496775, 6.388561, 6.269096, 6.184149, 6.188264, 6.200509, 6.309918, 6.388561, 6.517671, 6.408529, 6.073045, 6.318968, 6.456770, 6.333280, 6.180017, 6.214608, 6.287859, 6.102559, 6.293419, 6.173786, 6.483107, 6.329721, 6.381816, 6.352629, 6.278521, 6.218600, 6.238325, 6.570883, 6.347389, 6.208590, 6.244167, 6.177944, 6.202536, 6.380123, 6.146329, 6.137727, 6.561031, 6.381816, 6.526495, 6.366470, 6.343880, 6.289716, 6.329721, 6.152733, 6.180017, 6.338594, 6.146329, 6.253829, 6.182085, 6.129050, 6.224558, 6.144186, 6.109248, 6.186209, 6.202536, 6.450470, 6.202536, 6.278521, 6.322565, 6.242223, 6.061457, 6.122493, 6.180017, 6.139885, 6.466145, 6.045005, 6.510258, 6.282267, 6.313548, 6.198479, 6.226537, 6.480045, 6.232448, 6.317165, 6.242223, 6.216606, 6.122493, 6.270988, 6.142037, 6.212606, 6.188264, 6.216606, 6.150603, 6.249975, 6.224558, 6.200509, 6.234411, 6.133398, 6.309918, 6.357842, 6.304449, 6.228511, 6.269096, 6.248043, 6.452049, 6.242223, 6.270988, 6.342121, 6.198479, 6.625392, 6.526495, 6.416732, 6.401917, 6.444131, 6.336826, 6.194405, 6.478510, 6.990257, 6.579251, 6.499787, 6.395262, 6.802395, 6.526495, 6.606650, 6.634633, 6.569481, 6.498282, 6.436150, 6.532334, 6.434547, 6.395262, 6.591674, 6.577861, 6.533789, 6.559615, 6.790097, 6.815640, 6.828712, 6.401917, 6.693324, 6.354370, 6.507278, 6.527958, 6.679599, 7.009409, 6.300786, 6.431331, 6.744059, 6.533789, 6.423247, 6.335054, 6.642487, 6.495266, 6.410175, 6.190315, 6.289716, 6.159095, 6.352629, 6.276643, 6.234411, 6.318968, 6.342121, 6.297109, 6.363028, 6.345636, 6.318968, 6.257668, 6.276643, 6.380123, 6.263398, 6.317165, 6.381816, 6.652863, 6.774224, 6.593045, 6.436150, 6.272877, 6.206576, 6.142037, 6.710523, 6.380123, 6.502790, 6.184149, 6.426488, 6.220590, 6.196444, 6.463029, 6.336826 4.859812, 4.605170, 4.997212, 4.727388, 7.667626, 4.060443, 4.753590, 4.709530, 3.044522, 4.204693, 5.298317, 4.043051, 2.484907, 5.631212, 5.700444, 6.228511, 1.791759, 5.545177, 4.595120, 5.918894, 4.442651, 2.995732, 3.044522, 5.267858, 3.970292, 4.976734, 4.615121, 4.624973, 4.844187, 5.192957, 5.652489, 6.461468, 5.129899, 3.555348, 3.663562, 5.805135, 4.007333, 3.988984, 4.812184, 6.577861, 4.454347, 2.079442, 4.418841, 6.660575, 6.109248, 5.880533, 4.248495, 6.120297, 5.541264, 2.833213, 3.828641, 3.332205, 3.663562, 5.023881, 6.098074, 6.063785, 3.637586, 5.117994, 6.304449, 3.044522, 4.682131, 7.086738, 2.484907, 5.347108, 3.332205, 3.951244, 7.771910, 5.537334, 2.890372, 2.708050, 6.378426, 3.433987, 6.599870, 5.214936, 5.587249, 4.962845, 4.499810, 4.127134, 5.783825, 7.667626, 3.828641, 3.951244, 4.007333, 4.499810, 6.109248, 5.587249, 5.298317, 5.541264, 4.812184, 4.418841, 6.228511, 3.637586, 4.844187, 3.970292, 4.442651, 3.044522, 5.631212, 6.599870, 6.098074, 2.484907, 2.708050, 4.997212, 4.043051, 4.859812, 3.663562, 5.805135, 3.044522, 5.700444, 3.332205, 2.890372, 5.880533, 4.248495, 3.433987, 4.615121, 5.129899, 4.454347, 5.117994, 5.545177, 2.833213, 1.791759, 4.753590, 3.044522, 4.060443, 3.663562, 3.332205, 7.771910, 4.605170, 5.347108, 5.267858, 4.727388, 6.120297, 5.918894, 4.682131, 5.783825, 5.023881, 6.461468, 4.976734, 6.378426, 3.988984, 6.577861, 4.127134, 5.214936, 2.995732, 2.484907, 4.595120, 5.537334, 6.660575, 4.709530, 6.063785, 4.962845, 2.079442, 4.624973, 3.555348, 4.204693, 5.652489, 5.192957, 7.086738, 6.304449, 4.248495, 6.109248, 3.044522, 3.433987, 4.418841, 5.023881, 5.298317, 5.541264, 5.880533, 6.599870, 5.783825, 4.682131, 7.086738, 1.791759, 2.079442, 5.918894, 5.700444, 4.844187, 3.970292, 3.988984, 4.962845, 4.753590, 6.577861, 4.499810, 4.624973, 6.098074, 6.228511, 3.663562, 3.828641, 6.660575, 6.304449, 4.442651, 3.637586, 5.347108, 5.652489, 6.063785, 2.708050, 3.555348, 4.976734, 6.120297, 5.587249, 4.727388, 5.214936, 2.484907, 4.605170, 4.454347, 6.378426, 5.631212, 7.771910, 4.709530, 5.267858, 3.332205, 4.204693, 3.663562, 4.595120, 3.951244, 5.537334, 7.667626, 5.129899, 3.044522, 6.461468, 3.044522, 2.484907, 4.060443, 4.812184, 5.805135, 4.615121, 4.043051, 2.833213, 5.545177, 3.332205, 2.995732, 4.859812, 4.997212, 5.117994, 2.890372, 4.007333, 4.127134, 5.192957, 4.859812, 6.599870, 4.248495, 6.378426, 4.060443, 6.098074, 6.577861, 4.682131, 3.433987, 3.044522, 3.044522, 4.007333, 5.652489, 5.192957, 4.844187, 4.812184, 4.454347, 5.918894, 5.267858, 3.332205, 3.663562, 6.063785, 2.890372, 4.727388, 5.631212, 4.709530, 2.079442, 5.587249, 5.783825, 7.086738, 6.228511, 6.461468, 6.109248, 3.828641, 4.962845, 3.332205, 4.595120, 3.988984, 2.995732, 4.499810, 2.484907, 4.615121, 6.304449, 4.976734, 2.708050, 5.805135, 4.043051, 5.117994, 5.537334, 3.951244, 3.663562, 5.541264, 5.129899, 7.771910, 5.880533, 4.753590, 4.997212, 5.700444, 3.970292, 3.555348, 4.127134, 4.204693, 3.044522, 5.545177, 6.660575, 4.605170, 5.298317, 2.833213, 6.120297, 5.023881, 1.791759, 5.214936, 2.484907, 3.637586, 4.418841, 7.667626, 4.442651, 5.347108, 4.624973, 4.727388, 3.951244, 4.204693, 3.970292, 5.117994, 6.109248, 2.995732, 4.043051, 5.545177, 5.880533, 5.587249, 4.418841, 5.783825, 5.805135, 3.637586, 7.771910, 6.120297, 6.098074, 4.997212, 4.962845, 5.267858, 4.605170, 5.129899, 6.304449, 4.615121, 6.063785, 5.918894, 6.599870, 5.537334, 2.079442, 4.624973, 4.812184, 5.541264, 3.044522, 4.976734, 3.044522, 6.461468, 4.844187, 6.378426, 3.332205, 3.988984, 3.828641, 4.442651, 4.499810, 2.484907, 4.454347, 4.060443, 3.663562, 2.833213, 7.086738, 3.044522, 5.023881, 5.214936, 3.555348, 4.709530, 6.228511, 5.631212, 2.890372, 4.859812, 6.577861, 5.652489, 4.595120, 4.248495, 3.332205, 3.663562, 1.791759, 6.660575, 2.708050, 5.298317, 5.700444, 5.347108, 2.484907, 3.433987, 7.667626, 5.192957, 4.753590, 4.007333, 4.127134, 4.682131, 5.700444, 5.347108, 2.484907, 5.783825, 5.267858, 3.433987, 6.109248, 3.332205, 4.844187, 5.652489, 4.060443, 4.204693, 4.615121, 4.859812, 4.962845, 4.976734, 6.228511, 4.442651, 5.023881, 3.828641, 3.637586, 3.663562, 3.663562, 3.951244, 4.043051, 6.098074, 6.599870, 2.079442, 3.988984, 2.995732, 4.418841, 4.454347, 4.709530, 4.997212, 3.332205, 6.304449, 4.812184, 3.555348, 6.461468, 4.248495, 4.727388, 5.541264, 4.605170, 4.682131, 4.499810, 5.192957, 5.545177, 5.214936, 3.044522, 4.127134, 7.667626, 2.833213, 5.918894, 6.660575, 5.805135, 5.117994, 5.880533, 4.624973, 6.120297, 3.044522, 3.970292, 7.771910, 7.086738, 4.595120, 5.587249, 4.753590, 5.298317, 6.577861, 5.631212, 2.708050, 2.890372, 3.044522, 1.791759, 6.378426, 5.537334, 6.063785, 2.484907, 5.129899, 4.007333, 5.298317, 5.023881, 2.890372, 5.880533, 3.637586, 6.120297, 4.753590, 3.332205, 5.631212, 6.109248, 4.007333, 4.615121, 7.086738, 4.043051, 5.541264, 5.537334, 6.063785, 6.599870, 3.828641, 5.918894, 3.555348, 5.783825, 4.962845, 6.378426, 2.484907, 6.660575, 4.127134, 7.771910, 3.044522, 5.545177, 4.976734, 6.577861, 4.682131, 4.442651, 2.708050, 4.844187, 3.044522, 4.997212, 3.970292, 4.624973, 3.988984, 5.700444, 4.595120, 5.347108, 4.727388, 4.499810, 2.484907, 6.461468, 6.098074, 5.652489, 6.228511, 3.663562, 3.332205, 5.587249, 4.859812, 4.709530, 4.418841, 2.079442, 7.667626, 4.812184, 4.248495, 5.117994, 4.454347, 2.833213, 3.951244, 5.192957, 5.129899, 3.044522, 2.995732, 5.214936, 4.605170, 4.060443, 1.791759, 3.663562, 4.204693, 5.805135, 6.304449, 3.433987, 5.267858, 6.109248, 5.631212, 3.637586, 7.667626, 5.298317, 4.624973, 4.605170, 4.248495, 4.204693, 5.537334, 6.098074, 4.043051, 3.332205, 3.970292, 4.442651, 5.117994, 4.844187, 4.595120, 6.228511, 5.587249, 5.214936, 2.484907, 3.663562, 4.060443, 3.044522, 5.541264, 4.812184, 3.433987, 6.577861, 5.545177, 3.663562, 3.332205, 4.859812, 3.828641, 5.129899, 5.700444, 4.615121, 4.976734, 4.997212, 5.805135, 7.086738, 3.951244, 5.918894, 4.127134, 3.044522, 2.708050, 6.304449, 6.378426, 5.783825, 3.044522, 4.682131, 5.267858, 4.727388, 6.063785, 6.461468, 2.079442, 6.120297, 3.988984, 4.962845, 6.599870, 6.660575, 7.771910, 4.418841, 5.023881, 1.791759, 2.890372, 2.833213, 5.880533, 5.192957, 4.454347, 2.484907, 2.995732, 3.555348, 4.007333, 4.499810, 4.753590, 5.652489, 4.709530, 5.347108, 5.700444, 6.304449, 2.708050, 3.951244, 1.791759, 4.624973, 4.060443, 4.753590, 4.844187, 5.545177, 5.805135, 4.605170, 4.709530, 6.577861, 2.995732, 4.248495, 4.976734, 3.332205, 2.484907, 2.833213, 3.663562, 5.783825, 3.970292, 5.117994, 3.663562, 3.828641, 4.615121, 2.484907, 4.595120, 7.771910, 5.023881, 3.988984, 3.044522, 5.214936, 6.378426, 5.587249, 4.454347, 5.918894, 7.667626, 5.537334, 4.442651, 5.880533, 4.007333, 5.652489, 5.267858, 5.347108, 3.044522, 4.682131, 4.997212, 4.812184, 6.120297, 3.044522, 6.109248, 3.433987, 5.298317, 6.098074, 4.859812, 6.660575, 2.890372, 4.043051, 5.631212, 5.541264, 4.962845, 4.499810, 6.228511, 4.204693, 4.727388, 5.129899, 3.555348, 6.063785, 6.599870, 3.332205, 6.461468, 4.418841, 5.192957, 3.637586, 7.086738, 2.079442, 4.127134, 6.599870, 7.086738, 3.433987, 5.545177, 2.833213, 1.791759, 4.204693, 5.117994, 4.753590, 3.044522, 5.805135, 3.044522, 2.079442, 5.267858, 4.060443, 2.890372, 6.098074, 4.844187, 3.637586, 5.587249, 4.007333, 5.214936, 5.783825, 6.228511, 3.663562, 4.442651, 2.995732, 2.484907, 5.298317, 4.043051, 5.631212, 4.127134, 3.828641, 6.378426, 4.859812, 5.700444, 3.332205, 7.771910, 3.332205, 6.461468, 4.976734, 5.129899, 4.499810, 3.970292, 6.304449, 3.988984, 7.667626, 5.652489, 4.624973, 4.418841, 6.577861, 4.682131, 5.541264, 3.663562, 3.555348, 5.918894, 5.347108, 4.248495, 3.044522, 4.709530, 4.605170, 5.023881, 2.708050, 4.595120, 5.880533, 6.109248, 4.962845, 2.484907, 4.615121, 6.120297, 4.727388, 4.454347, 6.063785, 3.951244, 4.812184, 5.192957, 4.997212, 6.660575, 5.537334, 5.192957, 5.783825, 6.660575, 3.988984, 3.433987, 5.918894, 2.833213, 3.637586, 4.962845, 6.599870, 4.499810, 5.805135, 4.753590, 3.044522, 6.063785, 5.541264, 4.624973, 3.332205, 3.555348, 2.890372, 4.204693, 6.304449, 5.347108, 2.708050, 3.044522, 4.454347, 4.060443, 3.951244, 5.537334, 5.117994, 4.812184, 6.098074, 5.587249, 3.332205, 7.086738, 2.079442, 4.682131, 4.859812, 2.484907, 5.298317, 6.228511, 4.595120, 5.023881, 5.129899, 4.043051, 6.577861, 5.652489, 3.828641, 6.109248, 3.970292, 4.007333, 4.997212, 4.248495, 3.044522, 5.214936, 3.663562, 5.700444, 4.727388, 7.771910, 2.995732, 6.120297, 4.418841, 4.605170, 4.709530, 5.267858, 7.667626, 6.461468, 5.880533, 5.631212, 4.442651, 4.127134, 5.545177, 4.844187, 2.484907, 3.663562, 4.615121, 1.791759, 4.976734, 6.378426, 3.433987, 2.484907, 4.709530, 5.537334, 2.890372, 5.023881, 7.086738, 5.541264, 6.461468, 4.060443, 4.859812, 5.631212, 3.044522, 5.918894, 5.192957, 3.637586, 4.605170, 4.976734, 5.347108, 5.652489, 4.442651, 4.812184, 2.484907, 4.454347, 6.577861, 6.098074, 4.248495, 3.044522, 4.615121, 6.120297, 4.753590, 5.267858, 5.129899, 5.117994, 2.708050, 4.682131, 5.700444, 3.970292, 3.663562, 4.127134, 4.418841, 3.663562, 4.624973, 7.771910, 5.545177, 5.783825, 7.667626, 4.962845, 3.988984, 1.791759, 6.378426, 4.595120, 2.995732, 5.880533, 3.332205, 5.214936, 3.332205, 6.304449, 6.228511, 6.109248, 6.063785, 4.844187, 3.044522, 2.833213, 5.805135, 3.828641, 4.499810, 4.204693, 5.587249, 3.951244, 2.079442, 4.007333, 4.043051, 6.599870, 3.555348, 6.660575, 4.727388, 5.298317, 4.997212 1.3862944, 1.0986123, 0.6931472, 0.0000000, 3.1354942, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 1.7917595, 1.9459101, 0.0000000, 0.0000000, 0.6931472, 2.1972246, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 1.7917595, 0.0000000, 2.0794415, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 0.0000000, 2.3025851, 0.0000000, 0.0000000, 1.6094379, 2.7725887, 0.6931472, 0.6931472, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 1.3862944, 2.7725887, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 3.3322045, 0.6931472, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 1.3862944, 0.0000000, 1.0986123, 0.0000000, 3.1354942, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 1.0986123, 0.0000000, 0.0000000, 1.6094379, 1.9459101, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 1.0986123, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.3862944, 0.0000000, 1.9459101, 0.0000000, 1.7917595, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 2.0794415, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 3.3322045, 1.0986123, 1.3862944, 1.7917595, 0.0000000, 1.6094379, 2.1972246, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 2.3025851, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 2.7725887, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 1.0986123, 2.7725887, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 1.3862944, 2.7725887, 0.0000000, 0.0000000, 2.1972246, 1.7917595, 0.6931472, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 2.3025851, 0.0000000, 0.0000000, 1.0986123, 1.9459101, 0.0000000, 0.0000000, 2.7725887, 1.0986123, 0.0000000, 0.0000000, 1.3862944, 1.7917595, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 1.6094379, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.3862944, 0.6931472, 3.3322045, 0.0000000, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 3.1354942, 2.0794415, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.6931472, 1.0986123, 0.0000000, 0.0000000, 1.0986123, 1.0986123, 1.3862944, 1.0986123, 0.0000000, 1.3862944, 0.6931472, 1.0986123, 2.3025851, 1.3862944, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.7917595, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 2.1972246, 1.7917595, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 2.7725887, 1.9459101, 0.0000000, 0.6931472, 0.0000000, 1.3862944, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 1.9459101, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 2.0794415, 3.3322045, 0.6931472, 0.0000000, 0.6931472, 1.7917595, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 1.0986123, 1.0986123, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.6094379, 3.1354942, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 1.6094379, 0.0000000, 1.9459101, 0.0000000, 3.3322045, 1.6094379, 1.0986123, 0.6931472, 1.3862944, 1.7917595, 1.0986123, 2.0794415, 1.0986123, 0.0000000, 0.6931472, 2.1972246, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 2.7725887, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.9459101, 0.6931472, 0.0000000, 1.3862944, 2.3025851, 1.7917595, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 1.0986123, 1.7917595, 1.3862944, 0.0000000, 0.0000000, 3.1354942, 1.0986123, 0.0000000, 0.0000000, 1.0986123, 1.3862944, 1.7917595, 1.3862944, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 1.7917595, 0.6931472, 0.0000000, 0.0000000, 1.3862944, 1.3862944, 0.6931472, 1.9459101, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.3862944, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 3.1354942, 0.0000000, 2.1972246, 2.7725887, 1.9459101, 1.0986123, 0.6931472, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 3.3322045, 2.7725887, 0.6931472, 1.0986123, 0.0000000, 1.0986123, 2.3025851, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.6931472, 0.6931472, 0.0000000, 2.0794415, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 1.0986123, 0.0000000, 2.1972246, 0.0000000, 0.0000000, 1.3862944, 1.3862944, 0.0000000, 2.7725887, 1.0986123, 3.3322045, 0.0000000, 0.0000000, 0.6931472, 2.3025851, 1.3862944, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.7917595, 1.9459101, 0.0000000, 0.0000000, 1.0986123, 1.3862944, 0.0000000, 1.6094379, 0.0000000, 3.1354942, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 2.0794415, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.9459101, 1.0986123, 0.0000000, 1.7917595, 0.6931472, 0.6931472, 0.0000000, 3.1354942, 1.0986123, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.6931472, 1.9459101, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 2.3025851, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 2.0794415, 1.7917595, 0.0000000, 0.6931472, 0.6931472, 1.9459101, 2.7725887, 0.0000000, 2.1972246, 1.0986123, 0.0000000, 0.0000000, 1.0986123, 1.3862944, 0.0000000, 0.0000000, 1.3862944, 1.7917595, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 1.3862944, 1.0986123, 2.7725887, 3.3322045, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 1.3862944, 1.7917595, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 1.9459101, 1.0986123, 0.0000000, 2.3025851, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 3.3322045, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 1.0986123, 0.0000000, 2.1972246, 3.1354942, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 1.7917595, 1.7917595, 1.3862944, 0.0000000, 1.3862944, 0.6931472, 0.0000000, 1.6094379, 0.6931472, 0.6931472, 0.0000000, 1.0986123, 1.0986123, 1.3862944, 2.7725887, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.3862944, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 2.0794415, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 0.0000000, 1.6094379, 1.0986123, 0.0000000, 2.7725887, 0.0000000, 1.0986123, 1.0986123, 2.7725887, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 1.7917595, 0.6931472, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 1.3862944, 1.3862944, 1.7917595, 0.0000000, 3.3322045, 0.0000000, 0.0000000, 0.6931472, 2.0794415, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 3.1354942, 1.7917595, 0.0000000, 1.6094379, 2.3025851, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 2.1972246, 1.3862944, 0.0000000, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 2.7725887, 0.6931472, 1.0986123, 0.0000000, 2.7725887, 0.0000000, 0.0000000, 2.1972246, 0.0000000, 0.0000000, 1.3862944, 1.0986123, 0.0000000, 1.9459101, 0.0000000, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 2.7725887, 0.0000000, 1.3862944, 1.3862944, 0.0000000, 1.0986123, 1.9459101, 0.6931472, 0.0000000, 2.0794415, 0.0000000, 2.3025851, 1.7917595, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 3.3322045, 0.0000000, 1.6094379, 1.6094379, 1.0986123, 0.0000000, 1.7917595, 3.1354942, 0.0000000, 0.6931472, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.6931472, 0.0000000, 2.1972246, 1.0986123, 0.0000000, 1.0986123, 0.6931472, 1.3862944, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 2.3025851, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 1.6094379, 0.0000000, 1.7917595, 2.0794415, 1.0986123, 0.0000000, 1.3862944, 1.7917595, 0.0000000, 0.0000000, 1.0986123, 1.6094379, 0.0000000, 0.0000000, 3.3322045, 0.0000000, 0.0000000, 3.1354942, 1.3862944, 0.0000000, 0.0000000, 1.3862944, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.9459101, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 2.7725887, 0.0000000, 1.0986123, 0.6931472 Other 6.410175, 6.315358, 6.385194, 6.354370, 6.716595, 6.261492, 6.304449, 6.408529, 6.331502, 6.401917, 6.313548, 6.483107, 6.327937, 6.234411, 6.366470, 6.327937, 6.472346, 6.376727, 6.383507, 6.366470, 6.510258, 6.253829, 6.263398, 6.401917, 6.280396, 6.445720, 6.317165, 6.356108, 6.238325, 6.804615, 6.376727, 6.240276, 6.194405, 6.447306, 6.359574, 6.437752, 6.302619, 6.263398, 6.251904, 6.333280, 6.171701, 6.289716, 6.255750, 6.253829, 6.487684, 6.452049, 6.405228, 6.242223, 6.437752, 6.375025, 6.475433, 6.452049, 6.415097, 6.378426, 6.444131, 6.361302, 6.551080, 7.044033, 6.459904, 6.410175, 6.361302, 6.242223, 6.458338, 7.132498, 6.388561, 6.419995, 6.432940, 6.210600, 6.390241, 6.605298, 6.459904, 6.416732, 6.495266, 6.278521, 6.421622, 6.434547, 6.466145, 6.442540, 6.380123, 6.232448, 6.349139, 6.375025, 6.315358, 6.440947, 6.284134, 6.300786, 6.458338, 6.350886, 6.380123, 6.272877, 6.287859, 6.466145, 6.539586, 6.472346, 6.333280, 6.602588, 6.437752, 6.122493, 6.343880, 6.122493, 6.084499, 6.175867, 6.226537, 5.993961, 6.320768, 6.588926, 6.214608, 6.434547, 6.313548, 6.244167, 6.448889, 6.261492, 6.309918, 6.514713, 6.265301, 6.265301, 6.419995, 6.361302, 6.378426, 6.492240, 6.295266, 6.368187, 6.287859, 6.309918, 6.395262, 6.165418, 6.350886, 6.599870, 6.198479, 6.297109, 6.311735, 6.594413, 6.352629, 6.406880, 6.467699, 6.333280, 6.956545, 6.442540, 6.545350, 6.244167, 6.771936, 6.395262, 6.513230, 6.552508, 6.444131, 6.415097, 6.599870, 6.455199, 6.276643, 6.983790, 6.658011, 6.790097, 6.741701, 6.572283, 6.352629, 6.603944, 6.507278, 6.719013, 6.194405, 6.566672, 6.236370, 6.495266, 6.091310, 6.093570, 6.295266, 6.363028, 6.066108, 6.102559, 6.469250, 6.259581, 7.135687, 5.973810, 5.905362, 6.263398, 6.030685, 6.431331, 6.483107, 6.194405, 6.061457, 6.368187, 6.282267, 6.320768, 6.329721, 6.390241, 6.368187, 6.383507, 6.848005, 6.146329, 6.091310, 6.167516, 6.238325, 6.326149, 5.976351, 6.331502, 6.411818, 6.148468, 6.049733, 5.961005, 6.327937, 6.251904, 6.401917, 6.837333, 6.154858, 6.161207, 6.169611, 5.998937, 6.161207, 6.068426, 6.011267, 6.212606, 6.248043, 6.357842, 6.434547, 6.177944, 6.421622, 6.575076, 6.206576, 6.369901, 6.259581, 6.186209, 6.109248, 6.666957, 6.093570, 5.981414, 6.154858, 6.063785, 6.263398, 6.070738, 5.921578, 6.398595, 5.996452, 6.400257, 5.945421, 6.008813, 6.642487, 6.244167, 6.188264, 6.714171, 6.075346, 6.519147, 6.343880, 6.343880, 6.496775, 6.848005, 6.261492, 6.246107, 6.300786, 6.194405, 6.513230, 6.148468, 6.180017, 6.356108, 6.124683, 6.352629, 6.359574, 6.180017, 6.324359, 6.393591, 6.113682, 6.317165, 6.285998, 6.177944, 6.265301, 6.113682, 6.308098, 6.161207, 6.287859, 6.586172, 6.196444, 6.098074, 6.156979, 6.327937, 6.098074, 6.098074, 6.139885, 6.040255, 6.146329, 6.289716, 6.282267, 6.345636, 6.198479, 6.267201, 6.100319, 6.049733, 6.324359, 6.122493, 6.391917, 6.336826, 6.411818, 6.566672, 6.289716, 6.082219, 6.318968, 6.703188, 6.204558, 6.086775, 6.280396, 6.400257, 5.976351, 6.163315, 6.102559, 5.910797, 6.335054, 6.263398, 6.129050, 6.113682, 6.317165, 6.133398, 6.210600, 6.163315, 6.129050, 6.210600, 6.077642, 6.317165, 6.232448, 6.084499, 6.848005, 6.575076, 6.505784, 6.444131, 6.444131, 6.364751, 6.828712, 6.727432, 6.413459, 6.598509, 6.388561, 6.475433, 6.429719, 6.280396, 6.361302, 6.782192, 7.019297, 6.698268, 6.284134, 6.603944, 6.439350, 7.002156, 6.768493, 6.453625, 6.476972, 6.282267, 6.538140, 6.242223, 6.280396, 6.511745, 6.309918, 6.674561, 6.380123, 6.313548, 6.257668, 6.711740, 6.336826, 6.542472, 7.443664, 7.403670, 6.680855, 7.314553, 6.648985, 6.864848, 6.769642, 6.272877, 6.472346, 6.236370, 6.298949, 6.444131, 6.297109, 6.218600, 6.257668, 6.093570, 6.340359, 6.553933, 6.862758, 6.553933, 6.421622, 6.200509, 6.380123, 6.679599, 6.390241, 6.445720, 6.415097, 6.322565, 6.520621, 6.396930, 6.388561, 6.278521, 6.293419, 6.562444, 6.565265, 7.113142, 7.073270, 6.431331, 6.842683, 6.285998, 6.452049, 7.005789, 6.403574, 6.466145, 6.442540, 6.398595, 6.204558, 6.333280, 6.658011, 6.380123, 6.335054, 6.302619, 6.152733, 6.259581, 6.249975, 6.278521, 6.532334, 6.224558, 6.795706, 6.218600, 6.466145, 6.327937, 6.758095, 6.388561, 6.467699, 6.411818, 6.347389, 6.369901, 6.624065, 6.436150, 6.202536, 6.304449, 6.423247, 6.761573, 6.867974, 6.492240, 6.551080, 6.444131, 6.282267, 6.447306, 6.352629, 6.642487, 7.085901, 6.437752, 6.259581, 6.326149, 6.270988, 6.326149, 6.774224, 6.278521, 6.276643, 6.317165, 6.285998, 6.513230, 6.359574, 6.498282, 6.297109, 6.171701, 6.177944, 6.295266, 6.428105, 6.562444, 6.327937, 6.824374, 6.364751, 6.937314, 6.507278, 6.608001, 6.483107, 6.338594, 6.318968, 6.364751, 6.461468, 6.317165, 6.452049, 6.532334, 6.508769, 6.240276, 6.154858, 6.208590, 7.208600, 7.198184, 6.846943, 6.829794, 7.007601, 6.822197, 6.763885, 6.971669, 7.587311, 6.821107, 7.037906, 6.835185, 6.630683, 6.890609, 6.602588, 6.725034, 7.017506, 6.573680, 6.969791, 7.032624, 6.848005, 6.805723, 6.966024, 6.815640, 6.731018, 6.993015, 6.768493, 6.476972, 6.940222, 6.371612, 6.526495, 7.039660, 6.849066, 6.884487, 6.814543, 7.014814, 6.874198, 7.224025, 6.886532, 6.768493, 6.568078, 6.900731, 7.117206, 7.055313, 6.716595, 6.590301, 6.654153, 6.566672, 6.493754, 6.511745, 6.781058, 6.811244, 6.877296, 6.813445, 6.719013, 6.754604, 6.795706, 7.053586, 6.582025, 6.812345, 6.872128, 6.628041, 6.532334, 6.530878, 6.603944, 6.569481, 6.583409, 6.768493, 6.803505, 7.036148, 6.597146, 6.514713, 6.723832, 6.542472, 7.223296, 6.502790, 7.053586, 6.673298, 6.885510, 6.921658, 6.958448, 6.426488, 6.756932, 6.844815, 6.371612, 6.493754, 6.603944, 6.338594, 6.447306, 6.514713, 6.490724, 6.753438, 6.731018, 6.568078, 6.406880, 6.602588, 6.396930, 6.781058, 6.770789, 6.452049, 7.055313, 6.439350, 6.378426, 6.751101, 6.492240, 6.605298, 6.793466, 6.347389, 6.539586, 6.322565, 6.440947, 6.590301, 6.502790, 6.525030, 6.703188, 6.444131, 6.526495, 6.352629, 6.961296, 6.729824, 6.416732, 6.556778, 6.582025, 6.717805, 6.845880, 6.626718, 6.493754, 6.731018, 6.883463, 6.375025, 6.587550, 6.426488, 6.508769, 6.796824, 6.795706, 6.431331, 6.816736, 6.572283, 6.354370, 6.378426, 6.527958, 6.561031, 6.934397, 6.507278, 6.502790, 6.788972, 6.461468, 6.823286, 6.739337, 6.679599, 6.426488, 6.390241, 6.475433, 6.424869, 6.381816, 6.618739, 6.455199, 6.668228, 6.642487, 6.320768, 6.406880, 6.463029, 6.390241, 6.612041, 6.588926, 6.790097, 6.652863, 6.361302, 7.111512, 6.717805, 6.712956, 6.450470, 6.606650, 6.408529, 6.663133, 6.508769, 6.359574, 6.352629, 7.044905, 6.381816, 6.452049, 6.679599, 6.672033, 6.697034, 6.854355, 6.419995, 6.345636, 6.788972, 6.439350, 6.526495, 6.293419, 6.905753, 6.748760, 6.508769, 6.699500, 6.549651, 6.375025, 7.029973, 6.842683, 6.777647, 6.862758, 6.570883, 7.175490, 7.000334, 6.511745, 6.320768, 6.703188, 6.804615, 6.751101, 6.620073, 6.559615, 7.128496, 6.450470, 6.904751, 6.338594, 6.464588, 6.317165, 6.530878, 6.549651, 6.776507, 6.388561, 6.665684, 6.588926, 6.439350, 6.324359, 6.493754, 6.771936, 6.313548, 6.527958, 6.706862, 6.253829, 6.660575, 6.505784, 6.761573, 6.782192, 6.218600, 6.411818 4.753590, 4.615121, 4.997212, 3.332205, 2.484907, 5.631212, 3.663562, 5.347108, 6.660575, 5.880533, 3.044522, 4.859812, 3.433987, 5.652489, 6.228511, 4.499810, 4.624973, 5.267858, 1.791759, 2.708050, 4.418841, 5.117994, 3.044522, 2.484907, 3.970292, 3.663562, 4.976734, 4.812184, 6.120297, 3.637586, 5.214936, 3.951244, 5.023881, 7.086738, 6.063785, 4.595120, 4.454347, 6.599870, 4.442651, 6.304449, 5.783825, 4.127134, 3.332205, 6.109248, 2.890372, 4.043051, 5.541264, 6.378426, 4.007333, 4.060443, 4.605170, 5.918894, 5.805135, 5.298317, 2.833213, 5.129899, 5.545177, 4.204693, 4.248495, 5.537334, 6.098074, 7.667626, 4.727388, 3.044522, 5.587249, 5.700444, 2.995732, 6.461468, 3.988984, 4.682131, 3.555348, 4.709530, 6.577861, 3.828641, 7.771910, 4.844187, 5.192957, 2.079442, 4.962845, 4.595120, 4.962845, 4.812184, 4.499810, 3.555348, 2.833213, 4.844187, 4.454347, 4.060443, 6.120297, 7.086738, 5.652489, 2.079442, 5.545177, 4.709530, 2.890372, 3.970292, 5.214936, 6.378426, 3.951244, 7.667626, 5.023881, 2.484907, 5.541264, 6.660575, 2.484907, 4.976734, 6.109248, 5.298317, 5.805135, 3.044522, 3.433987, 4.997212, 4.859812, 4.043051, 6.098074, 4.624973, 5.587249, 4.605170, 3.828641, 5.880533, 6.063785, 3.044522, 5.918894, 6.228511, 3.332205, 5.192957, 5.631212, 3.637586, 6.304449, 7.771910, 4.753590, 3.044522, 4.127134, 4.418841, 2.708050, 6.577861, 5.267858, 5.347108, 3.663562, 5.783825, 3.988984, 4.682131, 5.537334, 3.332205, 4.007333, 4.204693, 3.663562, 4.442651, 4.727388, 6.599870, 5.117994, 4.248495, 5.129899, 6.461468, 2.995732, 5.700444, 4.615121, 1.791759, 4.007333, 4.248495, 6.660575, 4.997212, 6.378426, 5.880533, 4.615121, 4.043051, 6.098074, 5.541264, 4.418841, 2.484907, 6.120297, 5.192957, 4.976734, 5.214936, 6.304449, 4.812184, 3.663562, 6.228511, 3.663562, 3.332205, 4.962845, 1.791759, 4.753590, 4.442651, 2.995732, 3.951244, 2.833213, 6.063785, 5.023881, 6.577861, 4.127134, 4.204693, 6.461468, 3.637586, 3.332205, 5.298317, 5.545177, 5.805135, 5.347108, 5.267858, 2.484907, 2.890372, 3.988984, 5.918894, 7.086738, 5.700444, 3.433987, 5.117994, 5.587249, 3.555348, 5.129899, 5.537334, 2.079442, 4.844187, 5.652489, 5.783825, 4.709530, 4.605170, 7.667626, 4.595120, 4.859812, 4.682131, 3.970292, 4.727388, 6.599870, 3.044522, 4.454347, 3.828641, 6.109248, 4.060443, 5.631212, 4.624973, 3.044522, 7.771910, 3.044522, 4.499810, 2.708050, 3.951244, 3.637586, 4.418841, 4.976734, 3.663562, 5.805135, 3.332205, 6.660575, 5.347108, 1.791759, 2.079442, 4.682131, 5.652489, 5.631212, 4.859812, 7.667626, 5.880533, 3.828641, 5.298317, 4.844187, 4.727388, 5.537334, 4.709530, 3.988984, 6.599870, 6.228511, 4.615121, 7.771910, 5.545177, 2.833213, 4.605170, 5.918894, 5.541264, 4.442651, 3.970292, 5.129899, 4.007333, 4.043051, 6.098074, 6.461468, 5.117994, 4.060443, 6.109248, 3.044522, 3.555348, 6.304449, 5.192957, 4.454347, 4.997212, 5.214936, 4.248495, 4.499810, 2.995732, 4.753590, 5.023881, 6.063785, 3.044522, 5.587249, 5.267858, 4.962845, 4.204693, 3.663562, 3.332205, 6.378426, 2.890372, 4.624973, 4.812184, 7.086738, 6.577861, 3.433987, 3.044522, 4.127134, 6.120297, 2.708050, 5.783825, 2.484907, 2.484907, 4.595120, 5.700444, 2.484907, 4.442651, 6.461468, 5.192957, 5.541264, 3.044522, 4.615121, 1.791759, 3.663562, 5.918894, 6.599870, 4.454347, 4.859812, 4.812184, 6.577861, 3.044522, 4.682131, 3.433987, 5.129899, 2.708050, 5.545177, 4.043051, 6.063785, 5.652489, 4.844187, 5.805135, 2.833213, 6.660575, 4.976734, 2.484907, 5.587249, 4.499810, 7.667626, 4.727388, 5.117994, 4.624973, 5.700444, 5.631212, 3.332205, 3.637586, 3.663562, 4.709530, 5.298317, 4.204693, 3.555348, 5.347108, 4.962845, 6.378426, 6.120297, 5.880533, 6.109248, 4.997212, 5.783825, 5.267858, 4.127134, 3.044522, 4.418841, 2.079442, 7.771910, 5.023881, 5.214936, 3.332205, 6.304449, 4.753590, 4.060443, 5.537334, 6.228511, 3.970292, 3.951244, 7.086738, 4.605170, 3.828641, 4.007333, 2.995732, 4.248495, 4.595120, 2.890372, 3.988984, 6.098074, 2.708050, 3.988984, 5.652489, 4.418841, 5.298317, 6.120297, 4.615121, 4.709530, 4.442651, 5.117994, 6.063785, 6.304449, 7.086738, 4.204693, 6.098074, 4.997212, 6.228511, 3.637586, 6.577861, 3.555348, 3.044522, 4.248495, 6.660575, 3.663562, 3.433987, 4.060443, 7.771910, 4.727388, 4.859812, 4.605170, 5.267858, 5.541264, 3.332205, 2.079442, 4.844187, 4.624973, 5.880533, 4.127134, 2.833213, 4.976734, 3.332205, 2.995732, 5.537334, 6.378426, 4.962845, 3.828641, 2.484907, 4.043051, 5.347108, 4.753590, 7.667626, 5.129899, 5.587249, 4.682131, 5.214936, 4.812184, 6.461468, 4.595120, 5.023881, 5.700444, 2.484907, 5.545177, 4.499810, 5.631212, 3.044522, 5.805135, 3.663562, 3.951244, 6.599870, 5.918894, 1.791759, 3.970292, 5.783825, 4.007333, 2.890372, 4.454347, 3.044522, 5.192957, 6.109248, 4.043051, 2.484907, 7.771910, 3.044522, 3.951244, 5.023881, 2.708050, 4.709530, 2.079442, 5.214936, 5.298317, 4.060443, 4.127134, 4.844187, 7.667626, 4.727388, 4.624973, 3.044522, 4.595120, 3.555348, 3.970292, 2.995732, 4.605170, 4.499810, 5.587249, 6.109248, 5.783825, 7.086738, 3.988984, 5.631212, 6.660575, 4.682131, 4.442651, 4.753590, 5.880533, 4.007333, 3.637586, 4.248495, 5.267858, 4.962845, 6.098074, 1.791759, 2.484907, 2.890372, 5.545177, 4.418841, 2.833213, 3.663562, 5.918894, 6.304449, 3.828641, 4.615121, 4.204693, 4.454347, 4.812184, 6.378426, 5.192957, 5.117994, 5.129899, 6.228511, 6.461468, 5.700444, 6.599870, 3.332205, 4.976734, 6.577861, 4.859812, 6.120297, 3.433987, 3.332205, 5.537334, 5.347108, 3.663562, 5.652489, 6.063785, 5.805135, 4.997212, 5.541264, 3.044522, 3.044522, 5.117994, 7.667626, 3.663562, 4.962845, 5.347108, 4.127134, 4.682131, 5.880533, 6.120297, 5.541264, 4.418841, 3.044522, 4.454347, 4.727388, 5.805135, 3.332205, 6.228511, 2.079442, 3.988984, 5.192957, 4.624973, 4.007333, 6.577861, 4.442651, 5.298317, 1.791759, 4.709530, 6.660575, 4.499810, 5.700444, 4.595120, 6.378426, 3.433987, 4.204693, 3.970292, 6.098074, 5.214936, 4.844187, 6.063785, 4.615121, 7.086738, 4.605170, 4.997212, 4.812184, 6.109248, 3.332205, 5.129899, 3.951244, 7.771910, 5.023881, 5.587249, 5.631212, 3.044522, 4.859812, 2.995732, 2.484907, 4.043051, 5.537334, 5.267858, 2.708050, 2.484907, 4.753590, 4.248495, 3.555348, 6.304449, 2.890372, 6.599870, 3.637586, 3.663562, 4.060443, 4.976734, 5.652489, 5.783825, 5.545177, 5.918894, 6.461468, 3.828641, 2.833213, 3.044522, 3.332205, 6.461468, 3.970292, 5.192957, 2.484907, 5.783825, 4.454347, 4.007333, 5.700444, 3.044522, 6.063785, 6.109248, 6.378426, 2.708050, 5.918894, 5.214936, 5.267858, 5.587249, 5.023881, 3.555348, 6.660575, 3.828641, 4.812184, 4.844187, 4.605170, 2.833213, 4.976734, 5.805135, 4.204693, 6.599870, 6.120297, 5.652489, 2.890372, 7.667626, 5.117994, 4.624973, 5.298317, 4.727388, 3.637586, 4.043051, 4.418841, 4.615121, 3.951244, 2.995732, 4.682131, 6.577861, 5.347108, 4.499810, 4.248495, 3.663562, 4.442651, 5.541264, 2.484907, 3.663562, 2.079442, 6.304449, 3.332205, 6.228511, 4.595120, 3.044522, 3.988984, 6.098074, 5.880533, 4.060443, 5.631212, 5.545177, 4.753590, 1.791759, 4.997212, 4.127134, 4.859812, 7.086738, 5.537334, 4.962845, 4.709530, 3.433987, 5.129899, 7.771910 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.3862944, 2.7725887, 0.6931472, 0.0000000, 1.3862944, 0.0000000, 1.7917595, 1.9459101, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 0.0000000, 1.6094379, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 0.6931472, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 0.6931472, 1.0986123, 2.1972246, 1.9459101, 1.0986123, 0.0000000, 2.0794415, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 3.1354942, 0.0000000, 0.6931472, 1.0986123, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 2.3025851, 0.0000000, 3.3322045, 0.6931472, 1.0986123, 0.0000000, 1.3862944, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 1.6094379, 2.7725887, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 3.1354942, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 0.6931472, 0.6931472, 1.0986123, 1.9459101, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 0.6931472, 0.6931472, 0.6931472, 2.1972246, 1.9459101, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 1.0986123, 3.3322045, 0.0000000, 0.0000000, 1.0986123, 1.6094379, 0.0000000, 2.3025851, 1.7917595, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 2.0794415, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 0.6931472, 1.3862944, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.6094379, 0.0000000, 1.6094379, 1.0986123, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 2.3025851, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.9459101, 1.3862944, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 2.1972246, 2.7725887, 1.7917595, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 2.0794415, 0.6931472, 0.0000000, 0.6931472, 1.7917595, 0.0000000, 0.0000000, 1.0986123, 3.1354942, 0.6931472, 1.3862944, 1.3862944, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 3.3322045, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.6094379, 0.6931472, 0.0000000, 1.9459101, 0.0000000, 2.7725887, 1.3862944, 0.0000000, 0.0000000, 1.3862944, 1.7917595, 0.6931472, 1.3862944, 3.1354942, 0.6931472, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 1.9459101, 0.0000000, 3.3322045, 0.0000000, 0.0000000, 1.0986123, 2.1972246, 0.0000000, 0.0000000, 0.0000000, 2.0794415, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 1.0986123, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.0986123, 1.7917595, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 2.3025851, 0.0000000, 0.0000000, 1.0986123, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 2.1972246, 1.0986123, 0.0000000, 1.3862944, 0.0000000, 2.3025851, 0.6931472, 1.3862944, 0.0000000, 2.0794415, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.7917595, 0.6931472, 1.9459101, 0.0000000, 2.7725887, 0.6931472, 0.0000000, 1.0986123, 0.0000000, 3.1354942, 0.0000000, 1.0986123, 0.0000000, 1.7917595, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 1.3862944, 1.3862944, 1.3862944, 1.6094379, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 1.7917595, 1.0986123, 0.0000000, 1.6094379, 0.0000000, 3.3322045, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 0.6931472, 1.9459101, 0.0000000, 0.0000000, 2.7725887, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 1.7917595, 1.6094379, 1.0986123, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 1.0986123, 2.7725887, 0.0000000, 1.0986123, 0.6931472, 1.9459101, 0.0000000, 2.3025851, 0.0000000, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 0.0000000, 0.6931472, 3.3322045, 0.0000000, 1.3862944, 1.0986123, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 0.0000000, 3.1354942, 2.0794415, 1.0986123, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 1.0986123, 2.1972246, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 0.6931472, 0.0000000, 0.0000000, 3.3322045, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 1.0986123, 0.6931472, 3.1354942, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 2.7725887, 0.0000000, 0.6931472, 2.7725887, 1.3862944, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 1.3862944, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 2.1972246, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 1.0986123, 1.0986123, 2.0794415, 1.9459101, 0.0000000, 1.7917595, 1.0986123, 0.0000000, 0.6931472, 2.3025851, 1.3862944, 1.6094379, 0.0000000, 0.0000000, 0.6931472, 1.3862944, 0.0000000, 1.7917595, 0.6931472, 1.9459101, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 3.1354942, 0.0000000, 1.3862944, 1.3862944, 1.0986123, 1.3862944, 0.6931472, 1.6094379, 0.0000000, 1.6094379, 0.6931472, 0.0000000, 0.0000000, 1.9459101, 0.0000000, 1.9459101, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 2.3025851, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 1.7917595, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.6931472, 0.6931472, 0.0000000, 2.7725887, 1.0986123, 0.6931472, 0.0000000, 0.6931472, 0.0000000, 2.0794415, 0.0000000, 3.3322045, 0.0000000, 1.0986123, 0.6931472, 0.0000000, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.7917595, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.6931472, 0.6931472, 1.7917595, 0.0000000, 0.0000000, 2.1972246, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.7917595, 0.0000000, 0.6931472, 0.6931472, 1.3862944, 0.0000000, 2.1972246, 0.0000000, 1.7917595, 1.0986123, 0.0000000, 0.0000000, 2.7725887, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 0.0000000, 0.6931472, 1.9459101, 0.0000000, 1.0986123, 1.6094379, 1.7917595, 0.0000000, 3.1354942, 1.0986123, 0.0000000, 1.0986123, 0.0000000, 0.0000000, 0.0000000, 1.6094379, 0.0000000, 0.0000000, 0.0000000, 1.3862944, 2.3025851, 1.3862944, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 1.0986123, 0.0000000, 1.9459101, 0.6931472, 0.0000000, 0.0000000, 1.0986123, 0.6931472, 0.6931472, 0.6931472, 0.0000000, 0.0000000, 0.0000000, 0.6931472, 1.0986123, 1.3862944, 2.7725887, 0.6931472, 1.3862944, 0.0000000, 0.0000000, 2.0794415, 3.3322045 agg_lexdec %&gt;% gt() %&gt;% gt_plt_sparkline(RT) %&gt;% gt_plt_sparkline(Frequency) %&gt;% gt_plt_sparkline(FamilySize) #jxvwzkaysj table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #jxvwzkaysj thead, #jxvwzkaysj tbody, #jxvwzkaysj tfoot, #jxvwzkaysj tr, #jxvwzkaysj td, #jxvwzkaysj th { border-style: none; } #jxvwzkaysj p { margin: 0; padding: 0; } #jxvwzkaysj .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #jxvwzkaysj .gt_caption { padding-top: 4px; padding-bottom: 4px; } #jxvwzkaysj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #jxvwzkaysj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #jxvwzkaysj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jxvwzkaysj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jxvwzkaysj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #jxvwzkaysj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #jxvwzkaysj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #jxvwzkaysj .gt_column_spanner_outer:first-child { padding-left: 0; } #jxvwzkaysj .gt_column_spanner_outer:last-child { padding-right: 0; } #jxvwzkaysj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #jxvwzkaysj .gt_spanner_row { border-bottom-style: hidden; } #jxvwzkaysj .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #jxvwzkaysj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #jxvwzkaysj .gt_from_md > :first-child { margin-top: 0; } #jxvwzkaysj .gt_from_md > :last-child { margin-bottom: 0; } #jxvwzkaysj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #jxvwzkaysj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #jxvwzkaysj .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #jxvwzkaysj .gt_row_group_first td { border-top-width: 2px; } #jxvwzkaysj .gt_row_group_first th { border-top-width: 2px; } #jxvwzkaysj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jxvwzkaysj .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #jxvwzkaysj .gt_first_summary_row.thick { border-top-width: 2px; } #jxvwzkaysj .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jxvwzkaysj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #jxvwzkaysj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #jxvwzkaysj .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #jxvwzkaysj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #jxvwzkaysj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #jxvwzkaysj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jxvwzkaysj .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #jxvwzkaysj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #jxvwzkaysj .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #jxvwzkaysj .gt_left { text-align: left; } #jxvwzkaysj .gt_center { text-align: center; } #jxvwzkaysj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #jxvwzkaysj .gt_font_normal { font-weight: normal; } #jxvwzkaysj .gt_font_bold { font-weight: bold; } #jxvwzkaysj .gt_font_italic { font-style: italic; } #jxvwzkaysj .gt_super { font-size: 65%; } #jxvwzkaysj .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #jxvwzkaysj .gt_asterisk { font-size: 100%; vertical-align: 0; } #jxvwzkaysj .gt_indent_1 { text-indent: 5px; } #jxvwzkaysj .gt_indent_2 { text-indent: 10px; } #jxvwzkaysj .gt_indent_3 { text-indent: 15px; } #jxvwzkaysj .gt_indent_4 { text-indent: 20px; } #jxvwzkaysj .gt_indent_5 { text-indent: 25px; } NativeLanguage RT Frequency FamilySize English Other agg_lexdec %&gt;% gt() %&gt;% gt_plt_dist( RT, type = &quot;density&quot; ) %&gt;% gt_plt_dist( Frequency, type = &quot;boxplot&quot; ) %&gt;% gt_plt_dist( FamilySize, type = &quot;histogram&quot; ) #uvmkkytzgy table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #uvmkkytzgy thead, #uvmkkytzgy tbody, #uvmkkytzgy tfoot, #uvmkkytzgy tr, #uvmkkytzgy td, #uvmkkytzgy th { border-style: none; } #uvmkkytzgy p { margin: 0; padding: 0; } #uvmkkytzgy .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uvmkkytzgy .gt_caption { padding-top: 4px; padding-bottom: 4px; } #uvmkkytzgy .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uvmkkytzgy .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #uvmkkytzgy .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uvmkkytzgy .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uvmkkytzgy .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uvmkkytzgy .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uvmkkytzgy .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uvmkkytzgy .gt_column_spanner_outer:first-child { padding-left: 0; } #uvmkkytzgy .gt_column_spanner_outer:last-child { padding-right: 0; } #uvmkkytzgy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #uvmkkytzgy .gt_spanner_row { border-bottom-style: hidden; } #uvmkkytzgy .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #uvmkkytzgy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uvmkkytzgy .gt_from_md > :first-child { margin-top: 0; } #uvmkkytzgy .gt_from_md > :last-child { margin-bottom: 0; } #uvmkkytzgy .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uvmkkytzgy .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #uvmkkytzgy .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #uvmkkytzgy .gt_row_group_first td { border-top-width: 2px; } #uvmkkytzgy .gt_row_group_first th { border-top-width: 2px; } #uvmkkytzgy .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uvmkkytzgy .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #uvmkkytzgy .gt_first_summary_row.thick { border-top-width: 2px; } #uvmkkytzgy .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uvmkkytzgy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uvmkkytzgy .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uvmkkytzgy .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #uvmkkytzgy .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uvmkkytzgy .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uvmkkytzgy .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uvmkkytzgy .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uvmkkytzgy .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uvmkkytzgy .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uvmkkytzgy .gt_left { text-align: left; } #uvmkkytzgy .gt_center { text-align: center; } #uvmkkytzgy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uvmkkytzgy .gt_font_normal { font-weight: normal; } #uvmkkytzgy .gt_font_bold { font-weight: bold; } #uvmkkytzgy .gt_font_italic { font-style: italic; } #uvmkkytzgy .gt_super { font-size: 65%; } #uvmkkytzgy .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #uvmkkytzgy .gt_asterisk { font-size: 100%; vertical-align: 0; } #uvmkkytzgy .gt_indent_1 { text-indent: 5px; } #uvmkkytzgy .gt_indent_2 { text-indent: 10px; } #uvmkkytzgy .gt_indent_3 { text-indent: 15px; } #uvmkkytzgy .gt_indent_4 { text-indent: 20px; } #uvmkkytzgy .gt_indent_5 { text-indent: 25px; } NativeLanguage RT Frequency FamilySize English Other head(lexdec) %&gt;% select( NativeLanguage, RT, Frequency, FamilySize)%&gt;% gt() %&gt;% gt_plt_bar_pct( RT, labels = TRUE ) %&gt;% gt_plt_bar_pct( Frequency, labels=FALSE, fill = &quot;forestgreen&quot; ) #wztxyiiewn table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #wztxyiiewn thead, #wztxyiiewn tbody, #wztxyiiewn tfoot, #wztxyiiewn tr, #wztxyiiewn td, #wztxyiiewn th { border-style: none; } #wztxyiiewn p { margin: 0; padding: 0; } #wztxyiiewn .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #wztxyiiewn .gt_caption { padding-top: 4px; padding-bottom: 4px; } #wztxyiiewn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #wztxyiiewn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #wztxyiiewn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wztxyiiewn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wztxyiiewn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #wztxyiiewn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #wztxyiiewn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #wztxyiiewn .gt_column_spanner_outer:first-child { padding-left: 0; } #wztxyiiewn .gt_column_spanner_outer:last-child { padding-right: 0; } #wztxyiiewn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #wztxyiiewn .gt_spanner_row { border-bottom-style: hidden; } #wztxyiiewn .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #wztxyiiewn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #wztxyiiewn .gt_from_md > :first-child { margin-top: 0; } #wztxyiiewn .gt_from_md > :last-child { margin-bottom: 0; } #wztxyiiewn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #wztxyiiewn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #wztxyiiewn .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #wztxyiiewn .gt_row_group_first td { border-top-width: 2px; } #wztxyiiewn .gt_row_group_first th { border-top-width: 2px; } #wztxyiiewn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wztxyiiewn .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #wztxyiiewn .gt_first_summary_row.thick { border-top-width: 2px; } #wztxyiiewn .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wztxyiiewn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #wztxyiiewn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #wztxyiiewn .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #wztxyiiewn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #wztxyiiewn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #wztxyiiewn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wztxyiiewn .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wztxyiiewn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #wztxyiiewn .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #wztxyiiewn .gt_left { text-align: left; } #wztxyiiewn .gt_center { text-align: center; } #wztxyiiewn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #wztxyiiewn .gt_font_normal { font-weight: normal; } #wztxyiiewn .gt_font_bold { font-weight: bold; } #wztxyiiewn .gt_font_italic { font-style: italic; } #wztxyiiewn .gt_super { font-size: 65%; } #wztxyiiewn .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #wztxyiiewn .gt_asterisk { font-size: 100%; vertical-align: 0; } #wztxyiiewn .gt_indent_1 { text-indent: 5px; } #wztxyiiewn .gt_indent_2 { text-indent: 10px; } #wztxyiiewn .gt_indent_3 { text-indent: 15px; } #wztxyiiewn .gt_indent_4 { text-indent: 20px; } #wztxyiiewn .gt_indent_5 { text-indent: 25px; } NativeLanguage RT Frequency FamilySize English 99.9% 1.3862944 English 99.4% 1.0986123 English 100% 0.6931472 English 97.4% 0.0000000 English 94.9% 3.1354942 English 97.3% 0.6931472 lexdec%&gt;% select( NativeLanguage, RT, Frequency, FamilySize)%&gt;% gt_plt_summary() @import url(\"https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\"); #nfazjqzqzu table { font-family: Lato, system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #nfazjqzqzu thead, #nfazjqzqzu tbody, #nfazjqzqzu tfoot, #nfazjqzqzu tr, #nfazjqzqzu td, #nfazjqzqzu th { border-style: none; } #nfazjqzqzu p { margin: 0; padding: 0; } #nfazjqzqzu .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 3px; border-top-color: #FFFFFF; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nfazjqzqzu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #nfazjqzqzu .gt_title { color: #333333; font-size: 24px; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nfazjqzqzu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #nfazjqzqzu .gt_heading { background-color: #FFFFFF; text-align: left; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nfazjqzqzu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 0px; border-bottom-color: #D3D3D3; } #nfazjqzqzu .gt_col_headings { border-top-style: solid; border-top-width: 0px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nfazjqzqzu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 80%; font-weight: bolder; text-transform: uppercase; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nfazjqzqzu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 80%; font-weight: bolder; text-transform: uppercase; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nfazjqzqzu .gt_column_spanner_outer:first-child { padding-left: 0; } #nfazjqzqzu .gt_column_spanner_outer:last-child { padding-right: 0; } #nfazjqzqzu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #nfazjqzqzu .gt_spanner_row { border-bottom-style: hidden; } #nfazjqzqzu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 80%; font-weight: bolder; text-transform: uppercase; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #nfazjqzqzu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 80%; font-weight: bolder; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nfazjqzqzu .gt_from_md > :first-child { margin-top: 0; } #nfazjqzqzu .gt_from_md > :last-child { margin-bottom: 0; } #nfazjqzqzu .gt_row { padding-top: 7px; padding-bottom: 7px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #F6F7F7; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nfazjqzqzu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 80%; font-weight: bolder; text-transform: uppercase; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #nfazjqzqzu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #nfazjqzqzu .gt_row_group_first td { border-top-width: 2px; } #nfazjqzqzu .gt_row_group_first th { border-top-width: 2px; } #nfazjqzqzu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nfazjqzqzu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #nfazjqzqzu .gt_first_summary_row.thick { border-top-width: 2px; } #nfazjqzqzu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfazjqzqzu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nfazjqzqzu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nfazjqzqzu .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #nfazjqzqzu .gt_striped { background-color: #FAFAFA; } #nfazjqzqzu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nfazjqzqzu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nfazjqzqzu .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #nfazjqzqzu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nfazjqzqzu .gt_sourcenote { font-size: 12px; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #nfazjqzqzu .gt_left { text-align: left; } #nfazjqzqzu .gt_center { text-align: center; } #nfazjqzqzu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nfazjqzqzu .gt_font_normal { font-weight: normal; } #nfazjqzqzu .gt_font_bold { font-weight: bold; } #nfazjqzqzu .gt_font_italic { font-style: italic; } #nfazjqzqzu .gt_super { font-size: 65%; } #nfazjqzqzu .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #nfazjqzqzu .gt_asterisk { font-size: 100%; vertical-align: 0; } #nfazjqzqzu .gt_indent_1 { text-indent: 5px; } #nfazjqzqzu .gt_indent_2 { text-indent: 10px; } #nfazjqzqzu .gt_indent_3 { text-indent: 15px; } #nfazjqzqzu .gt_indent_4 { text-indent: 20px; } #nfazjqzqzu .gt_indent_5 { text-indent: 25px; } . 1659 rows x 4 cols Column Plot Overview Missing Mean Median SD NativeLanguage English and Other 0.0% — — — RT 0.0% 6.4 6.3 0.2 Frequency 0.0% 4.8 4.8 1.3 FamilySize 0.0% 0.7 0.0 0.9 16.2 DT: easy filtering &amp; sorting DT stands for “DataTables”, the Javascript library it interacts with. DT stands out for its ability to handle large datasets efficiently and its rich array of features like searching, sorting, and pagination. I love adding a DT table at the beginning of my data analysis Quarto report. It provides access to your raw data easily! Please check my full introduction to DT for more! Oh and this is how a DT table looks like: library(DT) lex.table &lt;- datatable(lexdec, filter = &quot;top&quot;, class = &#39;cell-border stripe hover compact&#39; ) # put it at the top of the table flextable is another solid option to create very polish static tables. It supports a wide range of formatting options, including merging cells, rotating text, and conditional formatting. It stands out due to its compatibility with various R Markdown formats, including Word, PowerPoint, and HTML. library(flextable) set_flextable_defaults( font.family = &quot;Arial&quot;, font.size = 10, border.color = &quot;gray&quot;, big.mark = &quot;&quot;) ft &lt;- flextable(head(lexdec)) |&gt; bold(part = &quot;header&quot;) ft .cl-d0e0da50{}.cl-d0d8ba50{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d0d8ba64{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d0dbdc08{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d0dbdc1c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d0dbeef0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0dbeefa{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0dbef04{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0dbef0e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0dbef0f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0dbef18{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}SubjectRTTrialSexNativeLanguageCorrectPrevTypePrevCorrectWordFrequencyFamilySizeSynsetCountLengthClassFreqSingularFreqPluralDerivEntropyComplexrInflmeanRTSubjFreqmeanSizemeanWeightBNCwBNCcBNCdBNCcRatioBNCdRatioA16.34035923FEnglishcorrectwordcorrectowl4.8598121.38629440.69314723animal54740.7912simplex-0.31015496.35823.123.47583.180612.0570650.0000006.1756020.0000000.512198A16.30809827FEnglishcorrectnonwordcorrectmole4.6051701.09861231.94591014animal69300.6968simplex0.81450806.41502.402.99992.61125.7388064.0622512.8502780.7078560.496667A16.34913929FEnglishcorrectnonwordcorrectcherry4.9972120.69314721.60943796plant83490.4754simplex0.51879386.34263.881.62781.20815.7165203.24980112.5887270.5684932.202166A16.18620930FEnglishcorrectwordcorrectpear4.7273880.00000001.09861234plant44680.0000simplex-0.42744406.33534.521.99081.61142.0503701.4624107.3632180.7132423.591166A16.02586632FEnglishcorrectnonwordcorrectdog7.6676263.13549422.07944153animal12338281.2129simplex0.39779616.29566.044.64294.516774.83849450.859385241.5610400.6795893.227765A16.18001733FEnglishcorrectwordcorrectblackberry4.0604430.69314721.386294410plant26310.3492complex-0.16989906.39593.281.58311.13651.2703380.1624901.1876160.1279110.934882 ft |&gt; highlight(i = ~ RT &gt; 6.3, j = &quot;RT&quot;, color = &quot;#ffe842&quot;) |&gt; bg(j = c(&quot;Frequency&quot;, &quot;FamilySize&quot;, &quot;Length&quot;), bg = scales::col_quantile(palette = c(&quot;wheat&quot;, &quot;red&quot;), domain =NULL)) |&gt; add_footer_lines(&quot;The &#39;lexdec&#39; dataset&quot;) .cl-d1014d44{}.cl-d0f791a0{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d0f791b4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d0f791be{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:rgba(255, 232, 66, 1.00);}.cl-d0fae058{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d0fae06c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d0faf4c6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4d0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4da{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4e4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4ee{width:0.75in;background-color:rgba(255, 112, 65, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4ef{width:0.75in;background-color:rgba(255, 0, 0, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4f8{width:0.75in;background-color:rgba(245, 222, 179, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf4f9{width:0.75in;background-color:rgba(255, 170, 121, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf502{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf50c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf50d{width:0.75in;background-color:rgba(245, 222, 179, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf50e{width:0.75in;background-color:rgba(255, 170, 121, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf516{width:0.75in;background-color:rgba(255, 0, 0, 1.00);vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d0faf517{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}SubjectRTTrialSexNativeLanguageCorrectPrevTypePrevCorrectWordFrequencyFamilySizeSynsetCountLengthClassFreqSingularFreqPluralDerivEntropyComplexrInflmeanRTSubjFreqmeanSizemeanWeightBNCwBNCcBNCdBNCcRatioBNCdRatioA16.34035923FEnglishcorrectwordcorrectowl4.8598121.38629440.69314723animal54740.7912simplex-0.31015496.35823.123.47583.180612.0570650.0000006.1756020.0000000.512198A16.30809827FEnglishcorrectnonwordcorrectmole4.6051701.09861231.94591014animal69300.6968simplex0.81450806.41502.402.99992.61125.7388064.0622512.8502780.7078560.496667A16.34913929FEnglishcorrectnonwordcorrectcherry4.9972120.69314721.60943796plant83490.4754simplex0.51879386.34263.881.62781.20815.7165203.24980112.5887270.5684932.202166A16.18620930FEnglishcorrectwordcorrectpear4.7273880.00000001.09861234plant44680.0000simplex-0.42744406.33534.521.99081.61142.0503701.4624107.3632180.7132423.591166A16.02586632FEnglishcorrectnonwordcorrectdog7.6676263.13549422.07944153animal12338281.2129simplex0.39779616.29566.044.64294.516774.83849450.859385241.5610400.6795893.227765A16.18001733FEnglishcorrectwordcorrectblackberry4.0604430.69314721.386294410plant26310.3492complex-0.16989906.39593.281.58311.13651.2703380.1624901.1876160.1279110.934882The 'lexdec' dataset select(lexdec, c(&quot;Frequency&quot;, &quot;FamilySize&quot;, &quot;Length&quot;,&quot;NativeLanguage&quot;)) %&gt;% summarizor(by = c(&quot;NativeLanguage&quot;)) |&gt; flextable::as_flextable(spread_first_col = TRUE) .cl-d12b641c{}.cl-d12398a4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d12398b8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d126dce4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d126dcf8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-d126dcf9{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d126dd02{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d126dd03{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-d126dd0c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d126dd16{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-d126f256{width:1.097in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f260{width:0.079in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f26a{width:0.851in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 1.5pt solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f26b{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f274{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f27e{width:0.851in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f288{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f289{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f292{width:0.851in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f293{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f29c{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2a6{width:0.851in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2a7{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2a8{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2b0{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2ba{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(190, 190, 190, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2bb{width:1.097in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d126f2c4{width:0.851in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(190, 190, 190, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}English(N=948)Other(N=711)FrequencyMean (SD)4.8 (1.3)4.8 (1.3)Median (IQR)4.8 (1.7)4.8 (1.7)Range1.8 - 7.81.8 - 7.8FamilySizeMean (SD)0.7 (0.9)0.7 (0.9)Median (IQR)0.0 (1.1)0.0 (1.1)Range0.0 - 3.30.0 - 3.3LengthMean (SD)5.9 (1.9)5.9 (1.9)Median (IQR)6.0 (2.0)6.0 (2.0)Range3.0 - 10.03.0 - 10.0 16.3 modelsummary library(modelsummary) library(tinytable) datasummary_skim(lexdec) ## Warning in !is.null(rmarkdown::metadata$output) &amp;&amp; rmarkdown::metadata$output %in% : &#39;length(x) = 2 &gt; 1&#39; in ## coercion to &#39;logical(1)&#39; Unique (#) Missing (%) Mean SD Min Median Max RT 531 0 6.4 0.2 5.8 6.3 7.6 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Trial 162 0 105.0 47.1 23.0 106.0 185.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Frequency 74 0 4.8 1.3 1.8 4.8 7.8 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } FamilySize 13 0 0.7 0.9 0.0 0.0 3.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } SynsetCount 9 0 1.3 0.4 0.7 1.1 2.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Length 8 0 5.9 1.9 3.0 6.0 10.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } FreqSingular 58 0 132.1 234.5 4.0 69.0 1518.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } FreqPlural 61 0 109.7 159.8 0.0 49.0 854.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } DerivEntropy 42 0 0.4 0.5 0.0 0.0 2.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } rInfl 72 0 0.3 0.9 −1.3 0.2 4.4 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } meanRT 79 0 6.4 0.1 6.2 6.4 6.6 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } SubjFreq 58 0 3.9 1.0 2.0 3.9 6.0 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } meanSize 78 0 2.9 1.0 1.3 3.1 4.8 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } meanWeight 78 0 2.6 1.0 0.8 2.8 4.7 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } BNCw 75 0 7.4 13.0 0.0 3.3 79.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } BNCc 35 0 5.0 13.6 0.0 0.6 83.2 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } BNCd 42 0 13.0 32.1 0.0 3.8 241.6 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } BNCcRatio 66 0 0.5 0.9 0.0 0.3 8.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } BNCdRatio 74 0 1.5 1.5 0.0 0.9 6.3 .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } datasummary_balance(~NativeLanguage, lexdec) ## Warning: These variables were omitted because they include more than 50 levels: Word. English (N=948) Other (N=711) Mean Std. Dev. Mean Std. Dev. Diff. in Means Std. Error RT 6.3 0.2 6.5 0.3 0.2 0.0 Trial 105.5 47.2 104.2 47.1 -1.3 2.3 Frequency 4.8 1.3 4.8 1.3 0.0 0.1 FamilySize 0.7 0.9 0.7 0.9 0.0 0.0 SynsetCount 1.3 0.4 1.3 0.4 0.0 0.0 Length 5.9 1.9 5.9 1.9 0.0 0.1 FreqSingular 132.1 234.5 132.1 234.6 0.0 11.6 FreqPlural 109.7 159.8 109.7 159.8 0.0 7.9 DerivEntropy 0.4 0.5 0.4 0.5 0.0 0.0 rInfl 0.3 0.9 0.3 0.9 0.0 0.0 meanRT 6.4 0.1 6.4 0.1 0.0 0.0 SubjFreq 3.9 1.0 3.9 1.0 0.0 0.1 meanSize 2.9 1.0 2.9 1.0 0.0 0.0 meanWeight 2.6 1.0 2.6 1.0 0.0 0.1 BNCw 7.4 13.0 7.4 13.0 0.0 0.6 BNCc 5.0 13.6 5.0 13.6 0.0 0.7 BNCd 13.0 32.1 13.0 32.1 0.0 1.6 BNCcRatio 0.5 0.9 0.5 0.9 0.0 0.0 BNCdRatio 1.5 1.5 1.5 1.5 0.0 0.1 N Pct. N Pct. Subject A1 79 8.3 0 0.0 A2 79 8.3 0 0.0 A3 0 0.0 79 11.1 C 79 8.3 0 0.0 D 0 0.0 79 11.1 I 0 0.0 79 11.1 J 0 0.0 79 11.1 K 79 8.3 0 0.0 M1 79 8.3 0 0.0 M2 0 0.0 79 11.1 P 0 0.0 79 11.1 R1 79 8.3 0 0.0 R2 79 8.3 0 0.0 R3 79 8.3 0 0.0 S 79 8.3 0 0.0 T1 79 8.3 0 0.0 T2 0 0.0 79 11.1 V 0 0.0 79 11.1 W1 79 8.3 0 0.0 W2 79 8.3 0 0.0 Z 0 0.0 79 11.1 Sex F 553 58.3 553 77.8 M 395 41.7 158 22.2 Correct correct 920 97.0 674 94.8 incorrect 28 3.0 37 5.2 PrevType nonword 482 50.8 373 52.5 word 466 49.2 338 47.5 PrevCorrect correct 911 96.1 631 88.7 incorrect 37 3.9 80 11.3 Class animal 528 55.7 396 55.7 plant 420 44.3 315 44.3 Complex complex 120 12.7 90 12.7 simplex 828 87.3 621 87.3 datasummary_correlation(lexdec) RT Trial Frequency FamilySize SynsetCount Length FreqSingular FreqPlural DerivEntropy rInfl meanRT SubjFreq meanSize meanWeight BNCw BNCc BNCd BNCcRatio BNCdRatio RT 1 . . . . . . . . . . . . . . . . . . Trial −.06 1 . . . . . . . . . . . . . . . . . Frequency −.23 .00 1 . . . . . . . . . . . . . . . . FamilySize −.19 .02 .71 1 . . . . . . . . . . . . . . . SynsetCount −.18 .00 .49 .55 1 . . . . . . . . . . . . . . Length .16 .01 −.43 −.63 −.33 1 . . . . . . . . . . . . . FreqSingular −.14 .02 .68 .67 .50 −.30 1 . . . . . . . . . . . . FreqPlural −.16 .02 .76 .70 .39 −.36 .89 1 . . . . . . . . . . . DerivEntropy −.15 .00 .52 .85 .48 −.57 .37 .41 1 . . . . . . . . . . rInfl .02 −.02 −.18 −.15 .16 .19 .06 −.21 −.14 1 . . . . . . . . . meanRT .34 .00 −.63 −.53 −.48 .45 −.39 −.45 −.42 .09 1 . . . . . . . . SubjFreq −.23 .02 .50 .25 .31 −.08 .44 .45 .08 .05 −.63 1 . . . . . . . meanSize .00 −.02 .42 .36 −.05 −.24 .44 .50 .12 −.19 .00 −.21 1 . . . . . . meanWeight −.01 −.02 .44 .37 −.04 −.25 .46 .52 .12 −.19 −.02 −.18 1.00 1 . . . . . BNCw −.13 .02 .64 .69 .45 −.32 .96 .87 .41 .03 −.37 .44 .42 .44 1 . . . . BNCc −.09 .01 .48 .58 .41 −.30 .69 .64 .42 .03 −.25 .24 .31 .33 .72 1 . . . BNCd −.12 .01 .52 .49 .40 −.23 .79 .70 .24 .06 −.34 .53 .30 .32 .80 .49 1 . . BNCcRatio −.03 −.01 .19 .23 .22 −.18 .12 .13 .24 .05 −.07 .01 .10 .10 .13 .74 .05 1 . BNCdRatio −.11 .01 .12 −.11 .10 .12 .13 .09 −.19 .10 −.32 .61 −.16 −.15 .09 −.02 .38 −.03 1 mod &lt;- lm(RT ~ NativeLanguage, data = lexdec) modelsummary(mod, output = &quot;table.docx&quot;) modelsummary(mod) Model 1 (Intercept) 6.318 (0.007) NativeLanguageOther 0.156 (0.011) Num.Obs. 1659 R2 0.102 R2 Adj. 0.101 AIC −179.8 BIC −163.6 Log.Lik. 92.917 F 188.227 RMSE 0.23 "],["r-markdown.html", "Chapter 17 R-markdown 17.1 YAML前置部分 17.2 代码块 17.3 Markdown区域", " Chapter 17 R-markdown 使用 R Markdown，可以轻松创建一个文档，将代码、代码结果以及任何与分析相关的文本或外部图像结合在一起。本教程大部分都是使用 R Markdown 创建的。 要创建一个新的 R Markdown 文件，在 R Studio 中选择“文件&gt;新建文件&gt;R Markdown…”。接着，会弹出一个名为“新建 R Markdown”的窗口。 创建 在这里，可以指定要创建的文件类型。标准类型是文档（HTML、PDF 或 Word），但您也可以选择其他选项，例如创建幻灯片（演示文稿）。由于 HTML 文件不像 PDF 和 Word 那样有传统的“页面”分隔，因此一般推荐使用 HTML 格式。此时还可以在各自的字段中为您的文档选择标题和作者名。最后，选择“确定”以创建新 R Markdown 文件。新文件会像创建新脚本时一样出现在R Studio 会话中。 R Markdown 文档时，看起来类似于一个脚本编辑器。有三个主要部分：前置部分、代码块和注释图文。 17.1 YAML前置部分 前置部分，为 R Markdown 指定设置和标题信息。例如，设置输出文档时使用的文件类型。 1、题目 YAML中定义 title:“语言数据科学教程” 2、作者、单位、时间 ， &gt; author: “XXX作者”。 若有多个作者，可用[]将多个作者包裹，作者间用逗号分隔。 author: [XXX1,XXX2]，结果每个作者占一行。 author: - name: “作者1,作者2” affiliation: &quot; 1XXX大学; 2XXX大学 &quot; date: &quot; September 08, 2024 &quot; 3、目录 YAML中定义，常见参数如下： - toc: yes 是所有输出格式的通用选项，用以自动生成目录。 - toc_depth: 4 通用选项，控制目录显示深度，即输出文档要显示的目录层级。 - toc_float(默认FALSE): true 适用于输出HTML格式文档。属性为true时，目录变为侧边栏，并固定在屏幕左侧。 - collapsed：false时折叠二级标题以下的标题。 - smooth_scroll(默认true)：是否在鼠标点击目录时添加平滑滚动。 - number_sections(默认FALSE)：若为true，自动添加目录序号。 17.2 代码块 chunk 代码块用于放置要运行的 R 代码。通常，会在 Markdown 中包含多个代码块，代表分析的不同部分。当你创建 Markdown 文件并将其转化为文档时，这些代码块会按顺序运行，且它们的输出会按照对应代码块在文档中的位置依次显示。 一个代码块以 {r name} 开始，如以下示例所示： 你可以看到，代码块以灰色阴影显示，并在右上角显示了几个图标。代码块有几个组成部分。 首先，代码块命名，可以方便识别代码块的作用，并有助于文档的组织。注意，每个代码块必须有不同的名称，否则在编译 R Markdown 文件时会出现错误。 在 {r ...} 中指定代码块的多个选项，这些选项以逗号分隔。常见选项包括： echo=TRUE 或 FALSE：如果选择 TRUE，代码块中的实际代码会与其生成的结果一起显示在文档中。 library(tidyverse) 如果选择 FALSE，代码不会显示，只有输出会显示。 warning=TRUE 或 FALSE：如果选择 TRUE（默认值），代码中的所有警告信息都会包含在文档中。如果选择 FALSE，这些警告信息将不会被包含。 include=TRUE 或 FALSE：如果选择 TRUE（默认值），代码的输出会包含在文档中。如果选择 FALSE，代码仍然会运行，但输出不会包含在文档中。这在某些情况下很有用，例如当你在后续代码块中使用此代码块的某些计算结果，但对该代码块的实际输出不感兴趣时。 eval= 默认TRUE，当FALSE，只显示代码，不运行代码。 message = FALSE：不输出提示信息，比如包的载入信息。 collapse = TRUE：代码块结果放在一个文本块。这个对于编写教程节省空间比较有用。 cache = TRUE：缓存运行结果，能加速后续再编译。这个对于编写教材中包含运算量较大的模块比较有用。 右上角的图标中，最右侧的选项用于只运行当前代码块，这对于查看代码块的输出以及/或在创建 R Markdown 文档时验证代码块中的代码是否正确运行非常必要。注意，当你运行一个代码块时，它的输出会显示在该代码块的正下方。要获取代码块生成的所有内容（输出、警告、错误等）。 17.3 Markdown区域 代码部分以外的空白区域是你放置文档中非 R 代码部分的地方。这个区域的工作方式与 Word 文档类似，你可以在其中放置文本、图像、表格等。这种将 R 代码、其结果与类似 Word 文档的功能结合在一起的方式，使你能够在一个文件中创建一个全面的分析报告。 你还可以格式化此区域内显示的文本。 "],["r语言学习资源分类汇总.html", "Chapter 18 R语言学习资源分类汇总 18.1 统计建模 18.2 机器学习 18.3 数据整理 18.4 数据可视化 18.5 网络分析 18.6 文本分析 18.7 网络抓取 18.8 社交媒体分析 18.9 语音分析 18.10 学科研究 18.11 R-markdown", " Chapter 18 R语言学习资源分类汇总 18.1 统计建模 首先，很多人了解R并开始使用它，尤其是用于研究目的，通常是因为他们需要进行一些统计建模，而R有相应的包可以完成这项工作。事实上，统计建模是R的一个重要优势。对于简单的统计建模，像SPSS这样的不需要编码的程序可以和R一样出色，许多人仍然依赖SPSS进行统计建模。然而，SPSS不是开源的，它是由公司拥有的，而R是开源的。这意味着全球的R用户可以通过开发包来丰富R的功能，而不是依赖于公司内部的一小部分人。如果需要更复杂和精密的模型，可以简单地用Google搜索：“model name” with R。 Summary and Analysis of Extension Program Evaluation in R 当我需要了解如何进行各种t检验、ANOVA或回归分析时，我常用的参考。 18.2 机器学习 Tidymodels packages The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. 18.3 数据整理 R for Data Science by Garrett Grolemund和Hadley Wickham 我的工作坊的教程。 其他书籍: Data Science Live Book The Pirate’s Guide to R 18.4 数据可视化 ggplot2: Elegant Graphics for Data Analysis。 cookbook R。 R Graphics Cookbook, 2nd edition。 地图可视化Geocomputation with R 流程图DiagramR 18.5 网络分析 Network Analysis in R by Robert Wiederstein An Introduction to Network Analysis for Psycholinguists by Cynthia Siew 网络可视化 Static and dynamic network visualization with R Introduction to Network Analysis with R ggnet2: network visualization with ggplot2 论文。 18.6 文本分析 Corpus Linguistics by Alvin Cheng-Hsien Chen Text mining with R Computational Stylistics Group is a cross-institutional research team focused on computer-assisted text analysis, stylometry, authorship attribution, sentiment analysis, and the like stuff. The research projects conducted by the team members could be described as an intersection of linguistics, literary criticism, and computer science – however the best name here would be “Digital Humanities”. The group is based mostly in Kraków, at the Institute of Polish Language (Polish Academy of Sciences), but also at the Jagiellonian University and the University of Antwerp. 18.7 网络抓取 rvest: easy web scraping with R 教程 1 教程 2 18.8 社交媒体分析 随着越来越多的人深深沉浸在社交媒体中，分析他们的语言行为可能会揭示出有价值的信息。 Learning Social Media Analytics with R 教程 1 18.9 语音分析 wrassp 是R的一个封装包，围绕Michel Scheffers的libassp（高级语音信号处理器）构建。libassp库旨在提供处理大多数常见音频格式的语音信号文件并执行语音科学/语音科学中常见分析的功能。这包括计算共振峰、基频、均方根、自动相关、一系列频谱分析、零交叉率、滤波等。这个封装包为R提供了libassp信号处理功能的大部分子集，并以一种（希望）用户友好的方式提供给用户。 -教程 soundgen 基于seewave的功能，增加了用于声音合成、操作和分析的高级函数（参见关于声音合成的小册子）。教程 EMU-R: EMU语音数据库管理系统（EMU-SDMS）是一组软件工具，旨在尽可能接近一个集成解决方案，用于生成、操作、查询、分析和管理语音数据库。手册 18.10 学科研究 文献计量bibliometrix Using R for psychological research The psych and psychTools packages 18.11 R-markdown 当你希望输出你的R工作时，将代码与分析嵌入在一起是理想的选择。这在Word或其他文件类型中并不容易实现。感谢r markdown包，你现在可以： 将单个R Markdown文档编译为不同格式的报告，如PDF、HTML或Word。 创建可以直接交互运行代码块的笔记本。 R Markdown: The Definitive Guide Tufte DataTables 生成网站和博客。 教程: blogdown: Creating Websites with R Markdown 编写多章节的书籍。 教程: bookdown: Authoring Books and Technical Documents with R Markdown 制作演示幻灯片 xaringan, creating remark.js through R Markdown reveal.js Presentations reveal.js 创建R的交互式教程 教程 pkgdown Markdown 语法 - 字体 + 我是宋体 + 我是楷体- 字号 + 我是尺寸 + 我是尺寸 - 颜色 + 我是蓝色 + 我是紫色 - 背景 + 黑色背景白字 对齐 这是居中对齐的段落 这是右对齐的段落 分散对齐 缩进 首行缩进：这是一行以上的段落，第1行进行了缩进，之后所有行没有缩进。缩进的宽度是可以调节的，这里展示的常见的缩进两个汉字宽度的形式，可以设置为2em或32px。 每当我开展R语言的工作坊时，我首先需要向参与者解释为什么他们应该学习R语言。 下面是使用R可以做的一些很酷的事情，涵盖了从数据分析、统计建模、可视化到制作幻灯片、仪表板、博客文章、互动教程等多个方面。 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
